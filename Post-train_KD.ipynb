{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "sys.path.insert(0, './PLM-NR')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from utils import MODEL_CLASSES\n",
    "from tnlrv3.tokenization_tnlrv3 import TuringNLRv3Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_turing = './unilmv2/'\n",
    "tokenizer = TuringNLRv3Tokenizer.from_pretrained(os.path.join(path_turing, 'unilm2-base-uncased-vocab.txt'),\n",
    "                                                 do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.news_query_vector_dim = 200\n",
    "        self.drop_rate = 0.2\n",
    "        self.news_dim = 256\n",
    "        self.num_hidden_layers = 4\n",
    "        self.corpus_path = './docs_filter.tsv'\n",
    "        self.num_teachers = 4\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TITLE_LEN = 24\n",
    "MAX_BODY_LEN = 512\n",
    "NPRATIO=9\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.corpus_path, encoding='utf-8') as f:\n",
    "    total_lines = f.readlines()\n",
    "len(total_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, bodies = [], []\n",
    "for line in total_lines:\n",
    "    splited = line.strip('\\n').split('\\t')\n",
    "    titles.append(splited[3])\n",
    "    bodies.append(splited[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must run Domain-specific_Post-train.ipynb first\n",
    "title_scorings, body_scorings = [], []\n",
    "for i in range(args.num_teachers):\n",
    "    with open(f'./teacher_title_emb_{i}.pkl', 'rb') as f:\n",
    "        title_scorings.append(pickle.load(f))\n",
    "    with open(f'./teacher_body_emb_{i}.pkl', 'rb') as f:\n",
    "        body_scorings.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillDataset(Dataset):\n",
    "    def __init__(self, titles, bodies, teacher_titles, teacher_bodies):\n",
    "        self.titles = titles\n",
    "        self.bodies = bodies\n",
    "        self.teacher_titles = teacher_titles\n",
    "        self.teacher_bodies = teacher_bodies\n",
    "        self.len = len(titles)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        select_list = list(range(0, idx)) + list(range(idx+1, self.len))\n",
    "        neg_idx = random.sample(select_list, NPRATIO)\n",
    "        neg_titles = [self.titles[i] for i in neg_idx]\n",
    "        pos_title = self.titles[idx]\n",
    "        titles = [tokenizer(title, max_length=MAX_TITLE_LEN, pad_to_max_length=True,\n",
    "                            truncation=True) for title in [pos_title] + neg_titles]\n",
    "        input_titles = np.array([title['input_ids'] + title['attention_mask'] for title in titles])\n",
    "        body = tokenizer(self.bodies[idx], max_length=MAX_BODY_LEN, pad_to_max_length=True,\n",
    "                        truncation=True)\n",
    "        input_body = np.array(body['input_ids'] + body['attention_mask'])\n",
    "                                 \n",
    "        total_idx = [idx] + neg_idx\n",
    "        input_teacher_titles = [x[total_idx] for x in self.teacher_titles]\n",
    "        input_teacher_bodies = [x[idx] for x in self.teacher_bodies]\n",
    "        label=0\n",
    "        return input_titles, input_body, label, input_teacher_titles, input_teacher_bodies\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_ds = DistillDataset(titles, bodies, title_scorings, body_scorings)\n",
    "distill_dl = DataLoader(distill_ds, batch_size=BATCH_SIZE, num_workers=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.att_fc1 = nn.Linear(emb_size, hidden_size)\n",
    "        self.att_fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: batch_size, candidate_size, emb_dim\n",
    "            attn_mask: batch_size, candidate_size\n",
    "        Returns:\n",
    "            (shape) batch_size, emb_dim\n",
    "        \"\"\"\n",
    "        bz = x.shape[0]\n",
    "        e = self.att_fc1(x)\n",
    "        e = nn.Tanh()(e)\n",
    "        alpha = self.att_fc2(e)\n",
    "        alpha = torch.exp(alpha)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            alpha = alpha * attn_mask.unsqueeze(2)\n",
    "        \n",
    "        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
    "        x = torch.bmm(x.permute(0, 2, 1), alpha).squeeze(dim=-1)\n",
    "        return x\n",
    "\n",
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        config_class, model_class, tokenizer_class = MODEL_CLASSES['tnlrv3']\n",
    "        self.bert_config = config_class.from_pretrained(\n",
    "            os.path.join(path_turing, 'unilm2-base-uncased-config.json'), \n",
    "            output_hidden_states=True,\n",
    "            num_hidden_layers=args.num_hidden_layers)\n",
    "        self.bert_model = model_class.from_pretrained(\n",
    "            os.path.join(path_turing, 'unilm2-base-uncased.bin'), config=self.bert_config)\n",
    "        self.attn = AttentionPooling(self.bert_config.hidden_size, args.news_query_vector_dim)\n",
    "        self.dense = nn.Linear(self.bert_config.hidden_size, args.news_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            x: batch_size, word_num * 2\n",
    "            mask: batch_size, word_num\n",
    "        '''\n",
    "        batch_size, num_words = x.shape\n",
    "        num_words = num_words // 2\n",
    "        text_ids = torch.narrow(x, 1, 0, num_words)\n",
    "        text_attmask = torch.narrow(x, 1, num_words, num_words)\n",
    "        word_vecs = self.bert_model(text_ids, text_attmask)[3][self.bert_config.num_hidden_layers]\n",
    "        news_vec = self.attn(word_vecs)\n",
    "        news_vec = self.dense(news_vec)\n",
    "        return news_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleBodySimModel(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(TitleBodySimModel, self).__init__()\n",
    "        self.news_encoder = NewsEncoder(args)\n",
    "        \n",
    "    def forward(self, title, body):\n",
    "        '''\n",
    "            title: bz, 1+K, MAX_TITLE_WORD * 2\n",
    "            body: bz, MAX_BODY_WORD * 2\n",
    "            labels: bz\n",
    "        '''\n",
    "        body_emb = self.news_encoder(body)             #bz,emb_dim\n",
    "        bz, candi_num, input_num = title.shape\n",
    "        title = title.reshape(-1, input_num)\n",
    "        title_emb = self.news_encoder(title)\n",
    "        title_emb = title_emb.reshape(bz, candi_num, -1) #bz, 1+K, emb_dim\n",
    "        \n",
    "        scores = torch.bmm(title_emb, body_emb.unsqueeze(dim=-1)).squeeze(-1)\n",
    "        \n",
    "        return scores, title_emb, body_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kd_ce_loss(logits_S, logits_T, temperature=1):\n",
    "    '''\n",
    "    Calculate the cross entropy between logits_S and logits_T\n",
    "    :param logits_S: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)\n",
    "    :param logits_T: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)\n",
    "    :param temperature: A float or a tensor of shape (batch_size, length) or (batch_size,)\n",
    "    '''\n",
    "    beta_logits_T = logits_T / temperature\n",
    "    beta_logits_S = logits_S / temperature\n",
    "    p_T = F.softmax(beta_logits_T, dim=-1)\n",
    "    loss = -(p_T * F.log_softmax(beta_logits_S, dim=-1)).sum(dim=-1).mean()\n",
    "    return loss\n",
    "\n",
    "def hid_mse_loss(state_S, state_T, mask=None, reduce=True):\n",
    "    '''\n",
    "    * Calculates the mse loss between `state_S` and `state_T`, which are the hidden state of the models.\n",
    "    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n",
    "    * If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.\n",
    "    :param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n",
    "    :param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n",
    "    :param torch.Tensor mask:    tensor of shape  (*batch_size*, *length*)\n",
    "    '''\n",
    "    if mask is None:\n",
    "        if not reduce:\n",
    "            loss = F.mse_loss(state_S, state_T, reduction='none').mean(dim=-1)\n",
    "        else:\n",
    "            loss = F.mse_loss(state_S, state_T)\n",
    "    else:\n",
    "        if not reduce:\n",
    "            loss = (F.mse_loss(state_S, state_T, reduction='none') *\n",
    "                    mask.unsqueeze(-1)).mean(dim=-1)\n",
    "        else:\n",
    "            valid_count = mask.sum() * state_S.size(-1)\n",
    "            loss = (F.mse_loss(state_S, state_T, reduction='none') *\n",
    "                    mask.unsqueeze(-1)).sum() / valid_count\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillModel(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(DistillModel, self).__init__()\n",
    "        self.student = TitleBodySimModel(args)\n",
    "        self.target_loss = nn.CrossEntropyLoss()\n",
    "        self.distill_loss = kd_ce_loss\n",
    "        self.emb_loss = hid_mse_loss\n",
    "        self.transform_matrix = nn.ModuleList([nn.Linear(args.news_dim, args.news_dim) for _ in range(args.num_teachers)])\n",
    "        for module in self.transform_matrix:\n",
    "            nn.init.xavier_uniform_(module.weight, gain=1.)\n",
    "            nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "    def forward(self, title, body, labels, teacher_titles, teacher_bodies):\n",
    "        '''\n",
    "            teacher_titles: [(batch_size, 1+K, news_emb) * num_teachers]\n",
    "            teacher_bodies: [(batch_size, news_emb) * num_teachers]\n",
    "        '''\n",
    "        student_score, student_title, student_body = self.student(title, body)\n",
    "        target_loss = self.target_loss(student_score, labels)\n",
    "\n",
    "        teacher_scores, teacher_losses, teacher_MSEs = [], [], []\n",
    "        for i, (teacher_title, teacher_body) in enumerate(zip(teacher_titles, teacher_bodies)):\n",
    "            teacher_score = torch.bmm(teacher_title, teacher_body.unsqueeze(dim=-1)).squeeze(dim=-1)\n",
    "            teacher_loss = F.cross_entropy(teacher_score, labels, reduction='none')\n",
    "            teacher_scores.append(teacher_score)\n",
    "            teacher_losses.append(teacher_loss)\n",
    "\n",
    "            teacher_title_proj = self.transform_matrix[i](teacher_title)\n",
    "            teacher_body_proj = self.transform_matrix[i](teacher_body)\n",
    "            teacher_MSE = \\\n",
    "                self.emb_loss(student_title, teacher_title_proj, reduce=False).mean(dim=-1) + \\\n",
    "                    self.emb_loss(student_body, teacher_body_proj, reduce=False)\n",
    "            teacher_MSEs.append(teacher_MSE)\n",
    "\n",
    "        teacher_losses = - torch.stack(teacher_losses, dim=-1)\n",
    "        teacher_weights = F.softmax(teacher_losses, dim=-1)\n",
    "\n",
    "        teacher_scores = torch.stack(teacher_scores, dim=-1)\n",
    "        teacher_scores = torch.bmm(teacher_scores, teacher_weights.unsqueeze(dim=-1)).squeeze(dim=-1)\n",
    "        distill_loss = self.distill_loss(student_score, teacher_scores)\n",
    "        emb_loss = (teacher_MSEs * teacher_weights).sum(dim=-1).mean()\n",
    "\n",
    "        loss = target_loss + distill_loss + emb_loss\n",
    "        return loss, target_loss, distill_loss, emb_loss, student_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distill_model = DistillModel(args)\n",
    "device = torch.device('cuda')\n",
    "distill_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_hat):\n",
    "    y_hat = torch.argmax(y_hat, dim=-1)\n",
    "    tot = y_true.shape[0]\n",
    "    hit = torch.sum(y_true == y_hat)\n",
    "    return hit.data.float() * 1.0 / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in distill_model.student.news_encoder.bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for index, layer in enumerate(distill_model.student.news_encoder.bert_model.bert.encoder.layer):\n",
    "    if index in [2, 3]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, p in distill_model.named_parameters():\n",
    "    print(name, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_param = filter(\n",
    "    lambda x: id(x) not in list(map(id, distill_model.student.news_encoder.bert_model.parameters())), distill_model.parameters())\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    [{'params':distill_model.student.news_encoder.bert_model.parameters(),'lr':1e-6},\n",
    "    {'params':rest_param,'lr':1e-5}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ep in range(1):\n",
    "    loss, target_loss, distill_loss, emb_loss = 0.0, 0.0, 0.0, 0.0\n",
    "    accuary = 0.0\n",
    "    cnt = 1\n",
    "    tqdm_util = tqdm(distill_dl)\n",
    "    distill_model.train()\n",
    "    for title,body,labels,teacher_title,teacher_body in tqdm_util:\n",
    "        title = title.cuda(non_blocking=True)\n",
    "        body = body.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "        teacher_title = teacher_title.cuda(non_blocking=True)\n",
    "        teacher_body = teacher_body.cuda(non_blocking=True)\n",
    "        \n",
    "        bz_loss, t_loss, d_loss, e_loss, y_hat = distill_model(title, body, labels, teacher_title, teacher_body)\n",
    "        \n",
    "        loss += bz_loss.data.float()\n",
    "        target_loss += t_loss.data.float()\n",
    "        distill_loss += d_loss.data.float()\n",
    "        emb_loss += e_loss.data.float()\n",
    "        accuary += acc(labels, y_hat)\n",
    "        optimizer.zero_grad()\n",
    "        bz_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if cnt % 10 == 0:\n",
    "            tqdm_util.set_description('ed: {}, loss: {:.5f}, t_loss: {:.5f}, d_loss: {:.5f}, e_loss: {:.5f}, acc: {:.5f}'.format(\n",
    "                cnt * BATCH_SIZE, loss.data / cnt, target_loss.data / cnt, distill_loss.data / cnt, emb_loss.data / cnt, accuary / cnt))\n",
    "        \n",
    "        cnt += 1\n",
    "        \n",
    "    ckpt_path = f'./first_stage_{args.num_hidden_layers}_layer.pt'\n",
    "    torch.save({'model_state_dict': distill_model.state_dict()}, ckpt_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "281345ef05f2418b1987ab66bde4c74b95701a6cc94cddb85ab0b47b963f4ee6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
