{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi\n",
    "This note book "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____————____————____————____————\n",
    "# Define the history size and fraction and EPOCHS\n",
    "# ____————____————____————____————\n",
    "HISTORY_SIZE = 20 #30\n",
    "FRACTION = 0.001 #Fraction af datasæt\n",
    "EPOCHS = 2\n",
    "FRACTION_testset = 0.0001\n",
    "#____————____————____————____————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 15:36:20.185462: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-20 15:36:20.303686: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-20 15:36:20.303726: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-20 15:36:20.304240: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-20 15:36:20.361603: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-20 15:36:21.641872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_ARTICLE_ID_COL,\n",
    "    DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL, #-------\n",
    "    DEFAULT_HISTORY_READ_TIME_COL #-------\n",
    ")\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "from ebrec.models.newsrec.dataloader import NRMSDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms\n",
    "from ebrec.models.newsrec import NRMSModel, NRMSWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Available devices: []\n",
      "____————____————____————____———\n",
      "HISTORY_SIZE: 20\n",
      "FRACTION: 0.001\n",
      "EPOCHS: 2\n",
      "FRACTION_testset: 0.0001\n",
      "____————____————____————____———\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 15:36:23.703034: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 9)\n",
      "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ article_i ┆ impressio ┆ impressio ┆ labels    │\n",
      "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ds_clicke ┆ n_id      ┆ n_time    ┆ ---       │\n",
      "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ d         ┆ ---       ┆ ---       ┆ list[i8]  │\n",
      "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ u32       ┆ datetime[ ┆           │\n",
      "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i64] ┆           ┆ μs]       ┆           │\n",
      "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 2490793 ┆ [9764378,  ┆ [41.0,    ┆ [3.0,     ┆ … ┆ [9772300] ┆ 115527786 ┆ 2023-05-2 ┆ [0, 0, …  │\n",
      "│         ┆ 9764617, … ┆ 87.0, …   ┆ 11.0, …   ┆   ┆           ┆           ┆ 0         ┆ 1]        │\n",
      "│         ┆ 9769328]   ┆ 20.0]     ┆ 431.0]    ┆   ┆           ┆           ┆ 07:05:59  ┆           │\n",
      "│ 841153  ┆ [9767417,  ┆ [null,    ┆ [296.0,   ┆ … ┆ [9775402] ┆ 25357695  ┆ 2023-05-2 ┆ [1, 0, …  │\n",
      "│         ┆ 9767417, … ┆ null, …   ┆ 416.0, …  ┆   ┆           ┆           ┆ 3         ┆ 0]        │\n",
      "│         ┆ 9638950]   ┆ null]     ┆ 0.0]      ┆   ┆           ┆           ┆ 19:04:24  ┆           │\n",
      "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n",
      "shape: (2, 9)\n",
      "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ article_i ┆ impressio ┆ impressio ┆ labels    │\n",
      "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ds_clicke ┆ n_id      ┆ n_time    ┆ ---       │\n",
      "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ d         ┆ ---       ┆ ---       ┆ list[i8]  │\n",
      "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ u32       ┆ datetime[ ┆           │\n",
      "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i32] ┆           ┆ μs]       ┆           │\n",
      "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 2070397 ┆ [9779227,  ┆ [100.0,   ┆ [75.0,    ┆ … ┆ [9779867] ┆ 204484859 ┆ 2023-05-2 ┆ [0, 0, …  │\n",
      "│         ┆ 9778813, … ┆ 33.0, …   ┆ 16.0, …   ┆   ┆           ┆           ┆ 9         ┆ 0]        │\n",
      "│         ┆ 9779408]   ┆ 45.0]     ┆ 2.0]      ┆   ┆           ┆           ┆ 19:04:03  ┆           │\n",
      "│ 767191  ┆ [9776184,  ┆ [45.0,    ┆ [11.0,    ┆ … ┆ [9783197] ┆ 414964653 ┆ 2023-05-2 ┆ [0, 1, …  │\n",
      "│         ┆ 9754265, … ┆ 78.0, …   ┆ 0.0, …    ┆   ┆           ┆           ┆ 7         ┆ 0]        │\n",
      "│         ┆ 9778158]   ┆ 100.0]    ┆ 107.0]    ┆   ┆           ┆           ┆ 17:06:18  ┆           │\n",
      "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var i…</td><td>&quot;Politiet frygt…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den ø…</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars …</td><td>&quot;Biografgængern…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har …</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natascha… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind for…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "# Setup and load everything\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "print(\"Loading data\")\n",
    "\n",
    "# List all physical devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available devices:\", physical_devices)\n",
    "\n",
    "\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "## Load dataset\n",
    "# #-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "# def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Load ebnerd - function\n",
    "#     \"\"\"\n",
    "#     df_history = (\n",
    "#         pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "#         # .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL,DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL)\n",
    "#         .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL,DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL,DEFAULT_HISTORY_READ_TIME_COL) #------------\n",
    "#         .pipe(\n",
    "#             truncate_history,\n",
    "#             column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "#             history_size=history_size,\n",
    "#             padding_value=0,\n",
    "#             enable_warning=False,\n",
    "#         )\n",
    "#     )\n",
    "#     df_behaviors = (\n",
    "#         pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "#         .collect()\n",
    "#         .pipe(\n",
    "#             slice_join_dataframes,\n",
    "#             df2=df_history.collect(),\n",
    "#             on=DEFAULT_USER_COL,\n",
    "#             how=\"left\",\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     return df_behaviors\n",
    "\n",
    "\n",
    "def ebnerd_from_path(\n",
    "    path: Path,\n",
    "    history_size: int = 30,\n",
    "    padding: int = 0,\n",
    "    user_col: str = DEFAULT_USER_COL,\n",
    "    history_aids_col: str = DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        # .select(user_col, history_aids_col)\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL,DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL,DEFAULT_HISTORY_READ_TIME_COL) #------------\n",
    "\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=history_aids_col,\n",
    "            history_size=history_size,\n",
    "            padding_value=padding,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=user_col,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors\n",
    "  \n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "  ### Generate labels\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "# We sample a few just to get started. For testset we just make up a dummy column with 0 and 1 - this is not the true labels.\n",
    "\n",
    "PATH = Path(\"/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/ebnerd_data\").expanduser()\n",
    "DATASPLIT = \"ebnerd_small\" # TODO if change to change make_embedding_artifacts.ipynb file (embeddings)\n",
    "\n",
    "# DATASPLIT = \"ebnerd__testset\"\n",
    "DUMP_DIR = PATH.joinpath(\"dump_artifacts\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "#In this example we sample the dataset, just to keep it smaller. Also, one can simply add the testset similary to the validation.\n",
    "\n",
    "### Define the Data Cols -- New ones here\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL, #--------neu \n",
    "    DEFAULT_HISTORY_READ_TIME_COL, #------- neu\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "]\n",
    "# COLUMNS = [\n",
    "#     DEFAULT_USER_COL,\n",
    "#     DEFAULT_IMPRESSION_ID_COL,\n",
    "#     DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "#     DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "#     DEFAULT_CLICKED_ARTICLES_COL,\n",
    "#     DEFAULT_INVIEW_ARTICLES_COL,\n",
    "# ]\n",
    "\n",
    "\n",
    "print(\"____————____————____————____———\")\n",
    "print(\"HISTORY_SIZE:\", HISTORY_SIZE)\n",
    "print(\"FRACTION:\", FRACTION)\n",
    "print(\"EPOCHS:\", EPOCHS)\n",
    "print(\"FRACTION_testset:\", FRACTION_testset)\n",
    "print(\"____————____————____————____———\")\n",
    "print(\"\")\n",
    "\n",
    "#____————____————____————____————\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=6,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "print(df_train.head(2))\n",
    "print(df_validation.head(2))\n",
    "\n",
    "\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "## Load articles\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "df_articles = pl.read_parquet(PATH.joinpath(DATASPLIT, \"articles.parquet\"))\n",
    "df_articles.head(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>scroll_percentage_fixed</th><th>read_time_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>inview_hour_differences</th><th>inview_article_categories</th><th>history_article_categories</th><th>inview_article_types</th><th>history_article_types</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[f32]</td><td>list[f32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>list[i64]</td><td>list[i64]</td><td>list[str]</td><td>list[str]</td></tr></thead><tbody><tr><td>2070397</td><td>[9779227, 9778813, … 9779408]</td><td>[100.0, 33.0, … 45.0]</td><td>[75.0, 16.0, … 2.0]</td><td>[9786351, 9785062, … 9781998]</td><td>[9779867]</td><td>204484859</td><td>[0, 0, … 0]</td><td>[7.648056, 5.520278, … 83.661667]</td><td>[118, 118, … 118]</td><td>[140, 414, … 2975]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td></tr><tr><td>767191</td><td>[9776184, 9754265, … 9778158]</td><td>[45.0, 78.0, … 100.0]</td><td>[11.0, 0.0, … 107.0]</td><td>[9784064, 9783197, … 9784344]</td><td>[9783197]</td><td>414964653</td><td>[0, 1, … 0]</td><td>[3.375833, 4.6875, … 0.433611]</td><td>[140, 498, … 118]</td><td>[498, 2077, … 118]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[&quot;article_default&quot;, &quot;article_opinionen&quot;, … &quot;article_default&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 13)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ inview_ar ┆ history_a ┆ inview_ar ┆ history_a │\n",
       "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ticle_cat ┆ rticle_ca ┆ ticle_typ ┆ rticle_ty │\n",
       "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ egories   ┆ tegories  ┆ es        ┆ pes       │\n",
       "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i64] ┆ list[i64] ┆ list[str] ┆ list[str] │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2070397 ┆ [9779227,  ┆ [100.0,   ┆ [75.0,    ┆ … ┆ [118,     ┆ [140,     ┆ [\"article ┆ [\"article │\n",
       "│         ┆ 9778813, … ┆ 33.0, …   ┆ 16.0, …   ┆   ┆ 118, …    ┆ 414, …    ┆ _default\" ┆ _default\" │\n",
       "│         ┆ 9779408]   ┆ 45.0]     ┆ 2.0]      ┆   ┆ 118]      ┆ 2975]     ┆ , \"articl ┆ , \"articl │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ e_def…    ┆ e_def…    │\n",
       "│ 767191  ┆ [9776184,  ┆ [45.0,    ┆ [11.0,    ┆ … ┆ [140,     ┆ [498,     ┆ [\"article ┆ [\"article │\n",
       "│         ┆ 9754265, … ┆ 78.0, …   ┆ 0.0, …    ┆   ┆ 498, …    ┆ 2077, …   ┆ _default\" ┆ _default\" │\n",
       "│         ┆ 9778158]   ┆ 100.0]    ┆ 107.0]    ┆   ┆ 118]      ┆ 118]      ┆ , \"articl ┆ , \"articl │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ e_def…    ┆ e_opi…    │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "# Loading the article embeddings and other features\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "### Added features and hourly difference between published and viewed article\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert polars DataFrame to pandas cuz easier\n",
    "df_train = df_train.to_pandas()\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_time\n",
    "article_time_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"published_time\"\n",
    ").to_dict(as_series=False)\n",
    "article_time_dict = dict(zip(\n",
    "    article_time_dict[\"article_id\"], \n",
    "    article_time_dict[\"published_time\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their timestamps\n",
    "def get_article_times(article_ids):\n",
    "    return [article_time_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the published-time\n",
    "df_train[\"inview_article_times\"] = df_train[\"article_ids_inview\"].apply(get_article_times)\n",
    "\n",
    "#add new column with the last publish_time for the clicked article\n",
    "df_train[\"clicked_article_time\"] = df_train[\"article_ids_clicked\"].apply(get_article_times)\n",
    "\n",
    "# Create a function to calculate hour differences\n",
    "def calculate_hour_differences(impression_time, article_times):\n",
    "        # If article_times is a single value (for clicked articles)\n",
    "    if not isinstance(article_times, list):\n",
    "        if article_times is None:\n",
    "            return None\n",
    "        return (impression_time - article_times).total_seconds() / 3600\n",
    "    \n",
    "    # If article_times is a list (for inview articles)\n",
    "    differences = [(impression_time - article_time).total_seconds() / 3600 \n",
    "                  if article_time is not None else None \n",
    "                  for article_time in article_times]\n",
    "    return differences\n",
    "\n",
    "# Use for inview articles\n",
    "df_train['inview_hour_differences'] = df_train.apply(\n",
    "    lambda row: calculate_hour_differences(row['impression_time'], row['inview_article_times']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# # Use for clicked article\n",
    "# df_train['clicked_hour_difference'] = df_train.apply(\n",
    "#    lambda row: calculate_hour_differences(row['impression_time'], row['clicked_article_time']), \n",
    "#    axis=1\n",
    "# )\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_category\n",
    "article_cat_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"category\"\n",
    ").to_dict(as_series=False)\n",
    "article_cat_dict = dict(zip(\n",
    "    article_cat_dict[\"article_id\"], \n",
    "    article_cat_dict[\"category\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their category\n",
    "def get_article_category(article_ids):\n",
    "    return [article_cat_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "#  Add new column with the article category\n",
    "df_train[\"inview_article_categories\"] = df_train[\"article_ids_inview\"].apply(get_article_category)\n",
    "\n",
    "df_train[\"history_article_categories\"] = df_train[\"article_id_fixed\"].apply(get_article_category)\n",
    "\n",
    "# Create a mapping dictionary from article_id to article_type\n",
    "article_type_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"article_type\"\n",
    ").to_dict(as_series=False)\n",
    "article_type_dict = dict(zip(\n",
    "    article_type_dict[\"article_id\"], \n",
    "    article_type_dict[\"article_type\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their type\n",
    "def get_article_type(article_ids):\n",
    "    return [article_type_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the article type\n",
    "df_train[\"inview_article_types\"] = df_train[\"article_ids_inview\"].apply(get_article_type)\n",
    "\n",
    "df_train[\"history_article_types\"] = df_train[\"article_id_fixed\"].apply(get_article_type)\n",
    "\n",
    "#drop columns with the time\n",
    "df_train = df_train.drop(['inview_article_times', 'clicked_article_time','impression_time'], axis=1)\n",
    "\n",
    "df_train = pl.from_pandas(df_train)\n",
    "\n",
    "df_train.head(2)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "# Same for Validaiton set\n",
    "######################\n",
    "\n",
    "# Convert polars DataFrame to pandas\n",
    "df_validation = df_validation.to_pandas()\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_time\n",
    "article_time_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"published_time\"\n",
    ").to_dict(as_series=False)\n",
    "article_time_dict = dict(zip(\n",
    "    article_time_dict[\"article_id\"], \n",
    "    article_time_dict[\"published_time\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their timestamps\n",
    "def get_article_times(article_ids):\n",
    "    return [article_time_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the published-time\n",
    "df_validation[\"inview_article_times\"] = df_validation[\"article_ids_inview\"].apply(get_article_times)\n",
    "\n",
    "#add new column with the last publish_time for the clicked article\n",
    "df_validation[\"clicked_article_time\"] = df_validation[\"article_ids_clicked\"].apply(get_article_times)\n",
    "\n",
    "# Create a function to calculate hour differences\n",
    "def calculate_hour_differences(impression_time, article_times):\n",
    "        # If article_times is a single value (for clicked articles)\n",
    "    if not isinstance(article_times, list):\n",
    "        if article_times is None:\n",
    "            return None\n",
    "        return (impression_time - article_times).total_seconds() / 3600\n",
    "    \n",
    "    # If article_times is a list (for inview articles)\n",
    "    differences = [(impression_time - article_time).total_seconds() / 3600 \n",
    "                  if article_time is not None else None \n",
    "                  for article_time in article_times]\n",
    "    return differences\n",
    "\n",
    "# Use for inview articles\n",
    "df_validation['inview_hour_differences'] = df_validation.apply(\n",
    "    lambda row: calculate_hour_differences(row['impression_time'], row['inview_article_times']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# # Use for clicked article -- might be leaky??\n",
    "# df_validation['clicked_hour_difference'] = df_validation.apply(\n",
    "#    lambda row: calculate_hour_differences(row['impression_time'], row['clicked_article_time']), \n",
    "#    axis=1\n",
    "# )\n",
    "# Create a mapping dictionary from article_id to last_modified_category\n",
    "article_cat_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"category\"\n",
    ").to_dict(as_series=False)\n",
    "article_cat_dict = dict(zip(\n",
    "    article_cat_dict[\"article_id\"], \n",
    "    article_cat_dict[\"category\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their category\n",
    "def get_article_category(article_ids):\n",
    "    return [article_cat_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "#  Add new column with the article category\n",
    "df_validation[\"inview_article_categories\"] = df_validation[\"article_ids_inview\"].apply(get_article_category)\n",
    "\n",
    "df_validation[\"history_article_categories\"] = df_validation[\"article_id_fixed\"].apply(get_article_category)\n",
    "\n",
    "# Create a mapping dictionary from article_id to article_type\n",
    "article_type_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"article_type\"\n",
    ").to_dict(as_series=False)\n",
    "article_type_dict = dict(zip(\n",
    "    article_type_dict[\"article_id\"], \n",
    "    article_type_dict[\"article_type\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their type\n",
    "def get_article_type(article_ids):\n",
    "    return [article_type_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the article type\n",
    "df_validation[\"inview_article_types\"] = df_validation[\"article_ids_inview\"].apply(get_article_type)\n",
    "\n",
    "df_validation[\"history_article_types\"] = df_validation[\"article_id_fixed\"].apply(get_article_type)\n",
    "\n",
    "\n",
    "#drop columns with the time\n",
    "df_validation = df_validation.drop(['inview_article_times', 'clicked_article_time','impression_time'], axis=1)\n",
    "\n",
    "\n",
    "df_validation = pl.from_pandas(df_validation)\n",
    "\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____————____————____————____———\n",
      "Using transformer model: Maltehb/danish-bert-botxo\n",
      "____————____————____————____———\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "## Init model using HuggingFace's tokenizer and wordembedding -- simmilar to original code\n",
    "\n",
    "# In the original implementation, they use the GloVe embeddings and tokenizer. To get going fast, we'll use a multilingual LLM from Hugging Face. \n",
    "# Utilizing the tokenizer to tokenize the articles and the word-embedding to init NRMS.\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "# TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "# TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-robe rta-large\"\n",
    "# TRANSFORMER_MODEL_NAME = \"google-bert/bert-base-multilingual-uncased\" \n",
    "#Argue for cased vs uncased.  TODO\n",
    "# #Cased might be better but to compare with malteHb we use uncased\n",
    "\n",
    "TRANSFORMER_MODEL_NAME = \"Maltehb/danish-bert-botxo\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30 #hardcoded somewhere ?? error if change\n",
    "\n",
    "print(\"\")\n",
    "print(\"____————____————____————____———\")\n",
    "print(\"Using transformer model:\", TRANSFORMER_MODEL_NAME)\n",
    "print(\"____————____————____————____———\")\n",
    "print(\"\")\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "\n",
    "\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")\n",
    "\n",
    "#_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "# print(\"df_train columns:\", df_train.columns)\n",
    "# print(\"df_validation columns:\", df_validation.columns)\n",
    "#_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=128,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: []\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# List all physical devices\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available devices:\", physical_devices)\n",
    "import torch.nn as nn\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 [Train]: 100%|██████████| 2/2 [00:00<00:00,  2.86it/s, loss=0.6191]\n",
      "Epoch 1/2 [Valid]: 100%|██████████| 4/4 [00:00<00:00,  4.75it/s, loss=0.7075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.6191, Val Loss: 0.3537\n",
      "\n",
      "Validation loss improved from inf to 0.35373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 [Train]: 100%|██████████| 2/2 [00:00<00:00,  6.70it/s, loss=0.4559]\n",
      "Epoch 2/2 [Valid]: 100%|██████████| 4/4 [00:00<00:00,  4.82it/s, loss=0.5754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 0.4559, Val Loss: 0.2877\n",
      "\n",
      "Validation loss improved from 0.35373 to 0.28769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/src/ebrec/models/newsrec/nrmspy_1.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(filepath))\n",
      "Predicting: 100%|██████████| 4/4 [00:00<00:00,  4.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# _-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "# Original Model\n",
    "# _-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "# Works fine -- Can change epochs on line 67\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "WEIGHTS_DIR = f\"downloads/data/state_dict/{MODEL_NAME}\"\n",
    "MODEL_WEIGHTS = f\"{WEIGHTS_DIR}/weights.pt\"\n",
    "\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "#  ModelCheckpoint for PyTorch -- adapted from Keraas\n",
    "class PyTorchModelCheckpoint:\n",
    "    def __init__(self, filepath, model_wrapper=None, save_best_only=True, save_weights_only=True, verbose=1):\n",
    "        self.filepath = filepath\n",
    "        self.model_wrapper = model_wrapper  # Store the model wrapper reference\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.verbose = verbose\n",
    "        self.best_val_loss = float('inf')\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss', None)\n",
    "        if val_loss is None:\n",
    "            return\n",
    "        \n",
    "        if self.save_best_only:\n",
    "            if val_loss < self.best_val_loss:\n",
    "                if self.verbose:\n",
    "                    print(f'\\nValidation loss improved from {self.best_val_loss:.5f} to {val_loss:.5f}')\n",
    "                self.best_val_loss = val_loss\n",
    "                # Use the model_wrapper reference\n",
    "                self.model_wrapper.save_weights(self.filepath)\n",
    "        else:\n",
    "            self.model_wrapper.save_weights(self.filepath)\n",
    "\n",
    "# Initialize model first\n",
    "hparams_nrms.history_size = HISTORY_SIZE\n",
    "\n",
    "\n",
    "# Best hyperparameters: {'head_num': 19, 'head_dim': 29, 'attention_hidden_dim': 145, 'dropout': 0.22088583494496855, 'learning_rate': 0.00030309205322750723}\n",
    "# From running NoteBook_Hyperparams.ipynb\n",
    "hparams_nrms.head_num = 19\n",
    "hparams_nrms.head_dim = 29\n",
    "hparams_nrms.attention_hidden_dim = 145\n",
    "hparams_nrms.dropout = 0.22088583494496855\n",
    "hparams_nrms.learning_rate = 0.00030309205322750723\n",
    "\n",
    "pytorch_model = NRMSModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "model = NRMSWrapper(pytorch_model)\n",
    "\n",
    "# Then create the callback with the model reference\n",
    "modelcheckpoint = PyTorchModelCheckpoint(\n",
    "    filepath=MODEL_WEIGHTS,\n",
    "    model_wrapper=model,  # Pass the model wrapper\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Training\n",
    "hist = model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=EPOCHS, ### EPOCHS INPUT\n",
    "    callbacks=[modelcheckpoint]\n",
    ")\n",
    "\n",
    "# Load weights using the wrapper\n",
    "model.load_weights(filepath=MODEL_WEIGHTS)\n",
    "\n",
    "# Get predictions\n",
    "pred_validation = model.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE ⛄️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebnerd_from_path(\n",
    "    path: Path,\n",
    "    history_size: int = 30,\n",
    "    padding: int = 0,\n",
    "    user_col: str = DEFAULT_USER_COL,\n",
    "    history_aids_col: str = DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(user_col, history_aids_col)\n",
    "        # .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL,DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL,DEFAULT_HISTORY_READ_TIME_COL) #------------\n",
    "\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=history_aids_col,\n",
    "            history_size=history_size,\n",
    "            padding_value=padding,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=user_col,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL, #--------neu \n",
    "    DEFAULT_HISTORY_READ_TIME_COL, #------- neu\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set-- low tech solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of test data: 1e-06\n"
     ]
    }
   ],
   "source": [
    "# NEW (OLD) CODE\n",
    "\n",
    "import datetime\n",
    "# # COLUMNS = [\n",
    "# #     DEFAULT_USER_COL,\n",
    "# #     DEFAULT_IMPRESSION_ID_COL,\n",
    "# #     DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "# #     DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "# #     DEFAULT_CLICKED_ARTICLES_COL,\n",
    "# #     DEFAULT_INVIEW_ARTICLES_COL,\n",
    "# # ]\n",
    "# df = (\n",
    "#     ebnerd_from_path(\n",
    "#         PATH.joinpath(DATASPLIT, \"train\"),\n",
    "#         history_size=HISTORY_SIZE,\n",
    "#         padding=0,\n",
    "#     )\n",
    "#     .select(COLUMNS)\n",
    "#     .pipe(\n",
    "#         sampling_strategy_wu2019,\n",
    "#         npratio=4,\n",
    "#         shuffle=True,\n",
    "#         with_replacement=True,\n",
    "#         seed=123,\n",
    "#     )\n",
    "#     .pipe(create_binary_labels_column)\n",
    "#     .sample(fraction=FRACTION)\n",
    "# )\n",
    "\n",
    "# dt_split = pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).max() - datetime.timedelta(days=1)\n",
    "# df_train = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) < dt_split)\n",
    "# df_validation = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) >= dt_split)\n",
    "\n",
    "# print(f\"Train samples: {df_train.height}\\nValidation samples: {df_validation.height}\")\n",
    "# df_train.head(2)\n",
    "\n",
    "\n",
    "PATH= Path(\"/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/ebnerd_data\").expanduser()\n",
    "datasplit=\"ebnerd_testset\"\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    # DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "]\n",
    "\n",
    "FRACTION_testset = 0.00001 # TODO 1 for actual run #0.01 for debugging -- or use 0.0001\n",
    "df_test = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(datasplit, \"test\"),\n",
    "        # PATH.joinpath(DATASPLIT, \"validation\"),\n",
    "        history_size=10,\n",
    "        padding=0,\n",
    "    )\n",
    "    .select(COLUMNS)\n",
    "    # .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION_testset)\n",
    ")\n",
    "print(\"Fraction of test data:\", FRACTION_testset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>impression_id</th>\n",
       "      <th>impression_time</th>\n",
       "      <th>article_id_fixed</th>\n",
       "      <th>article_ids_inview</th>\n",
       "      <th>inview_article_times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1028304</td>\n",
       "      <td>229394857</td>\n",
       "      <td>2023-06-01 16:58:18</td>\n",
       "      <td>[9789866, 9790085, 9790085, 9790104, 9790085, ...</td>\n",
       "      <td>[9792504, 9773543, 9788041, 9782800, 9792490, ...</td>\n",
       "      <td>[2023-06-01 16:49:39, 2023-05-20 12:56:01, 202...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  impression_id     impression_time  \\\n",
       "0  1028304      229394857 2023-06-01 16:58:18   \n",
       "\n",
       "                                    article_id_fixed  \\\n",
       "0  [9789866, 9790085, 9790085, 9790104, 9790085, ...   \n",
       "\n",
       "                                  article_ids_inview  \\\n",
       "0  [9792504, 9773543, 9788041, 9782800, 9792490, ...   \n",
       "\n",
       "                                inview_article_times  \n",
       "0  [2023-06-01 16:49:39, 2023-05-20 12:56:01, 202...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'article_ids_clicked'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'article_ids_clicked'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m df_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minview_article_times\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle_ids_inview\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_article_times)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#add new column with the last publish_time for the clicked article\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m df_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclicked_article_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marticle_ids_clicked\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(get_article_times)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Create a function to calculate hour differences\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_hour_differences\u001b[39m(impression_time, article_times):\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# If article_times is a single value (for clicked articles)\u001b[39;00m\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'article_ids_clicked'"
     ]
    }
   ],
   "source": [
    "## do the same as above for validation and training set -- adding new features\n",
    "    #Technically most features dont pass through the dataloader, but is done for consistencu\n",
    "\n",
    "# Convert polars DataFrame to pandas\n",
    "df_test = df_test.to_pandas()\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_time\n",
    "article_time_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"published_time\"\n",
    ").to_dict(as_series=False)\n",
    "article_time_dict = dict(zip(\n",
    "    article_time_dict[\"article_id\"], \n",
    "    article_time_dict[\"published_time\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their timestamps\n",
    "def get_article_times(article_ids):\n",
    "    return [article_time_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the published-time\n",
    "df_test[\"inview_article_times\"] = df_test[\"article_ids_inview\"].apply(get_article_times)\n",
    "\n",
    "#add new column with the last publish_time for the clicked article\n",
    "df_test[\"clicked_article_time\"] = df_test[\"article_ids_clicked\"].apply(get_article_times)\n",
    "\n",
    "# Create a function to calculate hour differences\n",
    "def calculate_hour_differences(impression_time, article_times):\n",
    "        # If article_times is a single value (for clicked articles)\n",
    "    if not isinstance(article_times, list):\n",
    "        if article_times is None:\n",
    "            return None\n",
    "        return (impression_time - article_times).total_seconds() / 3600\n",
    "    \n",
    "    # If article_times is a list (for inview articles)\n",
    "    differences = [(impression_time - article_time).total_seconds() / 3600 \n",
    "                  if article_time is not None else None \n",
    "                  for article_time in article_times]\n",
    "    return differences\n",
    "\n",
    "# Use for inview articles\n",
    "df_test['inview_hour_differences'] = df_test.apply(\n",
    "    lambda row: calculate_hour_differences(row['impression_time'], row['inview_article_times']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# # Use for clicked article -- might be leaky??\n",
    "# df_test['clicked_hour_difference'] = df_test.apply(\n",
    "#    lambda row: calculate_hour_differences(row['impression_time'], row['clicked_article_time']), \n",
    "#    axis=1\n",
    "# )\n",
    "# Create a mapping dictionary from article_id to last_modified_category\n",
    "article_cat_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"category\"\n",
    ").to_dict(as_series=False)\n",
    "article_cat_dict = dict(zip(\n",
    "    article_cat_dict[\"article_id\"], \n",
    "    article_cat_dict[\"category\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their category\n",
    "def get_article_category(article_ids):\n",
    "    return [article_cat_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "#  Add new column with the article category\n",
    "df_test[\"inview_article_categories\"] = df_test[\"article_ids_inview\"].apply(get_article_category)\n",
    "\n",
    "df_test[\"history_article_categories\"] = df_test[\"article_id_fixed\"].apply(get_article_category)\n",
    "\n",
    "# Create a mapping dictionary from article_id to article_type\n",
    "article_type_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"article_type\"\n",
    ").to_dict(as_series=False)\n",
    "article_type_dict = dict(zip(\n",
    "    article_type_dict[\"article_id\"], \n",
    "    article_type_dict[\"article_type\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their type\n",
    "def get_article_type(article_ids):\n",
    "    return [article_type_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the article type\n",
    "df_test[\"inview_article_types\"] = df_test[\"article_ids_inview\"].apply(get_article_type)\n",
    "\n",
    "df_test[\"history_article_types\"] = df_test[\"article_id_fixed\"].apply(get_article_type)\n",
    "\n",
    "\n",
    "#drop columns with the time\n",
    "df_test = df_test.drop(['inview_article_times', 'clicked_article_time','impression_time'], axis=1)\n",
    "\n",
    "\n",
    "df_test = pl.from_pandas(df_test)\n",
    "\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'with_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_290190/2821850579.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Dummy labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'with_columns'"
     ]
    }
   ],
   "source": [
    "df_test = df_test.with_columns(pl.lit(0).alias(\"labels\")) #Dummy labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TEST = 32\n",
    "\n",
    "test_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_test,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started doing predictions on testset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 43/43 [00:09<00:00,  4.78it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Started doing predictions on testset\")\n",
    "pred_test = model.scorer.predict(test_dataloader)\n",
    "df_test = add_prediction_scores(df_test, pred_test.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = MetricEvaluator(\n",
    "#     labels=df_test[\"labels\"].to_list(),\n",
    "#     predictions=df_test[\"scores\"].to_list(),\n",
    "#     metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    "# )\n",
    "# metrics.evaluate() ### IT IS SUPPOSDD TO BE COMMENTED OUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make submissions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>impression_id</th><th>impression_time</th><th>article_id_fixed</th><th>article_ids_inview</th><th>labels</th><th>scores</th><th>ranked_scores</th></tr><tr><td>u32</td><td>u32</td><td>datetime[μs]</td><td>list[i32]</td><td>list[i32]</td><td>i32</td><td>list[f64]</td><td>list[i64]</td></tr></thead><tbody><tr><td>878127</td><td>283087941</td><td>2023-06-02 13:27:26</td><td>[9788547, 9788149, … 9788752]</td><td>[9581248, 9791587, … 9531110]</td><td>0</td><td>[0.015704, 0.087865, … 0.090878]</td><td>[11, 7, … 5]</td></tr><tr><td>2089870</td><td>245509590</td><td>2023-06-03 15:24:41</td><td>[9781932, 9779248, … 9788621]</td><td>[9795964, 9516001, … 9514605]</td><td>0</td><td>[0.069655, 0.063797, … 0.021508]</td><td>[5, 6, … 8]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 8)\n",
       "┌─────────┬─────────────┬─────────────┬────────────┬────────────┬────────┬────────────┬────────────┐\n",
       "│ user_id ┆ impression_ ┆ impression_ ┆ article_id ┆ article_id ┆ labels ┆ scores     ┆ ranked_sco │\n",
       "│ ---     ┆ id          ┆ time        ┆ _fixed     ┆ s_inview   ┆ ---    ┆ ---        ┆ res        │\n",
       "│ u32     ┆ ---         ┆ ---         ┆ ---        ┆ ---        ┆ i32    ┆ list[f64]  ┆ ---        │\n",
       "│         ┆ u32         ┆ datetime[μs ┆ list[i32]  ┆ list[i32]  ┆        ┆            ┆ list[i64]  │\n",
       "│         ┆             ┆ ]           ┆            ┆            ┆        ┆            ┆            │\n",
       "╞═════════╪═════════════╪═════════════╪════════════╪════════════╪════════╪════════════╪════════════╡\n",
       "│ 878127  ┆ 283087941   ┆ 2023-06-02  ┆ [9788547,  ┆ [9581248,  ┆ 0      ┆ [0.015704, ┆ [11, 7, …  │\n",
       "│         ┆             ┆ 13:27:26    ┆ 9788149, … ┆ 9791587, … ┆        ┆ 0.087865,  ┆ 5]         │\n",
       "│         ┆             ┆             ┆ 9788752]   ┆ 9531110]   ┆        ┆ …          ┆            │\n",
       "│         ┆             ┆             ┆            ┆            ┆        ┆ 0.090878]  ┆            │\n",
       "│ 2089870 ┆ 245509590   ┆ 2023-06-03  ┆ [9781932,  ┆ [9795964,  ┆ 0      ┆ [0.069655, ┆ [5, 6, …   │\n",
       "│         ┆             ┆ 15:24:41    ┆ 9779248, … ┆ 9516001, … ┆        ┆ 0.063797,  ┆ 8]         │\n",
       "│         ┆             ┆             ┆ 9788621]   ┆ 9514605]   ┆        ┆ …          ┆            │\n",
       "│         ┆             ┆             ┆            ┆            ┆        ┆ 0.021508]  ┆            │\n",
       "└─────────┴─────────────┴─────────────┴────────────┴────────────┴────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test.with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1353it [00:00, 24416.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping /dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/ebnerd_data/dump_artifacts/predictions.txt to /dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/ebnerd_data/dump_artifacts/predictions_CORRECT_TEST_VERISON.zip\n",
      "Submission file created!\n",
      "DONE\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_submission_file(\n",
    "    impression_ids=df_test[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_test[\"ranked_scores\"],\n",
    "    path=DUMP_DIR.joinpath(\"predictions.txt\"),\n",
    "    filename_zip=f\"predictions_CORRECT_TEST_VERISON.zip\",\n",
    ")\n",
    "print(\"Submission file created!\")\n",
    "print(\"DONE\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas!\n",
      "    *  *  \n",
      "  *  -  - *\n",
      "   / O  O \\ \n",
      "  (    >    )\n",
      "   \\ '===' /\n",
      "   /|     |\\\n",
      "  /_|     |_\\\n"
     ]
    }
   ],
   "source": [
    "print(\"Merry Christmas!\")\n",
    "print(\"    *  *  \")\n",
    "print(\"  *  -  - *\")\n",
    "print(\"   / O  O \\\\ \")\n",
    "print(\"  (    >    )\")\n",
    "print(\"   \\\\ '===' /\")\n",
    "print(\"   /|     |\\\\\")\n",
    "print(\"  /_|     |_\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW SHIT that is shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_FRACITON=0.002\n",
    "# DEFAULT_IS_BEYOND_ACCURACY_COL = \"is_beyond_accuracy\"\n",
    "# from ebrec.utils._polars import split_df_chunks, concat_str_columns\n",
    "\n",
    "# NUM_CHUNKS_TEST = 10\n",
    "# CHUNKS_DONE = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating testset...\n",
      "Initiating testset without beyond-accuracy...\n"
     ]
    }
   ],
   "source": [
    "# # =====================================================================================\n",
    "# print(\"Initiating testset...\")\n",
    "# df_test = (\n",
    "#     ebnerd_from_path(\n",
    "#         PATH.joinpath(\"ebnerd_testset\", \"test\"),\n",
    "#         history_size=HISTORY_SIZE,\n",
    "#         padding=0,\n",
    "#     )\n",
    "#     .sample(fraction=TEST_FRACITON)\n",
    "#     .with_columns(\n",
    "#         pl.col(DEFAULT_INVIEW_ARTICLES_COL)\n",
    "#         .list.first()\n",
    "#         .alias(DEFAULT_CLICKED_ARTICLES_COL)\n",
    "#     )\n",
    "#     .select(COLUMNS + [DEFAULT_IS_BEYOND_ACCURACY_COL])\n",
    "#     .with_columns(\n",
    "#         pl.col(DEFAULT_INVIEW_ARTICLES_COL)\n",
    "#         .list.eval(pl.element() * 0)\n",
    "#         .alias(DEFAULT_LABELS_COL)\n",
    "#     )\n",
    "# )\n",
    "# # Split test in beyond-accuracy TRUE / FALSE. In the BA 'article_ids_inview' is 250.\n",
    "# df_test_wo_beyond = df_test.filter(~pl.col(DEFAULT_IS_BEYOND_ACCURACY_COL))\n",
    "# df_test_w_beyond = df_test.filter(pl.col(DEFAULT_IS_BEYOND_ACCURACY_COL))\n",
    "\n",
    "# df_test_chunks = split_df_chunks(df_test_wo_beyond, n_chunks=NUM_CHUNKS_TEST)\n",
    "# df_pred_test_wo_beyond = []\n",
    "# print(\"Initiating testset without beyond-accuracy...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test_w_beyond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26680, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test_wo_beyond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE_TEST_WO_B = 32\n",
    "# BATCH_SIZE_TEST_W_B = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime as dt\n",
    "# # Dump paths:\n",
    "# DUMP_DIR = Path(\"ebnerd_predictions\")\n",
    "# DUMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "# #\n",
    "# DT_NOW = dt.datetime.now()\n",
    "# #\n",
    "# MODEL_NAME =\"NRMS_MODEL\"\n",
    "# MODEL_OUTPUT_NAME = f\"{MODEL_NAME}-{DT_NOW}\"\n",
    "# #\n",
    "# ARTIFACT_DIR = DUMP_DIR.joinpath(\"test_predictions\", MODEL_OUTPUT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_CHUNKS_DIR = ARTIFACT_DIR.joinpath(\"test_chunks\")\n",
    "# TEST_CHUNKS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chunk: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 84/84 [00:09<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chunk: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 84/84 [00:09<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chunk: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 84/84 [00:09<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chunk: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 84/84 [00:09<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chunk: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 84/84 [00:09<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chunk: 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 84/84 [00:09<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chunk: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 84/84 [00:09<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chunk: 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 84/84 [00:09<00:00,  8.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chunk: 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 84/84 [00:09<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chunk: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 84/84 [00:09<00:00,  8.68it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for i, df_test_chunk in enumerate(df_test_chunks[CHUNKS_DONE:], start=1 + CHUNKS_DONE):\n",
    "#     print(f\"Test chunk: {i}/{len(df_test_chunks)}\")\n",
    "#     # Initialize DataLoader\n",
    "#     test_dataloader_wo_b = NRMSDataLoader(\n",
    "#         behaviors=df_test_chunk,\n",
    "#         article_dict=article_mapping,\n",
    "#         unknown_representation=\"zeros\",\n",
    "#         history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "#         eval_mode=True,\n",
    "#         batch_size=BATCH_SIZE_TEST_WO_B,\n",
    "#     )\n",
    "#     # Predict and clear session\n",
    "#     scores = model.scorer.predict(test_dataloader_wo_b)\n",
    "#     tf.keras.backend.clear_session()\n",
    "\n",
    "#     # Process the predictions\n",
    "#     df_test_chunk = add_prediction_scores(df_test_chunk, scores.tolist()).with_columns(\n",
    "#         pl.col(\"scores\")\n",
    "#         .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "#         .alias(\"ranked_scores\")\n",
    "#     )\n",
    "\n",
    "#     # Save the processed chunk\n",
    "#     df_test_chunk.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "#         TEST_CHUNKS_DIR.joinpath(f\"pred_wo_ba_{i}.parquet\")\n",
    "#     )\n",
    "\n",
    "#     # Append and clean up\n",
    "#     df_pred_test_wo_beyond.append(df_test_chunk)\n",
    "\n",
    "#     # Cleanup\n",
    "#     del df_test_chunk, test_dataloader_wo_b, scores\n",
    "#     gc.collect()\n",
    "\n",
    "# df_pred_test_wo_beyond = pl.concat(df_pred_test_wo_beyond)\n",
    "# df_pred_test_wo_beyond.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "#     TEST_CHUNKS_DIR.joinpath(\"pred_wo_ba.parquet\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating testset without beyond-accuracy...\n"
     ]
    }
   ],
   "source": [
    "# df_test_w_beyond_chunks = split_df_chunks(df_test_w_beyond, n_chunks=NUM_CHUNKS_TEST)\n",
    "# df_pred_test_w_beyond = []\n",
    "# print(\"Initiating testset without beyond-accuracy...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, df_test_w_beyond_chunk in enumerate(df_test_w_beyond_chunks[CHUNKS_DONE:], start=1 + CHUNKS_DONE):\n",
    "#     print(f\"Test chunk: {i}/{len(df_test_w_beyond_chunks)}\")\n",
    "#     # Initialize DataLoader\n",
    "#     test_dataloader_w_b = NRMSDataLoader(\n",
    "#         behaviors=df_test_w_beyond_chunk,\n",
    "#         article_dict=article_mapping,\n",
    "#         unknown_representation=\"zeros\",\n",
    "#         history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "#         eval_mode=True,\n",
    "#         batch_size=BATCH_SIZE_TEST_W_B,\n",
    "#     )\n",
    "#     # Predict and clear session\n",
    "#     scores = model.scorer.predict(test_dataloader_w_b)\n",
    "#     tf.keras.backend.clear_session()\n",
    "\n",
    "#     # Process the predictions\n",
    "#     df_test_w_beyond_chunk = add_prediction_scores(\n",
    "#         df_test_w_beyond_chunk, scores.tolist()\n",
    "#     ).with_columns(\n",
    "#         pl.col(\"scores\")\n",
    "#         .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "#         .alias(\"ranked_scores\")\n",
    "#     )\n",
    "\n",
    "#     # Save the processed chunk\n",
    "#     df_test_w_beyond_chunk.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "#         TEST_CHUNKS_DIR.joinpath(f\"pred_w_ba_{i}.parquet\")\n",
    "#     )\n",
    "\n",
    "#     # Append and clean up\n",
    "#     df_pred_test_w_beyond.append(df_test_w_beyond_chunk)\n",
    "\n",
    "#     # Cleanup\n",
    "#     del df_test_w_beyond_chunk, test_dataloader_w_b, scores\n",
    "#     gc.collect()\n",
    "\n",
    "# df_pred_test_w_beyond = pl.concat(df_pred_test_w_beyond)\n",
    "# df_pred_test_w_beyond.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "#     TEST_CHUNKS_DIR.joinpath(\"pred_w_ba.parquet\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # =====================================================================================\n",
    "# print(\"Initiating testset with beyond-accuracy...\")\n",
    "# test_dataloader_w_b = NRMSDataLoader(\n",
    "#     behaviors=df_test_w_beyond,\n",
    "#     article_dict=article_mapping,\n",
    "#     unknown_representation=\"zeros\",\n",
    "#     history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "#     eval_mode=True,\n",
    "#     batch_size=BATCH_SIZE_TEST_W_B,\n",
    "# )\n",
    "# scores = model.scorer.predict(test_dataloader_w_b)\n",
    "# df_pred_test_w_beyond = add_prediction_scores(\n",
    "#     df_test_w_beyond, scores.tolist()\n",
    "# ).with_columns(\n",
    "#     pl.col(\"scores\")\n",
    "#     .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "#     .alias(\"ranked_scores\")\n",
    "# )\n",
    "# df_pred_test_w_beyond.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "#     TEST_CHUNKS_DIR.joinpath(\"pred_w_ba.parquet\")\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # =====================================================================================\n",
    "# print(\"Saving prediction results...\")\n",
    "# df_test = pl.concat([df_pred_test_wo_beyond, df_pred_test_w_beyond])\n",
    "# df_test.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "#     ARTIFACT_DIR.joinpath(\"test_predictions.parquet\")\n",
    "# )\n",
    "\n",
    "# if TEST_CHUNKS_DIR.exists() and TEST_CHUNKS_DIR.is_dir():\n",
    "#     shutil.rmtree(TEST_CHUNKS_DIR)\n",
    "\n",
    "# write_submission_file(\n",
    "#     impression_ids=df_test[DEFAULT_IMPRESSION_ID_COL],\n",
    "#     prediction_scores=df_test[\"ranked_scores\"],\n",
    "#     path=ARTIFACT_DIR.joinpath(\"predictions.txt\"),\n",
    "#     filename_zip=f\"{MODEL_NAME}-{SEED}-{DATASPLIT}.zip\",\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
