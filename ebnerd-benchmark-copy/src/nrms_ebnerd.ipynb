{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we illustrate how to use the Neural News Recommendation with Multi-Head Self-Attention ([NRMS](https://aclanthology.org/D19-1671/)). The implementation is taken from the [recommenders](https://github.com/recommenders-team/recommenders) repository. We have simply stripped the model to keep it cleaner.\n",
    "\n",
    "We use a small dataset, which is downloaded from [recsys.eb.dk](https://recsys.eb.dk/). All the datasets are stored in the folder path ```~/ebnerd_data/*```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:32:54.372272: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-23 16:32:54.419352: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-23 16:32:54.419387: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-23 16:32:54.419438: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-23 16:32:54.429620: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-23 16:32:55.702032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_ARTICLE_ID_COL\n",
    ")\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "from ebrec.models.newsrec.dataloader import NRMSDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms\n",
    "from ebrec.models.newsrec import NRMSModel, NRMSWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:32:57.693753: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# List all physical devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available devices:\", physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL,DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df_behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate labels\n",
    "We sample a few just to get started. For testset we just make up a dummy column with 0 and 1 - this is not the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/ebnerd_data\").expanduser()\n",
    "DATASPLIT = \"ebnerd_small\"\n",
    "DUMP_DIR = PATH.joinpath(\"dump_artifacts\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we sample the dataset, just to keep it smaller. Also, one can simply add the testset similary to the validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 7)\n",
      "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
      "│ user_id ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ impression_i ┆ impression_t ┆ labels      │\n",
      "│ ---     ┆ ixed         ┆ inview       ┆ clicked      ┆ d            ┆ ime          ┆ ---         │\n",
      "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    │\n",
      "│         ┆ list[i32]    ┆ list[i64]    ┆ list[i64]    ┆ u32          ┆ datetime[μs] ┆             │\n",
      "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╡\n",
      "│ 139836  ┆ [0, 9745590, ┆ [9778728,    ┆ [9778657]    ┆ 149474       ┆ 2023-05-24   ┆ [0, 1, … 0] │\n",
      "│         ┆ … 9765156]   ┆ 9778657, …   ┆              ┆              ┆ 07:47:53     ┆             │\n",
      "│         ┆              ┆ 9778669]     ┆              ┆              ┆              ┆             │\n",
      "│ 143471  ┆ [9767637,    ┆ [9778682,    ┆ [9778623]    ┆ 150528       ┆ 2023-05-24   ┆ [0, 0, … 0] │\n",
      "│         ┆ 9769414, …   ┆ 9778657, …   ┆              ┆              ┆ 07:33:25     ┆             │\n",
      "│         ┆ 9770989]     ┆ 9778669]     ┆              ┆              ┆              ┆             │\n",
      "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┘\n",
      "shape: (2, 7)\n",
      "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
      "│ user_id ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ impression_i ┆ impression_t ┆ labels      │\n",
      "│ ---     ┆ ixed         ┆ inview       ┆ clicked      ┆ d            ┆ ime          ┆ ---         │\n",
      "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    │\n",
      "│         ┆ list[i32]    ┆ list[i32]    ┆ list[i32]    ┆ u32          ┆ datetime[μs] ┆             │\n",
      "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╡\n",
      "│ 22548   ┆ [9773295,    ┆ [9783865,    ┆ [9784696]    ┆ 96791        ┆ 2023-05-28   ┆ [0, 0, … 1] │\n",
      "│         ┆ 9769504, …   ┆ 9784710, …   ┆              ┆              ┆ 04:21:24     ┆             │\n",
      "│         ┆ 9776929]     ┆ 9784696]     ┆              ┆              ┆              ┆             │\n",
      "│ 22548   ┆ [9773295,    ┆ [9784281,    ┆ [9784281]    ┆ 96798        ┆ 2023-05-28   ┆ [1, 0, … 0] │\n",
      "│         ┆ 9769504, …   ┆ 9784583, …   ┆              ┆              ┆ 04:31:48     ┆             │\n",
      "│         ┆ 9776929]     ┆ 9782726]     ┆              ┆              ┆              ┆             │\n",
      "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "]\n",
    "HISTORY_SIZE = 20 #20\n",
    "FRACTION =1  # 0.01\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "print(df_train.head(2))\n",
    "print(df_validation.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var i…</td><td>&quot;Politiet frygt…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den ø…</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars …</td><td>&quot;Biografgængern…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har …</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natascha… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind for…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(DATASPLIT, \"articles.parquet\"))\n",
    "df_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hourly difference between published and viewed article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert polars DataFrame to pandas\n",
    "df_train = df_train.to_pandas()\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_time\n",
    "article_time_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"published_time\"\n",
    ").to_dict(as_series=False)\n",
    "article_time_dict = dict(zip(\n",
    "    article_time_dict[\"article_id\"], \n",
    "    article_time_dict[\"published_time\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their timestamps\n",
    "def get_article_times(article_ids):\n",
    "    return [article_time_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the published-time\n",
    "df_train[\"inview_article_times\"] = df_train[\"article_ids_inview\"].apply(get_article_times)\n",
    "\n",
    "#add new column with the last publish_time for the clicked article\n",
    "df_train[\"clicked_article_time\"] = df_train[\"article_ids_clicked\"].apply(get_article_times)\n",
    "\n",
    "# Create a function to calculate hour differences\n",
    "def calculate_hour_differences(impression_time, article_times):\n",
    "        # If article_times is a single value (for clicked articles)\n",
    "    if not isinstance(article_times, list):\n",
    "        if article_times is None:\n",
    "            return None\n",
    "        return (impression_time - article_times).total_seconds() / 3600\n",
    "    \n",
    "    # If article_times is a list (for inview articles)\n",
    "    differences = [(impression_time - article_time).total_seconds() / 3600 \n",
    "                  if article_time is not None else None \n",
    "                  for article_time in article_times]\n",
    "    return differences\n",
    "\n",
    "# Use for inview articles\n",
    "df_train['inview_hour_differences'] = df_train.apply(\n",
    "    lambda row: calculate_hour_differences(row['impression_time'], row['inview_article_times']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# # Use for clicked article\n",
    "# df_train['clicked_hour_difference'] = df_train.apply(\n",
    "#    lambda row: calculate_hour_differences(row['impression_time'], row['clicked_article_time']), \n",
    "#    axis=1\n",
    "# )\n",
    "\n",
    "\n",
    "#drop columns with the time\n",
    "df_train = df_train.drop(['inview_article_times', 'clicked_article_time','impression_time'], axis=1)\n",
    "\n",
    "df_train = pl.from_pandas(df_train)\n",
    "\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>inview_hour_differences</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td></tr></thead><tbody><tr><td>2559103</td><td>[9778168, 9778102, … 9779071]</td><td>[9779653, 9781354, … 9781535]</td><td>[9780372]</td><td>269179170</td><td>[0, 0, … 0]</td><td>[2.236389, 1.738056, … 1.913889]</td></tr><tr><td>1866066</td><td>[9777000, 9777704, … 9780271]</td><td>[9780874, 9770451, … 9784591]</td><td>[9784607]</td><td>329969502</td><td>[0, 0, … 0]</td><td>[56.234444, 224.853889, … 1.015]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┬──────────────┐\n",
       "│ user_id ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ impression_i ┆ labels      ┆ inview_hour_ │\n",
       "│ ---     ┆ ixed         ┆ inview       ┆ clicked      ┆ d            ┆ ---         ┆ differences  │\n",
       "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    ┆ ---          │\n",
       "│         ┆ list[i32]    ┆ list[i32]    ┆ list[i32]    ┆ u32          ┆             ┆ list[f64]    │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╪══════════════╡\n",
       "│ 2559103 ┆ [9778168,    ┆ [9779653,    ┆ [9780372]    ┆ 269179170    ┆ [0, 0, … 0] ┆ [2.236389,   │\n",
       "│         ┆ 9778102, …   ┆ 9781354, …   ┆              ┆              ┆             ┆ 1.738056, …  │\n",
       "│         ┆ 9779071]     ┆ 9781535]     ┆              ┆              ┆             ┆ 1.913889]    │\n",
       "│ 1866066 ┆ [9777000,    ┆ [9780874,    ┆ [9784607]    ┆ 329969502    ┆ [0, 0, … 0] ┆ [56.234444,  │\n",
       "│         ┆ 9777704, …   ┆ 9770451, …   ┆              ┆              ┆             ┆ 224.853889,  │\n",
       "│         ┆ 9780271]     ┆ 9784591]     ┆              ┆              ┆             ┆ … 1.015]     │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┴──────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert polars DataFrame to pandas\n",
    "df_validation = df_validation.to_pandas()\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_time\n",
    "article_time_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"published_time\"\n",
    ").to_dict(as_series=False)\n",
    "article_time_dict = dict(zip(\n",
    "    article_time_dict[\"article_id\"], \n",
    "    article_time_dict[\"published_time\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their timestamps\n",
    "def get_article_times(article_ids):\n",
    "    return [article_time_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the published-time\n",
    "df_validation[\"inview_article_times\"] = df_validation[\"article_ids_inview\"].apply(get_article_times)\n",
    "\n",
    "#add new column with the last publish_time for the clicked article\n",
    "df_validation[\"clicked_article_time\"] = df_validation[\"article_ids_clicked\"].apply(get_article_times)\n",
    "\n",
    "# Create a function to calculate hour differences\n",
    "def calculate_hour_differences(impression_time, article_times):\n",
    "        # If article_times is a single value (for clicked articles)\n",
    "    if not isinstance(article_times, list):\n",
    "        if article_times is None:\n",
    "            return None\n",
    "        return (impression_time - article_times).total_seconds() / 3600\n",
    "    \n",
    "    # If article_times is a list (for inview articles)\n",
    "    differences = [(impression_time - article_time).total_seconds() / 3600 \n",
    "                  if article_time is not None else None \n",
    "                  for article_time in article_times]\n",
    "    return differences\n",
    "\n",
    "# Use for inview articles\n",
    "df_validation['inview_hour_differences'] = df_validation.apply(\n",
    "    lambda row: calculate_hour_differences(row['impression_time'], row['inview_article_times']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# # Use for clicked article -- might be leaky??\n",
    "# df_validation['clicked_hour_difference'] = df_validation.apply(\n",
    "#    lambda row: calculate_hour_differences(row['impression_time'], row['clicked_article_time']), \n",
    "#    axis=1\n",
    "# )\n",
    "\n",
    "#drop columns with the time\n",
    "df_validation = df_validation.drop(['inview_article_times', 'clicked_article_time','impression_time'], axis=1)\n",
    "\n",
    "\n",
    "df_validation = pl.from_pandas(df_validation)\n",
    "\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model using HuggingFace's tokenizer and wordembedding\n",
    "In the original implementation, they use the GloVe embeddings and tokenizer. To get going fast, we'll use a multilingual LLM from Hugging Face. \n",
    "Utilizing the tokenizer to tokenize the articles and the word-embedding to init NRMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-large\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0562439 , -0.02876282, -0.07940674, ...,  0.02944946,\n",
       "         0.0267334 , -0.02970886],\n",
       "       [-0.0042305 ,  0.0026474 ,  0.0078125 , ..., -0.00432587,\n",
       "         0.00658798, -0.00568771],\n",
       "       [-0.0147171 ,  0.01287842, -0.10516357, ...,  0.11804199,\n",
       "         0.13781738,  0.203125  ],\n",
       "       ...,\n",
       "       [ 0.17944336,  0.1887207 ,  0.24584961, ..., -0.01290894,\n",
       "         0.18457031, -0.15307617],\n",
       "       [ 0.05737305, -0.03430176, -0.07635498, ...,  0.0725708 ,\n",
       "         0.06689453,  0.06781006],\n",
       "       [ 0.02632141,  0.01670837, -0.04385376, ...,  0.01147461,\n",
       "         0.08642578,  0.06854248]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=128,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: []\n"
     ]
    }
   ],
   "source": [
    "# List all physical devices\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available devices:\", physical_devices)\n",
    "# import torch.nn as nn\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# MODEL_NAME = \"NRMS\"\n",
    "# LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "# WEIGHTS_DIR = f\"downloads/data/state_dict/{MODEL_NAME}\"\n",
    "# MODEL_WEIGHTS = f\"{WEIGHTS_DIR}/weights.weights.h5\"\n",
    "\n",
    "# os.makedirs(LOG_DIR, exist_ok=True)\n",
    "# os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# # CALLBACKS\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "# modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=MODEL_WEIGHTS, save_best_only=True, save_weights_only=True, verbose=1\n",
    "# )\n",
    "\n",
    "# # hparams_nrms.history_size = HISTORY_SIZE\n",
    "# # model = NRMSModel(\n",
    "# #     hparams=hparams_nrms,\n",
    "# #     word2vec_embedding=word2vec_embedding,\n",
    "# #     # precomputed_embeddings=article_embeddings,\n",
    "# #     seed=42,\n",
    "# # )\n",
    "# # print(\"starting fitting\")\n",
    "\n",
    "# # hist = model.model.fit(\n",
    "# #     train_dataloader,\n",
    "# #     validation_data=val_dataloader,\n",
    "# #     epochs=1,\n",
    "# #     callbacks=[early_stopping, modelcheckpoint]#tensorboard_callback\n",
    "# # )\n",
    "# # print(\"load weigths\")\n",
    "# # _ = model.model.load_weights(filepath=MODEL_WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from ebrec.models.newsrec.nrmspy import NRMSModel, NRMSWrapper \n",
    "# # import torch\n",
    "# os.makedirs(LOG_DIR, exist_ok=True)\n",
    "# os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# # Create a custom ModelCheckpoint for PyTorch\n",
    "# class PyTorchModelCheckpoint:\n",
    "#     def __init__(self, filepath, save_best_only=True, save_weights_only=True, verbose=1):\n",
    "#         self.filepath = filepath\n",
    "#         self.save_best_only = save_best_only\n",
    "#         self.save_weights_only = save_weights_only\n",
    "#         self.verbose = verbose\n",
    "#         self.best_val_loss = float('inf')\n",
    "    \n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         val_loss = logs.get('val_loss', None)\n",
    "#         if val_loss is None:\n",
    "#             return\n",
    "        \n",
    "#         if self.save_best_only:\n",
    "#             if val_loss < self.best_val_loss:\n",
    "#                 if self.verbose:\n",
    "#                     print(f'\\nValidation loss improved from {self.best_val_loss:.5f} to {val_loss:.5f}')\n",
    "#                 self.best_val_loss = val_loss\n",
    "#                 torch.save(model.model.state_dict(), self.filepath)\n",
    "#         else:\n",
    "#             torch.save(model.model.state_dict(), self.filepath)\n",
    "\n",
    "# # CALLBACKS\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "# modelcheckpoint = PyTorchModelCheckpoint(\n",
    "#     filepath=MODEL_WEIGHTS,\n",
    "#     save_best_only=True,\n",
    "#     save_weights_only=True,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # # Update hyperparameters if needed\n",
    "# # hparams_nrms.head_num = 20  # number of attention heads\n",
    "# # hparams_nrms.head_dim = 20  # dimension of each head (total dimension will be head_num * head_dim)\n",
    "# # hparams_nrms.attention_hidden_dim = hparams_nrms.head_num * hparams_nrms.head_dim\n",
    "# # hparams_nrms.dropout = 0.2\n",
    "# # hparams_nrms.title_size = 30  # should match your MAX_TITLE_LENGTH\n",
    "\n",
    "# # # Initialize model\n",
    "# # hparams_nrms.history_size = HISTORY_SIZE\n",
    "# # pytorch_model = NRMSModel(\n",
    "# #     hparams=hparams_nrms,\n",
    "# #     word2vec_embedding=word2vec_embedding,\n",
    "# #     seed=42,\n",
    "# # )\n",
    "# # model = NRMSWrapper(pytorch_model)\n",
    "\n",
    "# # # Training\n",
    "# # hist = model.fit(\n",
    "# #     train_dataloader,\n",
    "# #     validation_data=val_dataloader,\n",
    "# #     epochs=5,\n",
    "# #     callbacks=[modelcheckpoint]\n",
    "# # )\n",
    "\n",
    "# # # Load weights using the wrapper\n",
    "# # model.load_weights(filepath=MODEL_WEIGHTS)\n",
    "\n",
    "# # # Get predictions\n",
    "# # pred_validation = model.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from ebrec.models.newsrec.nrmspy import NRMSModel, NRMSWrapper  # Add this import\n",
    "\n",
    "# MODEL_NAME = \"NRMS\"\n",
    "# LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "# WEIGHTS_DIR = f\"downloads/data/state_dict/{MODEL_NAME}\"\n",
    "# MODEL_WEIGHTS = f\"{WEIGHTS_DIR}/weights.pt\"\n",
    "\n",
    "# os.makedirs(LOG_DIR, exist_ok=True)\n",
    "# os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# # Create base model\n",
    "# base_model = NRMSModel(\n",
    "#     hparams=hparams_nrms,\n",
    "#     word2vec_embedding=word2vec_embedding,\n",
    "#     seed=42,\n",
    "# )\n",
    "\n",
    "# # Wrap model with NRMSWrapper\n",
    "# model = NRMSWrapper(\n",
    "#     model=base_model,\n",
    "#     device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# )\n",
    "\n",
    "# print(\"starting fitting\")\n",
    "\n",
    "# # Use wrapper's fit method\n",
    "# hist = model.fit(\n",
    "#     train_dataloader,\n",
    "#     validation_data=val_dataloader,\n",
    "#     epochs=1,\n",
    "#     callbacks=[early_stopping, modelcheckpoint]\n",
    "# )\n",
    "\n",
    "# print(\"load weights\")\n",
    "# model.load_weights(filepath=MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from ebrec.models.newsrec.nrmspy import NRMSModel, NRMSWrapper\n",
    "# import torch\n",
    "\n",
    "# # Setup directories\n",
    "# MODEL_NAME = \"NRMS\"\n",
    "# LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "# WEIGHTS_DIR = f\"downloads/data/state_dict/{MODEL_NAME}\"\n",
    "# MODEL_WEIGHTS = f\"{WEIGHTS_DIR}/weights.pt\"\n",
    "\n",
    "# os.makedirs(LOG_DIR, exist_ok=True)\n",
    "# os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# # Create tensorboard writer\n",
    "# writer = SummaryWriter(LOG_DIR)\n",
    "\n",
    "# # Setup ModelCheckpoint\n",
    "# class PyTorchModelCheckpoint:\n",
    "#     def __init__(self, filepath, save_best_only=True, save_weights_only=True, verbose=1):\n",
    "#         self.filepath = filepath\n",
    "#         self.save_best_only = save_best_only\n",
    "#         self.save_weights_only = save_weights_only\n",
    "#         self.verbose = verbose\n",
    "#         self.best_val_loss = float('inf')\n",
    "    \n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         val_loss = logs.get('val_loss', None)\n",
    "#         if val_loss is None:\n",
    "#             return\n",
    "        \n",
    "#         if self.save_best_only:\n",
    "#             if val_loss < self.best_val_loss:\n",
    "#                 if self.verbose:\n",
    "#                     print(f'\\nValidation loss improved from {self.best_val_loss:.5f} to {val_loss:.5f}')\n",
    "#                 self.best_val_loss = val_loss\n",
    "#                 torch.save(model.state_dict(), self.filepath)\n",
    "#         else:\n",
    "#             torch.save(model.state_dict(), self.filepath)\n",
    "\n",
    "# # Initialize callback\n",
    "# modelcheckpoint = PyTorchModelCheckpoint(\n",
    "#     filepath=MODEL_WEIGHTS,\n",
    "#     save_best_only=True,\n",
    "#     save_weights_only=True,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Create model and wrapper\n",
    "# model = NRMSModel(\n",
    "#     hparams=hparams_nrms,\n",
    "#     word2vec_embedding=word2vec_embedding,\n",
    "#     seed=42\n",
    "# )\n",
    "# wrapper = NRMSWrapper(model)\n",
    "\n",
    "# # Train\n",
    "# wrapper.fit(\n",
    "#     train_dataloader,\n",
    "#     validation_data=val_dataloader,\n",
    "#     epochs=5,\n",
    "#     callbacks=[modelcheckpoint],\n",
    "#     writer=writer  # Pass writer to fit method\n",
    "# )\n",
    "\n",
    "# # Close writer\n",
    "# writer.close()\n",
    "\n",
    "# # Load best weights and predict\n",
    "# wrapper.load_weights(MODEL_WEIGHTS)\n",
    "# predictions = wrapper.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create model with Keras-like interface\n",
    "# from ebrec.models.newsrec.nrmspy import NRMSModel, NRMSWrapper, KerasLikeModel\n",
    "\n",
    "# import os\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from ebrec.models.newsrec.nrmspy import NRMSModel, NRMSWrapper\n",
    "# import torch\n",
    "\n",
    "# # Setup directories\n",
    "# MODEL_NAME = \"NRMS\"\n",
    "# LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "# WEIGHTS_DIR = f\"downloads/data/state_dict/{MODEL_NAME}\"\n",
    "# MODEL_WEIGHTS = f\"{WEIGHTS_DIR}/weights.pt\"\n",
    "\n",
    "# os.makedirs(LOG_DIR, exist_ok=True)\n",
    "# os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# # Create tensorboard writer\n",
    "# writer = SummaryWriter(LOG_DIR)\n",
    "\n",
    "# # Setup ModelCheckpoint\n",
    "# class PyTorchModelCheckpoint:\n",
    "#     def __init__(self, filepath, save_best_only=True, save_weights_only=True, verbose=1):\n",
    "#         self.filepath = filepath\n",
    "#         self.save_best_only = save_best_only\n",
    "#         self.save_weights_only = save_weights_only\n",
    "#         self.verbose = verbose\n",
    "#         self.best_val_loss = float('inf')\n",
    "    \n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         val_loss = logs.get('val_loss', None)\n",
    "#         if val_loss is None:\n",
    "#             return\n",
    "        \n",
    "#         if self.save_best_only:\n",
    "#             if val_loss < self.best_val_loss:\n",
    "#                 if self.verbose:\n",
    "#                     print(f'\\nValidation loss improved from {self.best_val_loss:.5f} to {val_loss:.5f}')\n",
    "#                 self.best_val_loss = val_loss\n",
    "#                 torch.save(model.state_dict(), self.filepath)\n",
    "#         else:\n",
    "#             torch.save(model.state_dict(), self.filepath)\n",
    "\n",
    "# # Initialize callback\n",
    "# modelcheckpoint = PyTorchModelCheckpoint(\n",
    "#     filepath=MODEL_WEIGHTS,\n",
    "#     save_best_only=True,\n",
    "#     save_weights_only=True,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# pytorch_model = NRMSModel(hparams=hparams_nrms, word2vec_embedding=word2vec_embedding, seed=42)\n",
    "# keras_like_model = KerasLikeModel(pytorch_model)\n",
    "\n",
    "# # Compile model\n",
    "# keras_like_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# # Train model\n",
    "# keras_like_model.fit(\n",
    "#     train_dataloader,\n",
    "#     validation_data=val_dataloader,\n",
    "#     epochs=5,\n",
    "#     callbacks=[modelcheckpoint]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSWrapper init\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 [Train]: 100%|██████████| 184/184 [00:24<00:00,  7.39it/s, loss=0.4870]\n",
      "Epoch 1/1 [Valid]: 100%|██████████| 383/383 [00:58<00:00,  6.51it/s, loss=0.6787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.4870, Val Loss: 0.3261\n",
      "\n",
      "Validation loss improved from inf to 0.32606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/src/ebrec/models/newsrec/nrmspy_1.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(filepath))\n",
      "Predicting: 100%|██████████| 383/383 [00:58<00:00,  6.52it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "WEIGHTS_DIR = f\"downloads/data/state_dict/{MODEL_NAME}\"\n",
    "MODEL_WEIGHTS = f\"{WEIGHTS_DIR}/weights.pt\"\n",
    "\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# Create a custom ModelCheckpoint for PyTorch\n",
    "class PyTorchModelCheckpoint:\n",
    "    def __init__(self, filepath, model_wrapper=None, save_best_only=True, save_weights_only=True, verbose=1):\n",
    "        self.filepath = filepath\n",
    "        self.model_wrapper = model_wrapper  # Store the model wrapper reference\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.verbose = verbose\n",
    "        self.best_val_loss = float('inf')\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss', None)\n",
    "        if val_loss is None:\n",
    "            return\n",
    "        \n",
    "        if self.save_best_only:\n",
    "            if val_loss < self.best_val_loss:\n",
    "                if self.verbose:\n",
    "                    print(f'\\nValidation loss improved from {self.best_val_loss:.5f} to {val_loss:.5f}')\n",
    "                self.best_val_loss = val_loss\n",
    "                # Use the model_wrapper reference\n",
    "                self.model_wrapper.save_weights(self.filepath)\n",
    "        else:\n",
    "            self.model_wrapper.save_weights(self.filepath)\n",
    "\n",
    "# Initialize model first\n",
    "hparams_nrms.history_size = HISTORY_SIZE\n",
    "pytorch_model = NRMSModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "model = NRMSWrapper(pytorch_model)\n",
    "\n",
    "# Then create the callback with the model reference\n",
    "modelcheckpoint = PyTorchModelCheckpoint(\n",
    "    filepath=MODEL_WEIGHTS,\n",
    "    model_wrapper=model,  # Pass the model wrapper\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Training\n",
    "hist = model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=1,\n",
    "    callbacks=[modelcheckpoint]\n",
    ")\n",
    "\n",
    "# Load weights using the wrapper\n",
    "model.load_weights(filepath=MODEL_WEIGHTS)\n",
    "\n",
    "# Get predictions\n",
    "pred_validation = model.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving / loading model because hpc annoying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_FILE = f\"downloads/models/{MODEL_NAME}.h5\" \n",
    "\n",
    "# # Save the model after training\n",
    "# print(\"Saving the model...\")\n",
    "# os.makedirs(os.path.dirname(MODEL_FILE), exist_ok=True)\n",
    "# model.model.save(MODEL_FILE)  # Save the full model (architecture + weights)\n",
    "# print(f\"Model saved at {MODEL_FILE}\")\n",
    "\n",
    "##LOAD SAVED MODEL\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the saved model\n",
    "# print(f\"Loading the model from {MODEL_FILE}...\")\n",
    "# model.model = load_model(MODEL_FILE)\n",
    "# print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example how to compute some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_validation = model.scorer.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the predictions to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>inview_hour_differences</th><th>scores</th><th>is_known_user</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>list[f64]</td><td>bool</td></tr></thead><tbody><tr><td>2559103</td><td>[9778168, 9778102, … 9779071]</td><td>[9779653, 9781354, … 9781535]</td><td>[9780372]</td><td>269179170</td><td>[0, 0, … 0]</td><td>[2.236389, 1.738056, … 1.913889]</td><td>[0.227807, 0.172969, … 0.218408]</td><td>true</td></tr><tr><td>1866066</td><td>[9777000, 9777704, … 9780271]</td><td>[9780874, 9770451, … 9784591]</td><td>[9784607]</td><td>329969502</td><td>[0, 0, … 0]</td><td>[56.234444, 224.853889, … 1.015]</td><td>[0.230873, 0.334224, … 0.227539]</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 9)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ article_i ┆ article_i ┆ … ┆ labels    ┆ inview_ho ┆ scores    ┆ is_known_ │\n",
       "│ ---     ┆ _fixed     ┆ ds_inview ┆ ds_clicke ┆   ┆ ---       ┆ ur_differ ┆ ---       ┆ user      │\n",
       "│ u32     ┆ ---        ┆ ---       ┆ d         ┆   ┆ list[i8]  ┆ ences     ┆ list[f64] ┆ ---       │\n",
       "│         ┆ list[i32]  ┆ list[i32] ┆ ---       ┆   ┆           ┆ ---       ┆           ┆ bool      │\n",
       "│         ┆            ┆           ┆ list[i32] ┆   ┆           ┆ list[f64] ┆           ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2559103 ┆ [9778168,  ┆ [9779653, ┆ [9780372] ┆ … ┆ [0, 0, …  ┆ [2.236389 ┆ [0.227807 ┆ true      │\n",
       "│         ┆ 9778102, … ┆ 9781354,  ┆           ┆   ┆ 0]        ┆ ,         ┆ ,         ┆           │\n",
       "│         ┆ 9779071]   ┆ …         ┆           ┆   ┆           ┆ 1.738056, ┆ 0.172969, ┆           │\n",
       "│         ┆            ┆ 9781535]  ┆           ┆   ┆           ┆ …         ┆ …         ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 1.913889] ┆ 0.218408] ┆           │\n",
       "│ 1866066 ┆ [9777000,  ┆ [9780874, ┆ [9784607] ┆ … ┆ [0, 0, …  ┆ [56.23444 ┆ [0.230873 ┆ true      │\n",
       "│         ┆ 9777704, … ┆ 9770451,  ┆           ┆   ┆ 0]        ┆ 4, 224.85 ┆ ,         ┆           │\n",
       "│         ┆ 9780271]   ┆ …         ┆           ┆   ┆           ┆ 3889, …   ┆ 0.334224, ┆           │\n",
       "│         ┆            ┆ 9784591]  ┆           ┆   ┆           ┆ 1.015]    ┆ …         ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ 0.227539] ┆           │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "    add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    ")\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.5419201977973713,\n",
       "    \"mrr\": 0.33620641224139075,\n",
       "    \"ndcg@5\": 0.37563557466943853,\n",
       "    \"ndcg@10\": 0.45369826648145395\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>inview_hour_differences</th><th>scores</th><th>is_known_user</th><th>ranked_scores</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>list[f64]</td><td>bool</td><td>list[i64]</td></tr></thead><tbody><tr><td>2559103</td><td>[9778168, 9778102, … 9779071]</td><td>[9779653, 9781354, … 9781535]</td><td>[9780372]</td><td>269179170</td><td>[0, 0, … 0]</td><td>[2.236389, 1.738056, … 1.913889]</td><td>[0.227807, 0.172969, … 0.218408]</td><td>true</td><td>[3, 8, … 4]</td></tr><tr><td>1866066</td><td>[9777000, 9777704, … 9780271]</td><td>[9780874, 9770451, … 9784591]</td><td>[9784607]</td><td>329969502</td><td>[0, 0, … 0]</td><td>[56.234444, 224.853889, … 1.015]</td><td>[0.230873, 0.334224, … 0.227539]</td><td>true</td><td>[4, 1, … 5]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 10)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ article_i ┆ article_i ┆ … ┆ inview_ho ┆ scores    ┆ is_known_ ┆ ranked_sc │\n",
       "│ ---     ┆ _fixed     ┆ ds_inview ┆ ds_clicke ┆   ┆ ur_differ ┆ ---       ┆ user      ┆ ores      │\n",
       "│ u32     ┆ ---        ┆ ---       ┆ d         ┆   ┆ ences     ┆ list[f64] ┆ ---       ┆ ---       │\n",
       "│         ┆ list[i32]  ┆ list[i32] ┆ ---       ┆   ┆ ---       ┆           ┆ bool      ┆ list[i64] │\n",
       "│         ┆            ┆           ┆ list[i32] ┆   ┆ list[f64] ┆           ┆           ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2559103 ┆ [9778168,  ┆ [9779653, ┆ [9780372] ┆ … ┆ [2.236389 ┆ [0.227807 ┆ true      ┆ [3, 8, …  │\n",
       "│         ┆ 9778102, … ┆ 9781354,  ┆           ┆   ┆ ,         ┆ ,         ┆           ┆ 4]        │\n",
       "│         ┆ 9779071]   ┆ …         ┆           ┆   ┆ 1.738056, ┆ 0.172969, ┆           ┆           │\n",
       "│         ┆            ┆ 9781535]  ┆           ┆   ┆ …         ┆ …         ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆ 1.913889] ┆ 0.218408] ┆           ┆           │\n",
       "│ 1866066 ┆ [9777000,  ┆ [9780874, ┆ [9784607] ┆ … ┆ [56.23444 ┆ [0.230873 ┆ true      ┆ [4, 1, …  │\n",
       "│         ┆ 9777704, … ┆ 9770451,  ┆           ┆   ┆ 4, 224.85 ┆ ,         ┆           ┆ 5]        │\n",
       "│         ┆ 9780271]   ┆ …         ┆           ┆   ┆ 3889, …   ┆ 0.334224, ┆           ┆           │\n",
       "│         ┆            ┆ 9784591]  ┆           ┆   ┆ 1.015]    ┆ …         ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 0.227539] ┆           ┆           │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation = df_validation.with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the validation, simply add the testset to your flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24464it [00:00, 28394.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/predictions.txt to downloads/predictions.zip\n"
     ]
    }
   ],
   "source": [
    "write_submission_file(\n",
    "    impression_ids=df_validation[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_validation[\"ranked_scores\"],\n",
    "    path=\"downloads/predictions.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
