{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we illustrate how to use the Neural News Recommendation with Multi-Head Self-Attention ([NRMS](https://aclanthology.org/D19-1671/)). The implementation is taken from the [recommenders](https://github.com/recommenders-team/recommenders) repository. We have simply stripped the model to keep it cleaner.\n",
    "\n",
    "We use a small dataset, which is downloaded from [recsys.eb.dk](https://recsys.eb.dk/). All the datasets are stored in the folder path ```~/ebnerd_data/*```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 14:58:22.588786: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-02 14:58:22.592871: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-02 14:58:22.640459: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-02 14:58:22.640491: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-02 14:58:22.640525: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-02 14:58:22.650008: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-02 14:58:23.831138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_ARTICLE_ID_COL,\n",
    "    DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL, #-------\n",
    "    DEFAULT_HISTORY_READ_TIME_COL #-------\n",
    ")\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "from ebrec.models.newsrec.dataloader import NRMSDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms\n",
    "from ebrec.models.newsrec import NRMSModel, NRMSWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 14:58:25.710630: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# List all physical devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available devices:\", physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make gridsearch for hyper make arguments TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        # .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL,DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL)\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL,DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL,DEFAULT_HISTORY_READ_TIME_COL) #------------\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df_behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate labels\n",
    "We sample a few just to get started. For testset we just make up a dummy column with 0 and 1 - this is not the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/ebnerd_data\").expanduser()\n",
    "DATASPLIT = \"ebnerd_small\" # REMEMBER if change to change make_embedding_artifacts.ipynb file (embeddings)\n",
    "# DATASPLIT = \"ebnerd__testset\"\n",
    "DUMP_DIR = PATH.joinpath(\"dump_artifacts\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we sample the dataset, just to keep it smaller. Also, one can simply add the testset similary to the validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 9)\n",
      "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ article_i ┆ impressio ┆ impressio ┆ labels    │\n",
      "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ds_clicke ┆ n_id      ┆ n_time    ┆ ---       │\n",
      "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ d         ┆ ---       ┆ ---       ┆ list[i8]  │\n",
      "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ u32       ┆ datetime[ ┆           │\n",
      "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i64] ┆           ┆ μs]       ┆           │\n",
      "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 1103214 ┆ [9762520,  ┆ [100.0,   ┆ [54.0,    ┆ … ┆ [9774541] ┆ 42346545  ┆ 2023-05-2 ┆ [0, 1, …  │\n",
      "│         ┆ 9761561, … ┆ 34.0, …   ┆ 7.0, …    ┆   ┆           ┆           ┆ 2         ┆ 0]        │\n",
      "│         ┆ 9769197]   ┆ 53.0]     ┆ 3.0]      ┆   ┆           ┆           ┆ 09:28:43  ┆           │\n",
      "│ 2497899 ┆ [9769828,  ┆ [20.0,    ┆ [5.0,     ┆ … ┆ [9779269] ┆ 24814465  ┆ 2023-05-2 ┆ [0, 0, …  │\n",
      "│         ┆ 9769518, … ┆ 88.0, …   ┆ 10.0, …   ┆   ┆           ┆           ┆ 4         ┆ 0]        │\n",
      "│         ┆ 9770594]   ┆ 85.0]     ┆ 382.0]    ┆   ┆           ┆           ┆ 14:35:14  ┆           │\n",
      "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n",
      "shape: (2, 9)\n",
      "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ article_i ┆ impressio ┆ impressio ┆ labels    │\n",
      "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ds_clicke ┆ n_id      ┆ n_time    ┆ ---       │\n",
      "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ d         ┆ ---       ┆ ---       ┆ list[i8]  │\n",
      "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ u32       ┆ datetime[ ┆           │\n",
      "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i32] ┆           ┆ μs]       ┆           │\n",
      "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 1581880 ┆ [9775985,  ┆ [100.0,   ┆ [42.0,    ┆ … ┆ [9777846] ┆ 184836170 ┆ 2023-05-2 ┆ [0, 0, …  │\n",
      "│         ┆ 9777374, … ┆ 100.0, …  ┆ 62.0, …   ┆   ┆           ┆           ┆ 5         ┆ 1]        │\n",
      "│         ┆ 9779738]   ┆ null]     ┆ 0.0]      ┆   ┆           ┆           ┆ 08:12:54  ┆           │\n",
      "│ 401400  ┆ [9759955,  ┆ [28.0,    ┆ [11.0,    ┆ … ┆ [9787230] ┆ 214315254 ┆ 2023-05-3 ┆ [0, 0, …  │\n",
      "│         ┆ 9776897, … ┆ 14.0, …   ┆ 4.0, …    ┆   ┆           ┆           ┆ 0         ┆ 0]        │\n",
      "│         ┆ 9780096]   ┆ 53.0]     ┆ 7.0]      ┆   ┆           ┆           ┆ 04:39:26  ┆           │\n",
      "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL, #--------neu \n",
    "    DEFAULT_HISTORY_READ_TIME_COL, #------- neu\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    \n",
    "]\n",
    "HISTORY_SIZE = 30 #30\n",
    "FRACTION = 0.03 #Fraction af datasæt\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=6,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "print(df_train.head(2))\n",
    "print(df_validation.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var i…</td><td>&quot;Politiet frygt…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den ø…</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars …</td><td>&quot;Biografgængern…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har …</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natascha… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind for…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(DATASPLIT, \"articles.parquet\"))\n",
    "df_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Added features and hourly difference between published and viewed article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>scroll_percentage_fixed</th><th>read_time_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>inview_hour_differences</th><th>inview_article_categories</th><th>history_article_categories</th><th>inview_article_types</th><th>history_article_types</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[f32]</td><td>list[f32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>list[i64]</td><td>list[i64]</td><td>list[str]</td><td>list[str]</td></tr></thead><tbody><tr><td>1103214</td><td>[9762520, 9761561, … 9769197]</td><td>[100.0, 34.0, … 53.0]</td><td>[54.0, 7.0, … 3.0]</td><td>[9774429, 9774541, … 9771355]</td><td>[9774541]</td><td>42346545</td><td>[0, 1, … 0]</td><td>[0.624722, 1.684167, … 0.644444]</td><td>[142, 118, … 118]</td><td>[457, 512, … 140]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td></tr><tr><td>2497899</td><td>[9769828, 9769518, … 9770594]</td><td>[20.0, 88.0, … 85.0]</td><td>[5.0, 10.0, … 382.0]</td><td>[9779285, 9779285, … 9779285]</td><td>[9779269]</td><td>24814465</td><td>[0, 0, … 0]</td><td>[0.741944, 0.741944, … 0.741944]</td><td>[414, 414, … 414]</td><td>[118, 118, … 118]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 13)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ inview_ar ┆ history_a ┆ inview_ar ┆ history_a │\n",
       "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ticle_cat ┆ rticle_ca ┆ ticle_typ ┆ rticle_ty │\n",
       "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ egories   ┆ tegories  ┆ es        ┆ pes       │\n",
       "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i64] ┆ list[i64] ┆ list[str] ┆ list[str] │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 1103214 ┆ [9762520,  ┆ [100.0,   ┆ [54.0,    ┆ … ┆ [142,     ┆ [457,     ┆ [\"article ┆ [\"article │\n",
       "│         ┆ 9761561, … ┆ 34.0, …   ┆ 7.0, …    ┆   ┆ 118, …    ┆ 512, …    ┆ _default\" ┆ _default\" │\n",
       "│         ┆ 9769197]   ┆ 53.0]     ┆ 3.0]      ┆   ┆ 118]      ┆ 140]      ┆ , \"articl ┆ , \"articl │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ e_def…    ┆ e_def…    │\n",
       "│ 2497899 ┆ [9769828,  ┆ [20.0,    ┆ [5.0,     ┆ … ┆ [414,     ┆ [118,     ┆ [\"article ┆ [\"article │\n",
       "│         ┆ 9769518, … ┆ 88.0, …   ┆ 10.0, …   ┆   ┆ 414, …    ┆ 118, …    ┆ _default\" ┆ _default\" │\n",
       "│         ┆ 9770594]   ┆ 85.0]     ┆ 382.0]    ┆   ┆ 414]      ┆ 118]      ┆ , \"articl ┆ , \"articl │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ e_def…    ┆ e_def…    │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NEW\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert polars DataFrame to pandas\n",
    "df_train = df_train.to_pandas()\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_time\n",
    "article_time_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"published_time\"\n",
    ").to_dict(as_series=False)\n",
    "article_time_dict = dict(zip(\n",
    "    article_time_dict[\"article_id\"], \n",
    "    article_time_dict[\"published_time\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their timestamps\n",
    "def get_article_times(article_ids):\n",
    "    return [article_time_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the published-time\n",
    "df_train[\"inview_article_times\"] = df_train[\"article_ids_inview\"].apply(get_article_times)\n",
    "\n",
    "#add new column with the last publish_time for the clicked article\n",
    "df_train[\"clicked_article_time\"] = df_train[\"article_ids_clicked\"].apply(get_article_times)\n",
    "\n",
    "# Create a function to calculate hour differences\n",
    "def calculate_hour_differences(impression_time, article_times):\n",
    "        # If article_times is a single value (for clicked articles)\n",
    "    if not isinstance(article_times, list):\n",
    "        if article_times is None:\n",
    "            return None\n",
    "        return (impression_time - article_times).total_seconds() / 3600\n",
    "    \n",
    "    # If article_times is a list (for inview articles)\n",
    "    differences = [(impression_time - article_time).total_seconds() / 3600 \n",
    "                  if article_time is not None else None \n",
    "                  for article_time in article_times]\n",
    "    return differences\n",
    "\n",
    "# Use for inview articles\n",
    "df_train['inview_hour_differences'] = df_train.apply(\n",
    "    lambda row: calculate_hour_differences(row['impression_time'], row['inview_article_times']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# # Use for clicked article\n",
    "# df_train['clicked_hour_difference'] = df_train.apply(\n",
    "#    lambda row: calculate_hour_differences(row['impression_time'], row['clicked_article_time']), \n",
    "#    axis=1\n",
    "# )\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_category\n",
    "article_cat_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"category\"\n",
    ").to_dict(as_series=False)\n",
    "article_cat_dict = dict(zip(\n",
    "    article_cat_dict[\"article_id\"], \n",
    "    article_cat_dict[\"category\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their category\n",
    "def get_article_category(article_ids):\n",
    "    return [article_cat_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "#  Add new column with the article category\n",
    "df_train[\"inview_article_categories\"] = df_train[\"article_ids_inview\"].apply(get_article_category)\n",
    "\n",
    "df_train[\"history_article_categories\"] = df_train[\"article_id_fixed\"].apply(get_article_category)\n",
    "\n",
    "# Create a mapping dictionary from article_id to article_type\n",
    "article_type_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"article_type\"\n",
    ").to_dict(as_series=False)\n",
    "article_type_dict = dict(zip(\n",
    "    article_type_dict[\"article_id\"], \n",
    "    article_type_dict[\"article_type\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their type\n",
    "def get_article_type(article_ids):\n",
    "    return [article_type_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the article type\n",
    "df_train[\"inview_article_types\"] = df_train[\"article_ids_inview\"].apply(get_article_type)\n",
    "\n",
    "df_train[\"history_article_types\"] = df_train[\"article_id_fixed\"].apply(get_article_type)\n",
    "\n",
    "#drop columns with the time\n",
    "df_train = df_train.drop(['inview_article_times', 'clicked_article_time','impression_time'], axis=1)\n",
    "\n",
    "df_train = pl.from_pandas(df_train)\n",
    "\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>scroll_percentage_fixed</th><th>read_time_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>inview_hour_differences</th><th>inview_article_categories</th><th>history_article_categories</th><th>inview_article_types</th><th>history_article_types</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[f32]</td><td>list[f32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>list[i64]</td><td>list[i64]</td><td>list[str]</td><td>list[str]</td></tr></thead><tbody><tr><td>1581880</td><td>[9775985, 9777374, … 9779738]</td><td>[100.0, 100.0, … null]</td><td>[42.0, 62.0, … 0.0]</td><td>[9776315, 9772363, … 9777846]</td><td>[9777846]</td><td>184836170</td><td>[0, 0, … 1]</td><td>[64.012222, 141.3025, … 0.128889]</td><td>[2975, 2975, … 140]</td><td>[414, 142, … 414]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td></tr><tr><td>401400</td><td>[9759955, 9776897, … 9780096]</td><td>[28.0, 14.0, … 53.0]</td><td>[11.0, 4.0, … 7.0]</td><td>[9780460, 9553264, … 9506503]</td><td>[9787230]</td><td>214315254</td><td>[0, 0, … 0]</td><td>[11.757778, 3883.658056, … 4795.724444]</td><td>[140, 457, … 118]</td><td>[414, 118, … 118]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[&quot;article_default&quot;, &quot;article_scribblelive&quot;, … &quot;article_default&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 13)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ inview_ar ┆ history_a ┆ inview_ar ┆ history_a │\n",
       "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ticle_cat ┆ rticle_ca ┆ ticle_typ ┆ rticle_ty │\n",
       "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ egories   ┆ tegories  ┆ es        ┆ pes       │\n",
       "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i64] ┆ list[i64] ┆ list[str] ┆ list[str] │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 1581880 ┆ [9775985,  ┆ [100.0,   ┆ [42.0,    ┆ … ┆ [2975,    ┆ [414,     ┆ [\"article ┆ [\"article │\n",
       "│         ┆ 9777374, … ┆ 100.0, …  ┆ 62.0, …   ┆   ┆ 2975, …   ┆ 142, …    ┆ _default\" ┆ _default\" │\n",
       "│         ┆ 9779738]   ┆ null]     ┆ 0.0]      ┆   ┆ 140]      ┆ 414]      ┆ , \"articl ┆ , \"articl │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ e_def…    ┆ e_def…    │\n",
       "│ 401400  ┆ [9759955,  ┆ [28.0,    ┆ [11.0,    ┆ … ┆ [140,     ┆ [414,     ┆ [\"article ┆ [\"article │\n",
       "│         ┆ 9776897, … ┆ 14.0, …   ┆ 4.0, …    ┆   ┆ 457, …    ┆ 118, …    ┆ _default\" ┆ _default\" │\n",
       "│         ┆ 9780096]   ┆ 53.0]     ┆ 7.0]      ┆   ┆ 118]      ┆ 118]      ┆ , \"articl ┆ , \"articl │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ e_def…    ┆ e_scr…    │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert polars DataFrame to pandas\n",
    "df_validation = df_validation.to_pandas()\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_time\n",
    "article_time_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"published_time\"\n",
    ").to_dict(as_series=False)\n",
    "article_time_dict = dict(zip(\n",
    "    article_time_dict[\"article_id\"], \n",
    "    article_time_dict[\"published_time\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their timestamps\n",
    "def get_article_times(article_ids):\n",
    "    return [article_time_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the published-time\n",
    "df_validation[\"inview_article_times\"] = df_validation[\"article_ids_inview\"].apply(get_article_times)\n",
    "\n",
    "#add new column with the last publish_time for the clicked article\n",
    "df_validation[\"clicked_article_time\"] = df_validation[\"article_ids_clicked\"].apply(get_article_times)\n",
    "\n",
    "# Create a function to calculate hour differences\n",
    "def calculate_hour_differences(impression_time, article_times):\n",
    "        # If article_times is a single value (for clicked articles)\n",
    "    if not isinstance(article_times, list):\n",
    "        if article_times is None:\n",
    "            return None\n",
    "        return (impression_time - article_times).total_seconds() / 3600\n",
    "    \n",
    "    # If article_times is a list (for inview articles)\n",
    "    differences = [(impression_time - article_time).total_seconds() / 3600 \n",
    "                  if article_time is not None else None \n",
    "                  for article_time in article_times]\n",
    "    return differences\n",
    "\n",
    "# Use for inview articles\n",
    "df_validation['inview_hour_differences'] = df_validation.apply(\n",
    "    lambda row: calculate_hour_differences(row['impression_time'], row['inview_article_times']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# # Use for clicked article -- might be leaky??\n",
    "# df_validation['clicked_hour_difference'] = df_validation.apply(\n",
    "#    lambda row: calculate_hour_differences(row['impression_time'], row['clicked_article_time']), \n",
    "#    axis=1\n",
    "# )\n",
    "# Create a mapping dictionary from article_id to last_modified_category\n",
    "article_cat_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"category\"\n",
    ").to_dict(as_series=False)\n",
    "article_cat_dict = dict(zip(\n",
    "    article_cat_dict[\"article_id\"], \n",
    "    article_cat_dict[\"category\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their category\n",
    "def get_article_category(article_ids):\n",
    "    return [article_cat_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "#  Add new column with the article category\n",
    "df_validation[\"inview_article_categories\"] = df_validation[\"article_ids_inview\"].apply(get_article_category)\n",
    "\n",
    "df_validation[\"history_article_categories\"] = df_validation[\"article_id_fixed\"].apply(get_article_category)\n",
    "\n",
    "# Create a mapping dictionary from article_id to article_type\n",
    "article_type_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"article_type\"\n",
    ").to_dict(as_series=False)\n",
    "article_type_dict = dict(zip(\n",
    "    article_type_dict[\"article_id\"], \n",
    "    article_type_dict[\"article_type\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their type\n",
    "def get_article_type(article_ids):\n",
    "    return [article_type_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the article type\n",
    "df_validation[\"inview_article_types\"] = df_validation[\"article_ids_inview\"].apply(get_article_type)\n",
    "\n",
    "df_validation[\"history_article_types\"] = df_validation[\"article_id_fixed\"].apply(get_article_type)\n",
    "\n",
    "\n",
    "#drop columns with the time\n",
    "df_validation = df_validation.drop(['inview_article_times', 'clicked_article_time','impression_time'], axis=1)\n",
    "\n",
    "\n",
    "df_validation = pl.from_pandas(df_validation)\n",
    "\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model using HuggingFace's tokenizer and wordembedding\n",
    "In the original implementation, they use the GloVe embeddings and tokenizer. To get going fast, we'll use a multilingual LLM from Hugging Face. \n",
    "Utilizing the tokenizer to tokenize the articles and the word-embedding to init NRMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "# TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-large\"\n",
    "TRANSFORMER_MODEL_NAME = \"Maltehb/danish-bert-botxo\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30 #hardcoded somewhere ?? error if change\n",
    "\n",
    "\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "\n",
    "\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01057525,  0.0519704 ,  0.08909235, ..., -0.00795781,\n",
       "        -0.06168545, -0.07079539],\n",
       "       [-0.01835794,  0.04070301,  0.02630469, ..., -0.00612215,\n",
       "        -0.03679245, -0.00261144],\n",
       "       [-0.01904276,  0.02300256, -0.00536503, ...,  0.00180998,\n",
       "         0.01913669, -0.00572065],\n",
       "       ...,\n",
       "       [ 0.02969794, -0.02969835,  0.0127237 , ..., -0.0130282 ,\n",
       "        -0.00069379,  0.004221  ],\n",
       "       [ 0.03114044, -0.03700501,  0.01400322, ..., -0.00791059,\n",
       "         0.00770514, -0.00168254],\n",
       "       [ 0.0367507 , -0.0307173 ,  0.00670483, ..., -0.01460291,\n",
       "         0.00015374, -0.00201466]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=128,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all physical devices\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# print(\"Available devices:\", physical_devices)\n",
    "# import torch.nn as nn\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSWrapper init\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 [Train]:   0%|          | 0/55 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 338.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 253.75 MiB is free. Process 23564 has 1.36 GiB memory in use. Process 275171 has 6.24 GiB memory in use. Process 377164 has 448.00 MiB memory in use. Process 389821 has 24.73 GiB memory in use. Process 423385 has 944.00 MiB memory in use. Process 509202 has 982.00 MiB memory in use. Process 517287 has 1.10 GiB memory in use. Process 602174 has 982.00 MiB memory in use. Process 850463 has 788.00 MiB memory in use. Including non-PyTorch memory, this process has 1.01 GiB memory in use. Process 861832 has 708.00 MiB memory in use. Of the allocated memory 523.54 MiB is allocated by PyTorch, and 22.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 57\u001b[0m\n\u001b[1;32m     48\u001b[0m modelcheckpoint \u001b[38;5;241m=\u001b[39m PyTorchModelCheckpoint(\n\u001b[1;32m     49\u001b[0m     filepath\u001b[38;5;241m=\u001b[39mMODEL_WEIGHTS,\n\u001b[1;32m     50\u001b[0m     model_wrapper\u001b[38;5;241m=\u001b[39mmodel,  \u001b[38;5;66;03m# Pass the model wrapper\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodelcheckpoint\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Load weights using the wrapper\u001b[39;00m\n\u001b[1;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(filepath\u001b[38;5;241m=\u001b[39mMODEL_WEIGHTS)\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/src/ebrec/models/newsrec/nrmspy_1.py:79\u001b[0m, in \u001b[0;36mNRMSWrapper.fit\u001b[0;34m(self, train_dataloader, validation_data, epochs, callbacks)\u001b[0m\n\u001b[1;32m     76\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(labels)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 79\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, labels)\n\u001b[1;32m     82\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/src/ebrec/models/newsrec/nrmspy_1.py:263\u001b[0m, in \u001b[0;36mNRMSModel.forward\u001b[0;34m(self, his_input_title, pred_input_title, training)\u001b[0m\n\u001b[1;32m    261\u001b[0m his_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword2vec_embedding(his_input_title)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m--> 263\u001b[0m     his_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhis_embedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m his_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewsencoder(his_embedded)\n\u001b[1;32m    265\u001b[0m his_encoded \u001b[38;5;241m=\u001b[39m his_encoded\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mhistory_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/torch/nn/functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 338.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 253.75 MiB is free. Process 23564 has 1.36 GiB memory in use. Process 275171 has 6.24 GiB memory in use. Process 377164 has 448.00 MiB memory in use. Process 389821 has 24.73 GiB memory in use. Process 423385 has 944.00 MiB memory in use. Process 509202 has 982.00 MiB memory in use. Process 517287 has 1.10 GiB memory in use. Process 602174 has 982.00 MiB memory in use. Process 850463 has 788.00 MiB memory in use. Including non-PyTorch memory, this process has 1.01 GiB memory in use. Process 861832 has 708.00 MiB memory in use. Of the allocated memory 523.54 MiB is allocated by PyTorch, and 22.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "WEIGHTS_DIR = f\"downloads/data/state_dict/{MODEL_NAME}\"\n",
    "MODEL_WEIGHTS = f\"{WEIGHTS_DIR}/weights.pt\"\n",
    "\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# Create a custom ModelCheckpoint for PyTorch\n",
    "class PyTorchModelCheckpoint:\n",
    "    def __init__(self, filepath, model_wrapper=None, save_best_only=True, save_weights_only=True, verbose=1):\n",
    "        self.filepath = filepath\n",
    "        self.model_wrapper = model_wrapper  # Store the model wrapper reference\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.verbose = verbose\n",
    "        self.best_val_loss = float('inf')\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss', None)\n",
    "        if val_loss is None:\n",
    "            return\n",
    "        \n",
    "        if self.save_best_only:\n",
    "            if val_loss < self.best_val_loss:\n",
    "                if self.verbose:\n",
    "                    print(f'\\nValidation loss improved from {self.best_val_loss:.5f} to {val_loss:.5f}')\n",
    "                self.best_val_loss = val_loss\n",
    "                # Use the model_wrapper reference\n",
    "                self.model_wrapper.save_weights(self.filepath)\n",
    "        else:\n",
    "            self.model_wrapper.save_weights(self.filepath)\n",
    "\n",
    "# Initialize model first\n",
    "hparams_nrms.history_size = HISTORY_SIZE\n",
    "pytorch_model = NRMSModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "model = NRMSWrapper(pytorch_model)\n",
    "\n",
    "# Then create the callback with the model reference\n",
    "modelcheckpoint = PyTorchModelCheckpoint(\n",
    "    filepath=MODEL_WEIGHTS,\n",
    "    model_wrapper=model,  # Pass the model wrapper\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Training\n",
    "hist = model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=1,\n",
    "    callbacks=[modelcheckpoint]\n",
    ")\n",
    "\n",
    "# Load weights using the wrapper\n",
    "model.load_weights(filepath=MODEL_WEIGHTS)\n",
    "\n",
    "# Get predictions\n",
    "pred_validation = model.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Attempt at new\n",
    "\n",
    "# import os\n",
    "# from tqdm.notebook import tqdm\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# # Define constants\n",
    "# MODEL_NAME = \"NRMS\"\n",
    "# LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "# WEIGHTS_DIR = f\"downloads/data/state_dict/{MODEL_NAME}\"\n",
    "# MODEL_WEIGHTS = f\"{WEIGHTS_DIR}/weights.pt\"\n",
    "# TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-large\"\n",
    "\n",
    "# # Create necessary directories\n",
    "# os.makedirs(LOG_DIR, exist_ok=True)\n",
    "# os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# # Load transformer model and tokenizer\n",
    "# transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "# transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# # Create a custom ModelCheckpoint for PyTorch\n",
    "# class PyTorchModelCheckpoint:\n",
    "#     def __init__(self, filepath, model_wrapper=None, save_best_only=True, save_weights_only=True, verbose=1):\n",
    "#         self.filepath = filepath\n",
    "#         self.model_wrapper = model_wrapper  # Store the model wrapper reference\n",
    "#         self.save_best_only = save_best_only\n",
    "#         self.save_weights_only = save_weights_only\n",
    "#         self.verbose = verbose\n",
    "#         self.best_val_loss = float('inf')\n",
    "    \n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         val_loss = logs.get('val_loss', None)\n",
    "#         if val_loss is None:\n",
    "#             return\n",
    "        \n",
    "#         if self.save_best_only:\n",
    "#             if val_loss < self.best_val_loss:\n",
    "#                 if self.verbose:\n",
    "#                     print(f'\\nValidation loss improved from {self.best_val_loss:.5f} to {val_loss:.5f}')\n",
    "#                 self.best_val_loss = val_loss\n",
    "#                 # Use the model_wrapper reference\n",
    "#                 self.model_wrapper.save_weights(self.filepath)\n",
    "#         else:\n",
    "#             self.model_wrapper.save_weights(self.filepath)\n",
    "\n",
    "# # Define model hyperparameters\n",
    "# hparams_nrms = {\n",
    "#     \"head_num\": 8,\n",
    "#     \"head_dim\": 64,\n",
    "#     \"title_size\": MAX_TITLE_LENGTH,\n",
    "#     \"history_size\": HISTORY_SIZE,\n",
    "#     \"dropout\": 0.2 #NOT USED\n",
    "# } # from gpt\n",
    "\n",
    "# # init NRMS model\n",
    "# pytorch_model = NRMSModel(\n",
    "#     hparams=hparams_nrms,\n",
    "#     transformer_model=transformer_model,\n",
    "#     transformer_tokenizer=transformer_tokenizer,\n",
    "#     seed=42\n",
    "# )\n",
    "# model = NRMSWrapper(pytorch_model)\n",
    "\n",
    "# # Create the checkpoint callback\n",
    "# modelcheckpoint = PyTorchModelCheckpoint(\n",
    "#     filepath=MODEL_WEIGHTS,\n",
    "#     model_wrapper=model,  # Pass the model wrapper\n",
    "#     save_best_only=True,\n",
    "#     save_weights_only=True,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # fit\n",
    "# hist = model.fit(\n",
    "#     train_dataloader,\n",
    "#     validation_data=val_dataloader,\n",
    "#     epochs=1,\n",
    "#     callbacks=[modelcheckpoint]\n",
    "# )\n",
    "\n",
    "# # Load weights using the wrapper\n",
    "# model.load_weights(filepath=MODEL_WEIGHTS)\n",
    "\n",
    "# # Get predictions\n",
    "# pred_validation = model.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving / loading model because hpc annoying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_FILE = f\"downloads/models/{MODEL_NAME}.h5\" \n",
    "\n",
    "# # Save the model after training\n",
    "# print(\"Saving the model...\")\n",
    "# os.makedirs(os.path.dirname(MODEL_FILE), exist_ok=True)\n",
    "# model.model.save(MODEL_FILE)  # Save the full model (architecture + weights)\n",
    "# print(f\"Model saved at {MODEL_FILE}\")\n",
    "\n",
    "##LOAD SAVED MODEL\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the saved model\n",
    "# print(f\"Loading the model from {MODEL_FILE}...\")\n",
    "# model.model = load_model(MODEL_FILE)\n",
    "# print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example how to compute some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_validation = model.scorer.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the predictions to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>scroll_percentage_fixed</th><th>read_time_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>inview_hour_differences</th><th>inview_article_categories</th><th>inview_article_types</th><th>scores</th><th>is_known_user</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[f32]</td><td>list[f32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>list[i64]</td><td>list[str]</td><td>list[f64]</td><td>bool</td></tr></thead><tbody><tr><td>22548</td><td>[9772629, 9773335, … 9776929]</td><td>[29.0, null, … 95.0]</td><td>[6.0, 0.0, … 56.0]</td><td>[9784710, 9784591, … 9783865]</td><td>[9784696]</td><td>96791</td><td>[0, 0, … 0]</td><td>[6.814444, 8.833333, … 6.831111]</td><td>[142, 142, … 498]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[0.092089, 0.260742, … 0.157016]</td><td>true</td></tr><tr><td>22548</td><td>[9772629, 9773335, … 9776929]</td><td>[29.0, null, … 95.0]</td><td>[6.0, 0.0, … 56.0]</td><td>[9784406, 9784642, … 9784281]</td><td>[9784281]</td><td>96798</td><td>[0, 0, … 1]</td><td>[11.292222, 9.510556, … 12.075]</td><td>[414, 118, … 142]</td><td>[&quot;article_fullscreen_gallery&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[0.124995, 0.109533, … 0.046443]</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 13)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ inview_ar ┆ inview_ar ┆ scores    ┆ is_known_ │\n",
       "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ticle_cat ┆ ticle_typ ┆ ---       ┆ user      │\n",
       "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ egories   ┆ es        ┆ list[f64] ┆ ---       │\n",
       "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ ---       ┆           ┆ bool      │\n",
       "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i64] ┆ list[str] ┆           ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 22548   ┆ [9772629,  ┆ [29.0,    ┆ [6.0,     ┆ … ┆ [142,     ┆ [\"article ┆ [0.092089 ┆ true      │\n",
       "│         ┆ 9773335, … ┆ null, …   ┆ 0.0, …    ┆   ┆ 142, …    ┆ _default\" ┆ ,         ┆           │\n",
       "│         ┆ 9776929]   ┆ 95.0]     ┆ 56.0]     ┆   ┆ 498]      ┆ , \"articl ┆ 0.260742, ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ e_def…    ┆ …         ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ 0.157016] ┆           │\n",
       "│ 22548   ┆ [9772629,  ┆ [29.0,    ┆ [6.0,     ┆ … ┆ [414,     ┆ [\"article ┆ [0.124995 ┆ true      │\n",
       "│         ┆ 9773335, … ┆ null, …   ┆ 0.0, …    ┆   ┆ 118, …    ┆ _fullscre ┆ ,         ┆           │\n",
       "│         ┆ 9776929]   ┆ 95.0]     ┆ 56.0]     ┆   ┆ 142]      ┆ en_galler ┆ 0.109533, ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ y\", \"…    ┆ …         ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ 0.046443] ┆           │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "    add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    ")\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.5657954501202044,\n",
       "    \"mrr\": 0.3557576891716898,\n",
       "    \"ndcg@5\": 0.39685534906028735,\n",
       "    \"ndcg@10\": 0.4723051350875782\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 14)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>scroll_percentage_fixed</th><th>read_time_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>inview_hour_differences</th><th>inview_article_categories</th><th>inview_article_types</th><th>scores</th><th>is_known_user</th><th>ranked_scores</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[f32]</td><td>list[f32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>list[i64]</td><td>list[str]</td><td>list[f64]</td><td>bool</td><td>list[i64]</td></tr></thead><tbody><tr><td>22548</td><td>[9772629, 9773335, … 9776929]</td><td>[29.0, null, … 95.0]</td><td>[6.0, 0.0, … 56.0]</td><td>[9784710, 9784591, … 9783865]</td><td>[9784696]</td><td>96791</td><td>[0, 0, … 0]</td><td>[6.814444, 8.833333, … 6.831111]</td><td>[142, 142, … 498]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[0.092089, 0.260742, … 0.157016]</td><td>true</td><td>[5, 2, … 4]</td></tr><tr><td>22548</td><td>[9772629, 9773335, … 9776929]</td><td>[29.0, null, … 95.0]</td><td>[6.0, 0.0, … 56.0]</td><td>[9784406, 9784642, … 9784281]</td><td>[9784281]</td><td>96798</td><td>[0, 0, … 1]</td><td>[11.292222, 9.510556, … 12.075]</td><td>[414, 118, … 142]</td><td>[&quot;article_fullscreen_gallery&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[0.124995, 0.109533, … 0.046443]</td><td>true</td><td>[11, 17, … 22]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 14)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ inview_ar ┆ scores    ┆ is_known_ ┆ ranked_sc │\n",
       "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ticle_typ ┆ ---       ┆ user      ┆ ores      │\n",
       "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ es        ┆ list[f64] ┆ ---       ┆ ---       │\n",
       "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆           ┆ bool      ┆ list[i64] │\n",
       "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[str] ┆           ┆           ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 22548   ┆ [9772629,  ┆ [29.0,    ┆ [6.0,     ┆ … ┆ [\"article ┆ [0.092089 ┆ true      ┆ [5, 2, …  │\n",
       "│         ┆ 9773335, … ┆ null, …   ┆ 0.0, …    ┆   ┆ _default\" ┆ ,         ┆           ┆ 4]        │\n",
       "│         ┆ 9776929]   ┆ 95.0]     ┆ 56.0]     ┆   ┆ , \"articl ┆ 0.260742, ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆ e_def…    ┆ …         ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 0.157016] ┆           ┆           │\n",
       "│ 22548   ┆ [9772629,  ┆ [29.0,    ┆ [6.0,     ┆ … ┆ [\"article ┆ [0.124995 ┆ true      ┆ [11, 17,  │\n",
       "│         ┆ 9773335, … ┆ null, …   ┆ 0.0, …    ┆   ┆ _fullscre ┆ ,         ┆           ┆ … 22]     │\n",
       "│         ┆ 9776929]   ┆ 95.0]     ┆ 56.0]     ┆   ┆ en_galler ┆ 0.109533, ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆ y\", \"…    ┆ …         ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 0.046443] ┆           ┆           │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation = df_validation.with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the validation, simply add the testset to your flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244647it [00:13, 18481.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/predictions.txt to downloads/predictions.zip\n"
     ]
    }
   ],
   "source": [
    "write_submission_file(\n",
    "    impression_ids=df_validation[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_validation[\"ranked_scores\"],\n",
    "    path=\"downloads/predictions.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "\n",
    "# def get_args():\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         description=\"Argument parser for NRMSModel training\"\n",
    "#     )\n",
    "\n",
    "#     parser.add_argument(\n",
    "#         \"--data_path\",\n",
    "#         type=str,\n",
    "#         default=str(\"/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/ebnerd_data\"), #str(\"~/ebnerd_data\"),\n",
    "#         help=\"Path to the data directory\",\n",
    "#     )\n",
    "\n",
    "#     # General settings\n",
    "#     parser.add_argument(\"--seed\", type=int, default=123, help=\"Random seed\")\n",
    "#     parser.add_argument(\n",
    "#         \"--datasplit\", type=str, default=\"ebnerd_small\", help=\"Dataset split to use\"\n",
    "#     )\n",
    "#     parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug mode\")\n",
    "\n",
    "#     # Batch sizes\n",
    "#     parser.add_argument(\n",
    "#         \"--bs_train\", type=int, default=32, help=\"Batch size for training\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--bs_test\", type=int, default=32, help=\"Batch size for testing\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--batch_size_test_wo_b\",\n",
    "#         type=int,\n",
    "#         default=32,\n",
    "#         help=\"Batch size for testing without balancing\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--batch_size_test_w_b\",\n",
    "#         type=int,\n",
    "#         default=4,\n",
    "#         help=\"Batch size for testing with balancing\",\n",
    "#     )\n",
    "\n",
    "#     # History and ratios\n",
    "#     parser.add_argument(\n",
    "#         \"--history_size\", type=int, default=20, help=\"History size for the model\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--npratio\", type=int, default=4, help=\"Negative-positive ratio\"\n",
    "#     )\n",
    "\n",
    "#     # Training settings\n",
    "#     parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of epochs\")\n",
    "#     parser.add_argument(\n",
    "#         \"--train_fraction\",\n",
    "#         type=float,\n",
    "#         default=1.0,\n",
    "#         help=\"Fraction of training data to use\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--fraction_test\",\n",
    "#         type=float,\n",
    "#         default=1.0,\n",
    "#         help=\"Fraction of testing data to use\",\n",
    "#     )\n",
    "\n",
    "#     # Model and loader settings\n",
    "#     parser.add_argument(\n",
    "#         \"--nrms_loader\",\n",
    "#         type=str,\n",
    "#         default=\"NRMSDataLoaderPretransform\",\n",
    "#         choices=[\"NRMSDataLoaderPretransform\", \"NRMSDataLoader\"],\n",
    "#         help=\"Data loader type (speed or memory efficient)\",\n",
    "#     )\n",
    "\n",
    "#     # Chunk processing\n",
    "#     parser.add_argument(\n",
    "#         \"--n_chunks_test\", type=int, default=10, help=\"Number of test chunks to process\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--chunks_done\", type=int, default=0, help=\"Number of chunks already processed\"\n",
    "#     )\n",
    "\n",
    "#     # =====================================================================================\n",
    "#     #  ############################# UNIQUE FOR NRMSDocVec ###############################\n",
    "#     # =====================================================================================\n",
    "\n",
    "#     parser.add_argument(\n",
    "#         \"--document_embeddings\",\n",
    "#         type=str,\n",
    "#         default=\"Ekstra_Bladet_contrastive_vector/contrastive_vector.parquet\",\n",
    "#         help=\"Path to the document embeddings file\",\n",
    "#     )\n",
    "#     # Model function and architecture\n",
    "#     parser.add_argument(\n",
    "#         \"--title_size\", type=int, default=768, help=\"Size of title encoding\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--head_num\", type=int, default=16, help=\"Number of attention heads\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--head_dim\", type=int, default=16, help=\"Dimension of each attention head\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--attention_hidden_dim\",\n",
    "#         type=int,\n",
    "#         default=200,\n",
    "#         help=\"Dimension of attention hidden layers\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--newsencoder_units_per_layer\",\n",
    "#         nargs=\"+\",\n",
    "#         type=int,\n",
    "#         default=[512, 512, 512],\n",
    "#         help=\"List of units per layer in the news encoder\",\n",
    "#     )\n",
    "\n",
    "#     # Optimizer settings\n",
    "#     parser.add_argument(\n",
    "#         \"--optimizer\", type=str, default=\"adam\", help=\"Optimizer to use\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--loss\", type=str, default=\"cross_entropy_loss\", help=\"Loss function\"\n",
    "#     )\n",
    "#     parser.add_argument(\"--dropout\", type=float, default=0.2, help=\"Dropout rate\")\n",
    "#     parser.add_argument(\n",
    "#         \"--learning_rate\", type=float, default=1e-4, help=\"Learning rate\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--newsencoder_l2_regularization\",\n",
    "#         type=float,\n",
    "#         default=1e-4,\n",
    "#         help=\"L2 regularization for the news encoder\",\n",
    "#     )\n",
    "\n",
    "#     return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Running DOCVEC\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from ebrec.utils2._nlp import get_transformers_word_embeddings\n",
    "# from ebrec.utils2._articles import convert_text2encoding_with_transformers\n",
    "\n",
    "# from pathlib import Path\n",
    "# import tensorflow as tf\n",
    "# import datetime as dt\n",
    "# import polars as pl\n",
    "# import shutil\n",
    "# import gc\n",
    "# import os\n",
    "\n",
    "# from ebrec.utils2._constants import *\n",
    "\n",
    "# from ebrec.utils2._behaviors import (\n",
    "#     create_binary_labels_column,\n",
    "#     sampling_strategy_wu2019,\n",
    "#     add_prediction_scores,\n",
    "#     truncate_history,\n",
    "#     ebnerd_from_path,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "\n",
    "# from ebrec.utils2._python import (\n",
    "#     write_submission_file,\n",
    "#     rank_predictions_by_score,\n",
    "#     write_json_file,\n",
    "# )\n",
    "# from ebrec.utils2._articles import create_article_id_to_value_mapping\n",
    "# from ebrec.utils2._polars import split_df_chunks, concat_str_columns\n",
    "\n",
    "# from ebrec.models.newsrec.dataloader import NRMSDataLoader, NRMSDataLoaderPretransform\n",
    "# from ebrec.models.newsrec.model_config2 import (\n",
    "#     hparams_nrms,\n",
    "#     hparams_nrms_docvec,\n",
    "#     hparams_to_dict,\n",
    "#     print_hparams,\n",
    "# )\n",
    "# from ebrec.models.newsrec.nrms_docvec2 import NRMSDocVec\n",
    "# from ebrec.models.newsrec import NRMSModel\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# # from args_nrms_docvec import get_args\n",
    "\n",
    "\n",
    "# args = get_args()\n",
    "\n",
    "# for arg, val in vars(args).items():\n",
    "#     print(f\"{arg} : {val}\")\n",
    "\n",
    "# PATH = Path(args.data_path).expanduser()\n",
    "# # Access arguments as variables\n",
    "# SEED = args.seed\n",
    "# DATASPLIT = args.datasplit\n",
    "# DEBUG = args.debug\n",
    "# BS_TRAIN = args.bs_train\n",
    "# BS_TEST = args.bs_test\n",
    "# BATCH_SIZE_TEST_WO_B = args.batch_size_test_wo_b\n",
    "# BATCH_SIZE_TEST_W_B = args.batch_size_test_w_b\n",
    "# HISTORY_SIZE = args.history_size\n",
    "# NPRATIO = args.npratio\n",
    "# EPOCHS = args.epochs\n",
    "# TRAIN_FRACTION = args.train_fraction if not DEBUG else 0.0001\n",
    "# FRACTION_TEST = args.fraction_test if not DEBUG else 0.0001\n",
    "\n",
    "# NRMSLoader_training = (\n",
    "#     NRMSDataLoaderPretransform\n",
    "#     if args.nrms_loader == \"NRMSDataLoaderPretransform\"\n",
    "#     else NRMSDataLoader\n",
    "# )\n",
    "\n",
    "# # =====================================================================================\n",
    "# #  ############################# UNIQUE FOR NRMSModel ################################\n",
    "# # =====================================================================================\n",
    "\n",
    "# # Model in use:\n",
    "# model_func = NRMSDocVec\n",
    "# hparams = hparams_nrms_docvec\n",
    "# #\n",
    "# hparams.title_size = args.title_size\n",
    "# hparams.history_size = args.history_size\n",
    "# hparams.head_num = args.head_num\n",
    "# hparams.head_dim = args.head_dim\n",
    "# hparams.attention_hidden_dim = args.attention_hidden_dim\n",
    "# hparams.newsencoder_units_per_layer = args.newsencoder_units_per_layer\n",
    "# hparams.optimizer = args.optimizer\n",
    "# hparams.loss = args.loss\n",
    "# hparams.dropout = args.dropout\n",
    "# hparams.learning_rate = args.learning_rate\n",
    "# hparams.newsencoder_l2_regularization = args.newsencoder_l2_regularization\n",
    "\n",
    "\n",
    "# # =============\n",
    "# # Data-path\n",
    "# DOC_VEC_PATH = PATH.joinpath(f\"artifacts/{args.document_embeddings}\")\n",
    "# print(\"Initiating articles...\")\n",
    "# df_articles = pl.read_parquet(DOC_VEC_PATH)\n",
    "# article_mapping = create_article_id_to_value_mapping(\n",
    "#     df=df_articles, value_col=df_articles.columns[-1]\n",
    "# )\n",
    "\n",
    "# # =====================================================================================\n",
    "# #  ############################# UNIQUE FOR NRMSDocVec ###############################\n",
    "# # =====================================================================================\n",
    "\n",
    "# print_hparams(hparams)\n",
    "\n",
    "# # Dump paths:\n",
    "# DUMP_DIR = Path(\"ebnerd_predictions\")\n",
    "# DUMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "# #\n",
    "# DT_NOW = dt.datetime.now()\n",
    "# #\n",
    "# MODEL_NAME = model_func.__name__\n",
    "# MODEL_OUTPUT_NAME = f\"{MODEL_NAME}-{DT_NOW}\"\n",
    "# #\n",
    "# ARTIFACT_DIR = DUMP_DIR.joinpath(\"test_predictions\", MODEL_OUTPUT_NAME)\n",
    "# # Model monitoring:\n",
    "# MODEL_WEIGHTS = DUMP_DIR.joinpath(f\"state_dict/{MODEL_OUTPUT_NAME}/weights\")\n",
    "# LOG_DIR = DUMP_DIR.joinpath(f\"runs/{MODEL_OUTPUT_NAME}\")\n",
    "# # Evaluating the test test can be memory intensive, we'll chunk it up:\n",
    "# TEST_CHUNKS_DIR = ARTIFACT_DIR.joinpath(\"test_chunks\")\n",
    "# TEST_CHUNKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# N_CHUNKS_TEST = args.n_chunks_test\n",
    "# CHUNKS_DONE = args.chunks_done  # if it crashes, you can start from here.\n",
    "# # Just trying keeping the dataframe slime:\n",
    "# COLUMNS = [\n",
    "#     DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "#     DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "#     DEFAULT_INVIEW_ARTICLES_COL,\n",
    "#     DEFAULT_CLICKED_ARTICLES_COL,\n",
    "#     DEFAULT_IMPRESSION_ID_COL,\n",
    "#     DEFAULT_USER_COL,\n",
    "# ]\n",
    "# # Store hparams\n",
    "# write_json_file(\n",
    "#     hparams_to_dict(hparams),\n",
    "#     ARTIFACT_DIR.joinpath(f\"{MODEL_NAME}_hparams.json\"),\n",
    "# )\n",
    "# write_json_file(vars(args), ARTIFACT_DIR.joinpath(f\"{MODEL_NAME}_argparser.json\"))\n",
    "\n",
    "# # =====================================================================================\n",
    "# # We'll use the training + validation sets for training.\n",
    "# df = (\n",
    "#     pl.concat(\n",
    "#         [\n",
    "#             ebnerd_from_path(\n",
    "#                 PATH.joinpath(DATASPLIT, \"train\"),\n",
    "#                 history_size=HISTORY_SIZE,\n",
    "#                 padding=0,\n",
    "#             ),\n",
    "#             ebnerd_from_path(\n",
    "#                 PATH.joinpath(DATASPLIT, \"validation\"),\n",
    "#                 history_size=HISTORY_SIZE,\n",
    "#                 padding=0,\n",
    "#             ),\n",
    "#         ]\n",
    "#     )\n",
    "#     .sample(fraction=TRAIN_FRACTION, shuffle=True, seed=SEED)\n",
    "#     .select(COLUMNS)\n",
    "#     .pipe(\n",
    "#         sampling_strategy_wu2019,\n",
    "#         npratio=NPRATIO,\n",
    "#         shuffle=True,\n",
    "#         with_replacement=True,\n",
    "#         seed=SEED,\n",
    "#     )\n",
    "#     .pipe(create_binary_labels_column)\n",
    "# )\n",
    "\n",
    "# # We keep the last day of our training data as the validation set.\n",
    "# last_dt = df[DEFAULT_IMPRESSION_TIMESTAMP_COL].dt.date().max() - dt.timedelta(days=1)\n",
    "# df_train = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).dt.date() < last_dt)\n",
    "# df_validation = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).dt.date() >= last_dt)\n",
    "\n",
    "# # =====================================================================================\n",
    "# print(f\"Initiating training-dataloader\")\n",
    "# train_dataloader = NRMSLoader_training(\n",
    "#     behaviors=df_train,\n",
    "#     article_dict=article_mapping,\n",
    "#     unknown_representation=\"zeros\",\n",
    "#     history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "#     eval_mode=False,\n",
    "#     batch_size=BS_TRAIN,\n",
    "# )\n",
    "\n",
    "# val_dataloader = NRMSLoader_training(\n",
    "#     behaviors=df_validation,\n",
    "#     article_dict=article_mapping,\n",
    "#     unknown_representation=\"zeros\",\n",
    "#     history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "#     eval_mode=False,\n",
    "#     batch_size=BS_TRAIN,\n",
    "# )\n",
    "\n",
    "# # =====================================================================================\n",
    "# # CALLBACKS\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "#     log_dir=LOG_DIR,\n",
    "#     histogram_freq=1,\n",
    "# )\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor=\"val_auc\",\n",
    "#     mode=\"max\",\n",
    "#     patience=4,\n",
    "#     restore_best_weights=True,\n",
    "# )\n",
    "# modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=MODEL_WEIGHTS,\n",
    "#     monitor=\"val_auc\",\n",
    "#     mode=\"max\",\n",
    "#     save_best_only=True,\n",
    "#     save_weights_only=True,\n",
    "#     verbose=1,\n",
    "# )\n",
    "# lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "#     monitor=\"val_auc\",\n",
    "#     mode=\"max\",\n",
    "#     factor=0.2,\n",
    "#     patience=2,\n",
    "#     min_lr=1e-6,\n",
    "# )\n",
    "# callbacks = [tensorboard_callback, early_stopping, modelcheckpoint, lr_scheduler]\n",
    "\n",
    "# # =====================================================================================\n",
    "# model = model_func(\n",
    "#     hparams=hparams,\n",
    "#     seed=42,\n",
    "# )\n",
    "# model.model.compile(\n",
    "#     optimizer=model.model.optimizer,\n",
    "#     loss=model.model.loss,\n",
    "#     metrics=[\"AUC\"],\n",
    "# )\n",
    "# f\"Initiating {MODEL_NAME}, start training...\"\n",
    "# # =>\n",
    "# hist = model.model.fit(\n",
    "#     train_dataloader,\n",
    "#     validation_data=val_dataloader,\n",
    "#     epochs=EPOCHS,\n",
    "#     callbacks=callbacks,\n",
    "# )\n",
    "\n",
    "# print(f\"loading model: {MODEL_WEIGHTS}\")\n",
    "# model.model.load_weights(MODEL_WEIGHTS)\n",
    "\n",
    "# # =====================================================================================\n",
    "# print(\"Initiating testset...\")\n",
    "# df_test = (\n",
    "#     ebnerd_from_path(\n",
    "#         PATH.joinpath(\"ebnerd_testset\", \"test\"),\n",
    "#         history_size=HISTORY_SIZE,\n",
    "#         padding=0,\n",
    "#     )\n",
    "#     .sample(fraction=FRACTION_TEST)\n",
    "#     .with_columns(\n",
    "#         pl.col(DEFAULT_INVIEW_ARTICLES_COL)\n",
    "#         .list.first()\n",
    "#         .alias(DEFAULT_CLICKED_ARTICLES_COL)\n",
    "#     )\n",
    "#     .select(COLUMNS + [DEFAULT_IS_BEYOND_ACCURACY_COL])\n",
    "#     .with_columns(\n",
    "#         pl.col(DEFAULT_INVIEW_ARTICLES_COL)\n",
    "#         .list.eval(pl.element() * 0)\n",
    "#         .alias(DEFAULT_LABELS_COL)\n",
    "#     )\n",
    "# )\n",
    "# # Split test in beyond-accuracy TRUE / FALSE. In the BA 'article_ids_inview' is 250.\n",
    "# df_test_wo_beyond = df_test.filter(~pl.col(DEFAULT_IS_BEYOND_ACCURACY_COL))\n",
    "# df_test_w_beyond = df_test.filter(pl.col(DEFAULT_IS_BEYOND_ACCURACY_COL))\n",
    "\n",
    "# df_test_chunks = split_df_chunks(df_test_wo_beyond, n_chunks=N_CHUNKS_TEST)\n",
    "# df_pred_test_wo_beyond = []\n",
    "# print(\"Initiating testset without beyond-accuracy...\")\n",
    "# for i, df_test_chunk in enumerate(df_test_chunks[CHUNKS_DONE:], start=1 + CHUNKS_DONE):\n",
    "#     print(f\"Test chunk: {i}/{len(df_test_chunks)}\")\n",
    "#     # Initialize DataLoader\n",
    "#     test_dataloader_wo_b = NRMSDataLoader(\n",
    "#         behaviors=df_test_chunk,\n",
    "#         article_dict=article_mapping,\n",
    "#         unknown_representation=\"zeros\",\n",
    "#         history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "#         eval_mode=True,\n",
    "#         batch_size=BATCH_SIZE_TEST_WO_B,\n",
    "#     )\n",
    "#     # Predict and clear session\n",
    "#     scores = model.scorer.predict(test_dataloader_wo_b)\n",
    "#     tf.keras.backend.clear_session()\n",
    "\n",
    "#     # Process the predictions\n",
    "#     df_test_chunk = add_prediction_scores(df_test_chunk, scores.tolist()).with_columns(\n",
    "#         pl.col(\"scores\")\n",
    "#         .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "#         .alias(\"ranked_scores\")\n",
    "#     )\n",
    "\n",
    "#     # Save the processed chunk\n",
    "#     df_test_chunk.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "#         TEST_CHUNKS_DIR.joinpath(f\"pred_wo_ba_{i}.parquet\")\n",
    "#     )\n",
    "\n",
    "#     # Append and clean up\n",
    "#     df_pred_test_wo_beyond.append(df_test_chunk)\n",
    "\n",
    "#     # Cleanup\n",
    "#     del df_test_chunk, test_dataloader_wo_b, scores\n",
    "#     gc.collect()\n",
    "\n",
    "# df_pred_test_wo_beyond = pl.concat(df_pred_test_wo_beyond)\n",
    "# df_pred_test_wo_beyond.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "#     TEST_CHUNKS_DIR.joinpath(\"pred_wo_ba.parquet\")\n",
    "# )\n",
    "# # =====================================================================================\n",
    "# print(\"Initiating testset with beyond-accuracy...\")\n",
    "# test_dataloader_w_b = NRMSDataLoader(\n",
    "#     behaviors=df_test_w_beyond,\n",
    "#     article_dict=article_mapping,\n",
    "#     unknown_representation=\"zeros\",\n",
    "#     history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "#     eval_mode=True,\n",
    "#     batch_size=BATCH_SIZE_TEST_W_B,\n",
    "# )\n",
    "# scores = model.scorer.predict(test_dataloader_w_b)\n",
    "# df_pred_test_w_beyond = add_prediction_scores(\n",
    "#     df_test_w_beyond, scores.tolist()\n",
    "# ).with_columns(\n",
    "#     pl.col(\"scores\")\n",
    "#     .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "#     .alias(\"ranked_scores\")\n",
    "# )\n",
    "# df_pred_test_w_beyond.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "#     TEST_CHUNKS_DIR.joinpath(\"pred_w_ba.parquet\")\n",
    "# )\n",
    "\n",
    "# # =====================================================================================\n",
    "# print(\"Saving prediction results...\")\n",
    "# df_test = pl.concat([df_pred_test_wo_beyond, df_pred_test_w_beyond])\n",
    "# df_test.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "#     ARTIFACT_DIR.joinpath(\"test_predictions.parquet\")\n",
    "# )\n",
    "\n",
    "# if TEST_CHUNKS_DIR.exists() and TEST_CHUNKS_DIR.is_dir():\n",
    "#     shutil.rmtree(TEST_CHUNKS_DIR)\n",
    "\n",
    "# write_submission_file(\n",
    "#     impression_ids=df_test[DEFAULT_IMPRESSION_ID_COL],\n",
    "#     prediction_scores=df_test[\"ranked_scores\"],\n",
    "#     path=ARTIFACT_DIR.joinpath(\"predictions.txt\"),\n",
    "#     filename_zip=f\"{MODEL_NAME}-{SEED}-{DATASPLIT}.zip\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
