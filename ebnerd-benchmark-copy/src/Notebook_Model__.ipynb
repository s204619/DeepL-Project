{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This note book run the model (with optimal hyperparameters) and finally creates a predictions.txt file that can be submitted to codabench\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____————____————____————____————\n",
    "# Define the history size and fraction and EPOCHS\n",
    "# ____————____————____————____————\n",
    "HISTORY_SIZE = 20 #30\n",
    "FRACTION = 0.001 #Fraction af datasæt\n",
    "EPOCHS = 3\n",
    "FRACTION_testset = 0.0001\n",
    "#____————____————____————____————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 21:34:03.988395: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-20 21:34:04.034750: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-20 21:34:04.034791: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-20 21:34:04.034828: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-20 21:34:04.045266: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-20 21:34:05.384164: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_ARTICLE_ID_COL,\n",
    "    DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL, #-------\n",
    "    DEFAULT_HISTORY_READ_TIME_COL #-------\n",
    ")\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "from ebrec.models.newsrec.dataloader import NRMSDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms\n",
    "from ebrec.models.newsrec import NRMSModel, NRMSWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Available devices: []\n",
      "____————____————____————____———\n",
      "HISTORY_SIZE: 20\n",
      "FRACTION: 0.001\n",
      "EPOCHS: 3\n",
      "FRACTION_testset: 0.0001\n",
      "____————____————____————____———\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 21:34:07.492627: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 9)\n",
      "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ article_i ┆ impressio ┆ impressio ┆ labels    │\n",
      "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ds_clicke ┆ n_id      ┆ n_time    ┆ ---       │\n",
      "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ d         ┆ ---       ┆ ---       ┆ list[i8]  │\n",
      "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ u32       ┆ datetime[ ┆           │\n",
      "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i64] ┆           ┆ μs]       ┆           │\n",
      "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 838158  ┆ [9758312,  ┆ [100.0,   ┆ [21.0,    ┆ … ┆ [9779511] ┆ 524788077 ┆ 2023-05-2 ┆ [0, 0, …  │\n",
      "│         ┆ 9759162, … ┆ 100.0, …  ┆ 2.0, …    ┆   ┆           ┆           ┆ 4         ┆ 1]        │\n",
      "│         ┆ 9728166]   ┆ 100.0]    ┆ 10.0]     ┆   ┆           ┆           ┆ 18:12:13  ┆           │\n",
      "│ 1149991 ┆ [9767551,  ┆ [37.0,    ┆ [4.0,     ┆ … ┆ [9776047] ┆ 393863099 ┆ 2023-05-2 ┆ [0, 1, …  │\n",
      "│         ┆ 9767528, … ┆ 14.0, …   ┆ 11.0, …   ┆   ┆           ┆           ┆ 3         ┆ 0]        │\n",
      "│         ┆ 9769630]   ┆ 100.0]    ┆ 35.0]     ┆   ┆           ┆           ┆ 10:30:07  ┆           │\n",
      "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n",
      "shape: (2, 9)\n",
      "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ article_i ┆ impressio ┆ impressio ┆ labels    │\n",
      "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ds_clicke ┆ n_id      ┆ n_time    ┆ ---       │\n",
      "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ d         ┆ ---       ┆ ---       ┆ list[i8]  │\n",
      "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ u32       ┆ datetime[ ┆           │\n",
      "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i32] ┆           ┆ μs]       ┆           │\n",
      "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 2494285 ┆ [9779019,  ┆ [100.0,   ┆ [115.0,   ┆ … ┆ [9780517] ┆ 96398451  ┆ 2023-05-2 ┆ [0, 0, …  │\n",
      "│         ┆ 9778915, … ┆ 20.0, …   ┆ 139.0, …  ┆   ┆           ┆           ┆ 6         ┆ 0]        │\n",
      "│         ┆ 9779642]   ┆ 100.0]    ┆ 895.0]    ┆   ┆           ┆           ┆ 18:39:04  ┆           │\n",
      "│ 2412823 ┆ [9774899,  ┆ [12.0,    ┆ [418.0,   ┆ … ┆ [9786209] ┆ 422073979 ┆ 2023-05-3 ┆ [1, 0, …  │\n",
      "│         ┆ 9774972, … ┆ 33.0, …   ┆ 11.0, …   ┆   ┆           ┆           ┆ 0         ┆ 0]        │\n",
      "│         ┆ 9779748]   ┆ 34.0]     ┆ 7.0]      ┆   ┆           ┆           ┆ 16:10:00  ┆           │\n",
      "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var i…</td><td>&quot;Politiet frygt…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den ø…</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars …</td><td>&quot;Biografgængern…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har …</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natascha… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind for…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "# Setup and load everything\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "print(\"Loading data\")\n",
    "\n",
    "# List all physical devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available devices:\", physical_devices)\n",
    "\n",
    "\n",
    "# -_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "# # Load dataset\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        # .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL,DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL)\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL,DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL,DEFAULT_HISTORY_READ_TIME_COL) #------------\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df_behaviors\n",
    "\n",
    "  \n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "  ### Generate labels\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "# \n",
    "PATH = Path(\"/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/ebnerd_data\").expanduser()\n",
    "DATASPLIT = \"ebnerd_small\" # \n",
    "\n",
    "DUMP_DIR = PATH.joinpath(\"dump_artifacts\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "## Define the Data Cols -- New ones here\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL, #--------neu \n",
    "    DEFAULT_HISTORY_READ_TIME_COL, #------- neu\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "print(\"____————____————____————____———\")\n",
    "print(\"HISTORY_SIZE:\", HISTORY_SIZE)\n",
    "print(\"FRACTION:\", FRACTION)\n",
    "print(\"EPOCHS:\", EPOCHS)\n",
    "print(\"FRACTION_testset:\", FRACTION_testset)\n",
    "print(\"____————____————____————____———\")\n",
    "print(\"\")\n",
    "#____————____————____————____————\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=6,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "print(df_train.head(2))\n",
    "print(df_validation.head(2))\n",
    "\n",
    "\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "## Load articles\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "df_articles = pl.read_parquet(PATH.joinpath(DATASPLIT, \"articles.parquet\"))\n",
    "df_articles.head(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>scroll_percentage_fixed</th><th>read_time_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>inview_hour_differences</th><th>inview_article_categories</th><th>history_article_categories</th><th>inview_article_types</th><th>history_article_types</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[f32]</td><td>list[f32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>list[i64]</td><td>list[i64]</td><td>list[str]</td><td>list[str]</td></tr></thead><tbody><tr><td>2494285</td><td>[9779019, 9778915, … 9779642]</td><td>[100.0, 20.0, … 100.0]</td><td>[115.0, 139.0, … 895.0]</td><td>[9694022, 9783276, … 9783164]</td><td>[9780517]</td><td>96398451</td><td>[0, 0, … 0]</td><td>[1462.956389, 0.287222, … 1.950556]</td><td>[414, 414, … 140]</td><td>[457, 512, … 414]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td></tr><tr><td>2412823</td><td>[9774899, 9774972, … 9779748]</td><td>[12.0, 33.0, … 34.0]</td><td>[418.0, 11.0, … 7.0]</td><td>[9786209, 9788462, … 9780702]</td><td>[9786209]</td><td>422073979</td><td>[1, 0, … 0]</td><td>[31.175556, 3.613611, … 8.446944]</td><td>[414, 512, … 457]</td><td>[118, 118, … 414]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 13)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ inview_ar ┆ history_a ┆ inview_ar ┆ history_a │\n",
       "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ticle_cat ┆ rticle_ca ┆ ticle_typ ┆ rticle_ty │\n",
       "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ egories   ┆ tegories  ┆ es        ┆ pes       │\n",
       "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i64] ┆ list[i64] ┆ list[str] ┆ list[str] │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2494285 ┆ [9779019,  ┆ [100.0,   ┆ [115.0,   ┆ … ┆ [414,     ┆ [457,     ┆ [\"article ┆ [\"article │\n",
       "│         ┆ 9778915, … ┆ 20.0, …   ┆ 139.0, …  ┆   ┆ 414, …    ┆ 512, …    ┆ _default\" ┆ _default\" │\n",
       "│         ┆ 9779642]   ┆ 100.0]    ┆ 895.0]    ┆   ┆ 140]      ┆ 414]      ┆ , \"articl ┆ , \"articl │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ e_def…    ┆ e_def…    │\n",
       "│ 2412823 ┆ [9774899,  ┆ [12.0,    ┆ [418.0,   ┆ … ┆ [414,     ┆ [118,     ┆ [\"article ┆ [\"article │\n",
       "│         ┆ 9774972, … ┆ 33.0, …   ┆ 11.0, …   ┆   ┆ 512, …    ┆ 118, …    ┆ _default\" ┆ _default\" │\n",
       "│         ┆ 9779748]   ┆ 34.0]     ┆ 7.0]      ┆   ┆ 457]      ┆ 414]      ┆ , \"articl ┆ , \"articl │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ e_def…    ┆ e_def…    │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "# Loading the article embeddings and other feature stuff\n",
    "\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "### Added features and hourly difference between published and viewed article\n",
    "\n",
    "## NEW\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert polars DataFrame to pandas\n",
    "df_train = df_train.to_pandas()\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_time\n",
    "article_time_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"published_time\"\n",
    ").to_dict(as_series=False)\n",
    "article_time_dict = dict(zip(\n",
    "    article_time_dict[\"article_id\"], \n",
    "    article_time_dict[\"published_time\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their timestamps\n",
    "def get_article_times(article_ids):\n",
    "    return [article_time_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the published-time\n",
    "df_train[\"inview_article_times\"] = df_train[\"article_ids_inview\"].apply(get_article_times)\n",
    "\n",
    "#add new column with the last publish_time for the clicked article\n",
    "df_train[\"clicked_article_time\"] = df_train[\"article_ids_clicked\"].apply(get_article_times)\n",
    "\n",
    "# Create a function to calculate hour differences\n",
    "def calculate_hour_differences(impression_time, article_times):\n",
    "        # If article_times is a single value (for clicked articles)\n",
    "    if not isinstance(article_times, list):\n",
    "        if article_times is None:\n",
    "            return None\n",
    "        return (impression_time - article_times).total_seconds() / 3600\n",
    "    \n",
    "    # If article_times is a list (for inview articles)\n",
    "    differences = [(impression_time - article_time).total_seconds() / 3600 \n",
    "                  if article_time is not None else None \n",
    "                  for article_time in article_times]\n",
    "    return differences\n",
    "\n",
    "# Use for inview articles\n",
    "df_train['inview_hour_differences'] = df_train.apply(\n",
    "    lambda row: calculate_hour_differences(row['impression_time'], row['inview_article_times']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# # Use for clicked article\n",
    "# df_train['clicked_hour_difference'] = df_train.apply(\n",
    "#    lambda row: calculate_hour_differences(row['impression_time'], row['clicked_article_time']), \n",
    "#    axis=1\n",
    "# )\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_category\n",
    "article_cat_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"category\"\n",
    ").to_dict(as_series=False)\n",
    "article_cat_dict = dict(zip(\n",
    "    article_cat_dict[\"article_id\"], \n",
    "    article_cat_dict[\"category\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their category\n",
    "def get_article_category(article_ids):\n",
    "    return [article_cat_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "#  Add new column with the article category\n",
    "df_train[\"inview_article_categories\"] = df_train[\"article_ids_inview\"].apply(get_article_category)\n",
    "\n",
    "df_train[\"history_article_categories\"] = df_train[\"article_id_fixed\"].apply(get_article_category)\n",
    "\n",
    "# Create a mapping dictionary from article_id to article_type\n",
    "article_type_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"article_type\"\n",
    ").to_dict(as_series=False)\n",
    "article_type_dict = dict(zip(\n",
    "    article_type_dict[\"article_id\"], \n",
    "    article_type_dict[\"article_type\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their type\n",
    "def get_article_type(article_ids):\n",
    "    return [article_type_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the article type\n",
    "df_train[\"inview_article_types\"] = df_train[\"article_ids_inview\"].apply(get_article_type)\n",
    "\n",
    "df_train[\"history_article_types\"] = df_train[\"article_id_fixed\"].apply(get_article_type)\n",
    "\n",
    "#drop columns with the time\n",
    "df_train = df_train.drop(['inview_article_times', 'clicked_article_time','impression_time'], axis=1)\n",
    "\n",
    "df_train = pl.from_pandas(df_train)\n",
    "\n",
    "df_train.head(2)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "# Convert polars DataFrame to pandas\n",
    "df_validation = df_validation.to_pandas()\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_time\n",
    "article_time_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"published_time\"\n",
    ").to_dict(as_series=False)\n",
    "article_time_dict = dict(zip(\n",
    "    article_time_dict[\"article_id\"], \n",
    "    article_time_dict[\"published_time\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their timestamps\n",
    "def get_article_times(article_ids):\n",
    "    return [article_time_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the published-time\n",
    "df_validation[\"inview_article_times\"] = df_validation[\"article_ids_inview\"].apply(get_article_times)\n",
    "\n",
    "#add new column with the last publish_time for the clicked article\n",
    "df_validation[\"clicked_article_time\"] = df_validation[\"article_ids_clicked\"].apply(get_article_times)\n",
    "\n",
    "# Create a function to calculate hour differences\n",
    "def calculate_hour_differences(impression_time, article_times):\n",
    "        # If article_times is a single value (for clicked articles)\n",
    "    if not isinstance(article_times, list):\n",
    "        if article_times is None:\n",
    "            return None\n",
    "        return (impression_time - article_times).total_seconds() / 3600\n",
    "    \n",
    "    # If article_times is a list (for inview articles)\n",
    "    differences = [(impression_time - article_time).total_seconds() / 3600 \n",
    "                  if article_time is not None else None \n",
    "                  for article_time in article_times]\n",
    "    return differences\n",
    "\n",
    "# Use for inview articles\n",
    "df_validation['inview_hour_differences'] = df_validation.apply(\n",
    "    lambda row: calculate_hour_differences(row['impression_time'], row['inview_article_times']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# # Use for clicked article -- might be leaky??\n",
    "# df_validation['clicked_hour_difference'] = df_validation.apply(\n",
    "#    lambda row: calculate_hour_differences(row['impression_time'], row['clicked_article_time']), \n",
    "#    axis=1\n",
    "# )\n",
    "# Create a mapping dictionary from article_id to last_modified_category\n",
    "article_cat_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"category\"\n",
    ").to_dict(as_series=False)\n",
    "article_cat_dict = dict(zip(\n",
    "    article_cat_dict[\"article_id\"], \n",
    "    article_cat_dict[\"category\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their category\n",
    "def get_article_category(article_ids):\n",
    "    return [article_cat_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "#  Add new column with the article category\n",
    "df_validation[\"inview_article_categories\"] = df_validation[\"article_ids_inview\"].apply(get_article_category)\n",
    "\n",
    "df_validation[\"history_article_categories\"] = df_validation[\"article_id_fixed\"].apply(get_article_category)\n",
    "\n",
    "# Create a mapping dictionary from article_id to article_type\n",
    "article_type_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"article_type\"\n",
    ").to_dict(as_series=False)\n",
    "article_type_dict = dict(zip(\n",
    "    article_type_dict[\"article_id\"], \n",
    "    article_type_dict[\"article_type\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their type\n",
    "def get_article_type(article_ids):\n",
    "    return [article_type_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the article type\n",
    "df_validation[\"inview_article_types\"] = df_validation[\"article_ids_inview\"].apply(get_article_type)\n",
    "\n",
    "df_validation[\"history_article_types\"] = df_validation[\"article_id_fixed\"].apply(get_article_type)\n",
    "\n",
    "\n",
    "#drop columns with the time\n",
    "df_validation = df_validation.drop(['inview_article_times', 'clicked_article_time','impression_time'], axis=1)\n",
    "\n",
    "\n",
    "df_validation = pl.from_pandas(df_validation)\n",
    "\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____————____————____————____———\n",
      "Using transformer model: Maltehb/danish-bert-botxo\n",
      "____————____————____————____———\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "## Init model using HuggingFace's tokenizer and wordembedding\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "# TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "# TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-robe rta-large\"\n",
    "# TRANSFORMER_MODEL_NAME = \"google-bert/bert-base-multilingual-uncased\" \n",
    "# #Cased might be better but to compare with malteHb we use uncased\n",
    "\n",
    "TRANSFORMER_MODEL_NAME = \"Maltehb/danish-bert-botxo\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30 #hardcoded somewhere ?? error if change\n",
    "\n",
    "print(\"\")\n",
    "print(\"____————____————____————____———\")\n",
    "print(\"Using transformer model:\", TRANSFORMER_MODEL_NAME)\n",
    "print(\"____————____————____————____———\")\n",
    "print(\"\")\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "\n",
    "\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")\n",
    "\n",
    "#_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "# print(\"df_train columns:\", df_train.columns)\n",
    "# print(\"df_validation columns:\", df_validation.columns)\n",
    "#_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=128,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: []\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# List all physical devices\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available devices:\", physical_devices)\n",
    "import torch.nn as nn\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Train]: 100%|██████████| 2/2 [00:00<00:00,  3.28it/s, loss=0.6193]\n",
      "Epoch 1/3 [Valid]: 100%|██████████| 4/4 [00:01<00:00,  3.74it/s, loss=0.7212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.6193, Val Loss: 0.3606\n",
      "\n",
      "Validation loss improved from inf to 0.36058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 [Train]: 100%|██████████| 2/2 [00:00<00:00,  7.25it/s, loss=0.4607]\n",
      "Epoch 2/3 [Valid]: 100%|██████████| 4/4 [00:00<00:00,  4.86it/s, loss=0.6010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 0.4607, Val Loss: 0.3005\n",
      "\n",
      "Validation loss improved from 0.36058 to 0.30048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 [Train]: 100%|██████████| 2/2 [00:00<00:00,  7.36it/s, loss=0.4225]\n",
      "Epoch 3/3 [Valid]: 100%|██████████| 4/4 [00:00<00:00,  5.10it/s, loss=0.7646]\n",
      "/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/src/ebrec/models/newsrec/nrmspy_1.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(filepath))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 0.4225, Val Loss: 0.3823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 4/4 [00:00<00:00,  5.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# _-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "# Original Model\n",
    "# _-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "# Works fine -- Can change epochs on line 67\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "WEIGHTS_DIR = f\"downloads/data/state_dict/{MODEL_NAME}\"\n",
    "MODEL_WEIGHTS = f\"{WEIGHTS_DIR}/weights.pt\"\n",
    "\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# Create a custom ModelCheckpoint for PyTorch\n",
    "class PyTorchModelCheckpoint:\n",
    "    def __init__(self, filepath, model_wrapper=None, save_best_only=True, save_weights_only=True, verbose=1):\n",
    "        self.filepath = filepath\n",
    "        self.model_wrapper = model_wrapper  # Store the model wrapper reference\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.verbose = verbose\n",
    "        self.best_val_loss = float('inf')\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss', None)\n",
    "        if val_loss is None:\n",
    "            return\n",
    "        \n",
    "        if self.save_best_only:\n",
    "            if val_loss < self.best_val_loss:\n",
    "                if self.verbose:\n",
    "                    print(f'\\nValidation loss improved from {self.best_val_loss:.5f} to {val_loss:.5f}')\n",
    "                self.best_val_loss = val_loss\n",
    "                # Use the model_wrapper reference\n",
    "                self.model_wrapper.save_weights(self.filepath)\n",
    "        else:\n",
    "            self.model_wrapper.save_weights(self.filepath)\n",
    "\n",
    "# Initialize model first\n",
    "hparams_nrms.history_size = HISTORY_SIZE\n",
    "\n",
    "# Best hyperparameters: {'head_num': 19, 'head_dim': 29, 'attention_hidden_dim': 145, 'dropout': 0.22088583494496855, 'learning_rate': 0.00030309205322750723}\n",
    "\n",
    "hparams_nrms.head_num = 19\n",
    "hparams_nrms.head_dim = 29\n",
    "hparams_nrms.attention_hidden_dim = 145\n",
    "hparams_nrms.dropout = 0.22088583494496855\n",
    "hparams_nrms.learning_rate = 0.00030309205322750723\n",
    "\n",
    "pytorch_model = NRMSModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "model = NRMSWrapper(pytorch_model)\n",
    "\n",
    "# Then create the callback with the model reference\n",
    "modelcheckpoint = PyTorchModelCheckpoint(\n",
    "    filepath=MODEL_WEIGHTS,\n",
    "    model_wrapper=model,  # Pass the model wrapper\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Training\n",
    "hist = model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=EPOCHS, ### EPOCHS INPUT\n",
    "    callbacks=[modelcheckpoint]\n",
    ")\n",
    "\n",
    "# Load weights using the wrapper\n",
    "model.load_weights(filepath=MODEL_WEIGHTS)\n",
    "\n",
    "# Get predictions\n",
    "pred_validation = model.predict(val_dataloader)\n",
    "#the following example was run on a very very small fraction due to demand on the HPC but shows tre training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 4/4 [00:00<00:00,  5.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>scroll_percentage_fixed</th><th>read_time_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>inview_hour_differences</th><th>inview_article_categories</th><th>history_article_categories</th><th>inview_article_types</th><th>history_article_types</th><th>scores</th><th>is_known_user</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[f32]</td><td>list[f32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>list[i64]</td><td>list[i64]</td><td>list[str]</td><td>list[str]</td><td>list[f64]</td><td>bool</td></tr></thead><tbody><tr><td>2494285</td><td>[9779019, 9778915, … 9779642]</td><td>[100.0, 20.0, … 100.0]</td><td>[115.0, 139.0, … 895.0]</td><td>[9694022, 9783276, … 9783164]</td><td>[9780517]</td><td>96398451</td><td>[0, 0, … 0]</td><td>[1462.956389, 0.287222, … 1.950556]</td><td>[414, 414, … 140]</td><td>[457, 512, … 414]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[0.061735, 0.061008, … 0.060499]</td><td>false</td></tr><tr><td>2412823</td><td>[9774899, 9774972, … 9779748]</td><td>[12.0, 33.0, … 34.0]</td><td>[418.0, 11.0, … 7.0]</td><td>[9786209, 9788462, … 9780702]</td><td>[9786209]</td><td>422073979</td><td>[1, 0, … 0]</td><td>[31.175556, 3.613611, … 8.446944]</td><td>[414, 512, … 457]</td><td>[118, 118, … 414]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[0.071656, 0.078209, … 0.072025]</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 15)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ inview_ar ┆ history_a ┆ scores    ┆ is_known_ │\n",
       "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ticle_typ ┆ rticle_ty ┆ ---       ┆ user      │\n",
       "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ es        ┆ pes       ┆ list[f64] ┆ ---       │\n",
       "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ ---       ┆           ┆ bool      │\n",
       "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[str] ┆ list[str] ┆           ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2494285 ┆ [9779019,  ┆ [100.0,   ┆ [115.0,   ┆ … ┆ [\"article ┆ [\"article ┆ [0.061735 ┆ false     │\n",
       "│         ┆ 9778915, … ┆ 20.0, …   ┆ 139.0, …  ┆   ┆ _default\" ┆ _default\" ┆ ,         ┆           │\n",
       "│         ┆ 9779642]   ┆ 100.0]    ┆ 895.0]    ┆   ┆ , \"articl ┆ , \"articl ┆ 0.061008, ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆ e_def…    ┆ e_def…    ┆ …         ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ 0.060499] ┆           │\n",
       "│ 2412823 ┆ [9774899,  ┆ [12.0,    ┆ [418.0,   ┆ … ┆ [\"article ┆ [\"article ┆ [0.071656 ┆ false     │\n",
       "│         ┆ 9774972, … ┆ 33.0, …   ┆ 11.0, …   ┆   ┆ _default\" ┆ _default\" ┆ ,         ┆           │\n",
       "│         ┆ 9779748]   ┆ 34.0]     ┆ 7.0]      ┆   ┆ , \"articl ┆ , \"articl ┆ 0.078209, ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆ e_def…    ┆ e_def…    ┆ …         ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ 0.072025] ┆           │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_validation = model.scorer.predict(val_dataloader)\n",
    "df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "    add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    ")\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.4622685998004199,\n",
       "    \"mrr\": 0.28988834620846804,\n",
       "    \"ndcg@5\": 0.31347571059689133,\n",
       "    \"ndcg@10\": 0.4098546309246086\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()\n",
    "#This is very bad, but is also only trained on like 0.1% of the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE ⛄️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #The following submission file is on the VALDIAITON set and therefore not valid to submit\n",
    "\n",
    "# df_validation = df_validation.with_columns(\n",
    "#     pl.col(\"scores\")\n",
    "#     .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "#     .alias(\"ranked_scores\")\n",
    "# )\n",
    "# df_validation.head(2)\n",
    "\n",
    "# write_submission_file(\n",
    "#     impression_ids=df_validation[DEFAULT_IMPRESSION_ID_COL],\n",
    "#     prediction_scores=df_validation[\"ranked_scores\"],\n",
    "#     path=\"downloads/predictions.txt\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
