{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we illustrate how to use the Neural News Recommendation with Multi-Head Self-Attention ([NRMS](https://aclanthology.org/D19-1671/)). The implementation is taken from the [recommenders](https://github.com/recommenders-team/recommenders) repository. We have simply stripped the model to keep it cleaner.\n",
    "\n",
    "We use a small dataset, which is downloaded from [recsys.eb.dk](https://recsys.eb.dk/). All the datasets are stored in the folder path ```~/ebnerd_data/*```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 14:22:43.659486: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-06 14:22:43.705339: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-06 14:22:43.705372: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-06 14:22:43.705408: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-06 14:22:43.714965: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-06 14:22:44.993503: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_ARTICLE_ID_COL\n",
    ")\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "from ebrec.models.newsrec.dataloader import NRMSDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms\n",
    "from ebrec.models.newsrec import NRMSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 14:22:46.836412: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# List all physical devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available devices:\", physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL,DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df_behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate labels\n",
    "We sample a few just to get started. For testset we just make up a dummy column with 0 and 1 - this is not the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/ebnerd_data\").expanduser()\n",
    "DATASPLIT = \"ebnerd_small\"\n",
    "DUMP_DIR = PATH.joinpath(\"dump_artifacts\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we sample the dataset, just to keep it smaller. Also, one can simply add the testset similary to the validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 7)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ user_id â”† article_id_f â”† article_ids_ â”† article_ids_ â”† impression_i â”† impression_t â”† labels      â”‚\n",
      "â”‚ ---     â”† ixed         â”† inview       â”† clicked      â”† d            â”† ime          â”† ---         â”‚\n",
      "â”‚ u32     â”† ---          â”† ---          â”† ---          â”† ---          â”† ---          â”† list[i8]    â”‚\n",
      "â”‚         â”† list[i32]    â”† list[i64]    â”† list[i64]    â”† u32          â”† datetime[Î¼s] â”†             â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 1095292 â”† [9768564,    â”† [9773470,    â”† [9773470]    â”† 151515081    â”† 2023-05-21   â”† [1, 0, â€¦ 0] â”‚\n",
      "â”‚         â”† 9769135, â€¦   â”† 9773873, â€¦   â”†              â”†              â”† 04:09:06     â”†             â”‚\n",
      "â”‚         â”† 9770492]     â”† 9773943]     â”†              â”†              â”†              â”†             â”‚\n",
      "â”‚ 2262394 â”† [9750189,    â”† [9769917,    â”† [9771330]    â”† 447709565    â”† 2023-05-18   â”† [0, 0, â€¦ 1] â”‚\n",
      "â”‚         â”† 9749349, â€¦   â”† 9470078, â€¦   â”†              â”†              â”† 16:57:29     â”†             â”‚\n",
      "â”‚         â”† 9770452]     â”† 9771330]     â”†              â”†              â”†              â”†             â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "shape: (2, 7)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ user_id â”† article_id_f â”† article_ids_ â”† article_ids_ â”† impression_i â”† impression_t â”† labels      â”‚\n",
      "â”‚ ---     â”† ixed         â”† inview       â”† clicked      â”† d            â”† ime          â”† ---         â”‚\n",
      "â”‚ u32     â”† ---          â”† ---          â”† ---          â”† ---          â”† ---          â”† list[i8]    â”‚\n",
      "â”‚         â”† list[i32]    â”† list[i32]    â”† list[i32]    â”† u32          â”† datetime[Î¼s] â”†             â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 475656  â”† [9777856,    â”† [9775978,    â”† [9790559]    â”† 417162050    â”† 2023-06-01   â”† [0, 0, â€¦ 0] â”‚\n",
      "â”‚         â”† 9778328, â€¦   â”† 9789494, â€¦   â”†              â”†              â”† 04:24:06     â”†             â”‚\n",
      "â”‚         â”† 9779498]     â”† 9777339]     â”†              â”†              â”†              â”†             â”‚\n",
      "â”‚ 302970  â”† [9776508,    â”† [9782996,    â”† [9783164]    â”† 32922802     â”† 2023-05-27   â”† [0, 0, â€¦ 0] â”‚\n",
      "â”‚         â”† 9776049, â€¦   â”† 9783004, â€¦   â”†              â”†              â”† 09:45:06     â”†             â”‚\n",
      "â”‚         â”† 4108820]     â”† 9781998]     â”†              â”†              â”†              â”†             â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "]\n",
    "HISTORY_SIZE = 20 #20\n",
    "FRACTION =0.05  # 0.01\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "print(df_train.head(2))\n",
    "print(df_validation.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[Î¼s]</td><td>bool</td><td>str</td><td>datetime[Î¼s]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var iâ€¦</td><td>&quot;Politiet frygtâ€¦</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den Ã¸â€¦</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_defaulâ€¦</td><td>&quot;https://ekstraâ€¦</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars â€¦</td><td>&quot;BiografgÃ¦ngernâ€¦</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har â€¦</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_defaulâ€¦</td><td>&quot;https://ekstraâ€¦</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Ã˜konomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 21)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ article_i â”† title     â”† subtitle  â”† last_modi â”† â€¦ â”† total_pag â”† total_rea â”† sentiment â”† sentimen â”‚\n",
       "â”‚ d         â”† ---       â”† ---       â”† fied_time â”†   â”† eviews    â”† d_time    â”† _score    â”† t_label  â”‚\n",
       "â”‚ ---       â”† str       â”† str       â”† ---       â”†   â”† ---       â”† ---       â”† ---       â”† ---      â”‚\n",
       "â”‚ i32       â”†           â”†           â”† datetime[ â”†   â”† i32       â”† f32       â”† f32       â”† str      â”‚\n",
       "â”‚           â”†           â”†           â”† Î¼s]       â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 3001353   â”† Natascha  â”† Politiet  â”† 2023-06-2 â”† â€¦ â”† null      â”† null      â”† 0.9955    â”† Negative â”‚\n",
       "â”‚           â”† var ikke  â”† frygter   â”† 9         â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† den       â”† nu, at    â”† 06:20:33  â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† fÃ¸rste    â”† Nataschaâ€¦ â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 3003065   â”† Kun Star  â”† BiografgÃ¦ â”† 2023-06-2 â”† â€¦ â”† null      â”† null      â”† 0.846     â”† Positive â”‚\n",
       "â”‚           â”† Wars      â”† ngerne    â”† 9         â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† tjente    â”† strÃ¸mmer  â”† 06:20:35  â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† mere      â”† ind forâ€¦  â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(DATASPLIT, \"articles.parquet\"))\n",
    "df_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hourly difference between published and viewed article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## NEW\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Convert polars DataFrame to pandas\n",
    "# df_train = df_train.to_pandas()\n",
    "\n",
    "# # Create a mapping dictionary from article_id to last_modified_time\n",
    "# article_time_dict = df_articles.select(\n",
    "#     \"article_id\", \n",
    "#     \"published_time\"\n",
    "# ).to_dict(as_series=False)\n",
    "# article_time_dict = dict(zip(\n",
    "#     article_time_dict[\"article_id\"], \n",
    "#     article_time_dict[\"published_time\"]\n",
    "# ))\n",
    "\n",
    "# # Create a function to map article IDs to their timestamps\n",
    "# def get_article_times(article_ids):\n",
    "#     return [article_time_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# # Add new column with the published-time\n",
    "# df_train[\"inview_article_times\"] = df_train[\"article_ids_inview\"].apply(get_article_times)\n",
    "\n",
    "# #add new column with the last publish_time for the clicked article\n",
    "# df_train[\"clicked_article_time\"] = df_train[\"article_ids_clicked\"].apply(get_article_times)\n",
    "\n",
    "# # Create a function to calculate hour differences\n",
    "# def calculate_hour_differences(impression_time, article_times):\n",
    "#         # If article_times is a single value (for clicked articles)\n",
    "#     if not isinstance(article_times, list):\n",
    "#         if article_times is None:\n",
    "#             return None\n",
    "#         return (impression_time - article_times).total_seconds() / 3600\n",
    "    \n",
    "#     # If article_times is a list (for inview articles)\n",
    "#     differences = [(impression_time - article_time).total_seconds() / 3600 \n",
    "#                   if article_time is not None else None \n",
    "#                   for article_time in article_times]\n",
    "#     return differences\n",
    "\n",
    "# # Use for inview articles\n",
    "# df_train['inview_hour_differences'] = df_train.apply(\n",
    "#     lambda row: calculate_hour_differences(row['impression_time'], row['inview_article_times']), \n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# # Use for clicked article\n",
    "# df_train['clicked_hour_difference'] = df_train.apply(\n",
    "#    lambda row: calculate_hour_differences(row['impression_time'], row['clicked_article_time']), \n",
    "#    axis=1\n",
    "# )\n",
    "\n",
    "\n",
    "# #drop columns with the time\n",
    "# df_train = df_train.drop(['inview_article_times', 'clicked_article_time','impression_time'], axis=1)\n",
    "\n",
    "# df_train = pl.from_pandas(df_train)\n",
    "\n",
    "# df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert polars DataFrame to pandas\n",
    "# df_validation = df_validation.to_pandas()\n",
    "\n",
    "# # Create a mapping dictionary from article_id to last_modified_time\n",
    "# article_time_dict = df_articles.select(\n",
    "#     \"article_id\", \n",
    "#     \"published_time\"\n",
    "# ).to_dict(as_series=False)\n",
    "# article_time_dict = dict(zip(\n",
    "#     article_time_dict[\"article_id\"], \n",
    "#     article_time_dict[\"published_time\"]\n",
    "# ))\n",
    "\n",
    "# # Create a function to map article IDs to their timestamps\n",
    "# def get_article_times(article_ids):\n",
    "#     return [article_time_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# # Add new column with the published-time\n",
    "# df_validation[\"inview_article_times\"] = df_validation[\"article_ids_inview\"].apply(get_article_times)\n",
    "\n",
    "# #add new column with the last publish_time for the clicked article\n",
    "# df_validation[\"clicked_article_time\"] = df_validation[\"article_ids_clicked\"].apply(get_article_times)\n",
    "\n",
    "# # Create a function to calculate hour differences\n",
    "# def calculate_hour_differences(impression_time, article_times):\n",
    "#         # If article_times is a single value (for clicked articles)\n",
    "#     if not isinstance(article_times, list):\n",
    "#         if article_times is None:\n",
    "#             return None\n",
    "#         return (impression_time - article_times).total_seconds() / 3600\n",
    "    \n",
    "#     # If article_times is a list (for inview articles)\n",
    "#     differences = [(impression_time - article_time).total_seconds() / 3600 \n",
    "#                   if article_time is not None else None \n",
    "#                   for article_time in article_times]\n",
    "#     return differences\n",
    "\n",
    "# # Use for inview articles\n",
    "# df_validation['inview_hour_differences'] = df_validation.apply(\n",
    "#     lambda row: calculate_hour_differences(row['impression_time'], row['inview_article_times']), \n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# # Use for clicked article\n",
    "# df_validation['clicked_hour_difference'] = df_validation.apply(\n",
    "#    lambda row: calculate_hour_differences(row['impression_time'], row['clicked_article_time']), \n",
    "#    axis=1\n",
    "# )\n",
    "\n",
    "# #drop columns with the time\n",
    "# df_validation = df_validation.drop(['inview_article_times', 'clicked_article_time','impression_time'], axis=1)\n",
    "\n",
    "\n",
    "# df_validation = pl.from_pandas(df_validation)\n",
    "\n",
    "# df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model using HuggingFace's tokenizer and wordembedding\n",
    "In the original implementation, they use the GloVe embeddings and tokenizer. To get going fast, we'll use a multilingual LLM from Hugging Face. \n",
    "Utilizing the tokenizer to tokenize the articles and the word-embedding to init NRMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "# TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-large\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=128,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: []\n"
     ]
    }
   ],
   "source": [
    "# List all physical devices\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available devices:\", physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - ETA: 0s - loss: 1.5823\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/NRMS/weights.weights.h5\n",
      "92/92 [==============================] - 800s 9s/step - loss: 1.5823 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "WEIGHTS_DIR = f\"downloads/data/state_dict/{MODEL_NAME}\"\n",
    "MODEL_WEIGHTS = f\"{WEIGHTS_DIR}/weights.weights.h5\"\n",
    "\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# CALLBACKS\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=MODEL_WEIGHTS, save_best_only=True, save_weights_only=True, verbose=1\n",
    ")\n",
    "\n",
    "hparams_nrms.history_size = HISTORY_SIZE\n",
    "model = NRMSModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "hist = model.model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=1,\n",
    "    callbacks=[early_stopping, modelcheckpoint]#tensorboard_callback\n",
    ")\n",
    "_ = model.model.load_weights(filepath=MODEL_WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving / loading model because hpc annoying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_FILE = f\"downloads/models/{MODEL_NAME}.h5\" \n",
    "\n",
    "# # Save the model after training\n",
    "# print(\"Saving the model...\")\n",
    "# os.makedirs(os.path.dirname(MODEL_FILE), exist_ok=True)\n",
    "# model.model.save(MODEL_FILE)  # Save the full model (architecture + weights)\n",
    "# print(f\"Model saved at {MODEL_FILE}\")\n",
    "\n",
    "##LOAD SAVED MODEL\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the saved model\n",
    "# print(f\"Loading the model from {MODEL_FILE}...\")\n",
    "# model.model = load_model(MODEL_FILE)\n",
    "# print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example how to compute some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 614s 3s/step\n"
     ]
    }
   ],
   "source": [
    "pred_validation = model.scorer.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the predictions to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>impression_time</th><th>labels</th><th>scores</th><th>is_known_user</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>datetime[Î¼s]</td><td>list[i8]</td><td>list[f64]</td><td>bool</td></tr></thead><tbody><tr><td>475656</td><td>[9777856, 9778328, â€¦ 9779498]</td><td>[9775978, 9789494, â€¦ 9777339]</td><td>[9790559]</td><td>417162050</td><td>2023-06-01 04:24:06</td><td>[0, 0, â€¦ 0]</td><td>[0.575569, 0.523114, â€¦ 0.459132]</td><td>true</td></tr><tr><td>302970</td><td>[9776508, 9776049, â€¦ 4108820]</td><td>[9782996, 9783004, â€¦ 9781998]</td><td>[9783164]</td><td>32922802</td><td>2023-05-27 09:45:06</td><td>[0, 0, â€¦ 0]</td><td>[0.558757, 0.591558, â€¦ 0.535333]</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 9)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ user_id â”† article_id â”† article_i â”† article_i â”† â€¦ â”† impressio â”† labels    â”† scores    â”† is_known_ â”‚\n",
       "â”‚ ---     â”† _fixed     â”† ds_inview â”† ds_clicke â”†   â”† n_time    â”† ---       â”† ---       â”† user      â”‚\n",
       "â”‚ u32     â”† ---        â”† ---       â”† d         â”†   â”† ---       â”† list[i8]  â”† list[f64] â”† ---       â”‚\n",
       "â”‚         â”† list[i32]  â”† list[i32] â”† ---       â”†   â”† datetime[ â”†           â”†           â”† bool      â”‚\n",
       "â”‚         â”†            â”†           â”† list[i32] â”†   â”† Î¼s]       â”†           â”†           â”†           â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 475656  â”† [9777856,  â”† [9775978, â”† [9790559] â”† â€¦ â”† 2023-06-0 â”† [0, 0, â€¦  â”† [0.575569 â”† true      â”‚\n",
       "â”‚         â”† 9778328, â€¦ â”† 9789494,  â”†           â”†   â”† 1         â”† 0]        â”† ,         â”†           â”‚\n",
       "â”‚         â”† 9779498]   â”† â€¦         â”†           â”†   â”† 04:24:06  â”†           â”† 0.523114, â”†           â”‚\n",
       "â”‚         â”†            â”† 9777339]  â”†           â”†   â”†           â”†           â”† â€¦         â”†           â”‚\n",
       "â”‚         â”†            â”†           â”†           â”†   â”†           â”†           â”† 0.459132] â”†           â”‚\n",
       "â”‚ 302970  â”† [9776508,  â”† [9782996, â”† [9783164] â”† â€¦ â”† 2023-05-2 â”† [0, 0, â€¦  â”† [0.558757 â”† true      â”‚\n",
       "â”‚         â”† 9776049, â€¦ â”† 9783004,  â”†           â”†   â”† 7         â”† 0]        â”† ,         â”†           â”‚\n",
       "â”‚         â”† 4108820]   â”† â€¦         â”†           â”†   â”† 09:45:06  â”†           â”† 0.591558, â”†           â”‚\n",
       "â”‚         â”†            â”† 9781998]  â”†           â”†   â”†           â”†           â”† â€¦         â”†           â”‚\n",
       "â”‚         â”†            â”†           â”†           â”†   â”†           â”†           â”† 0.535333] â”†           â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "    add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    ")\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.5408061672863077,\n",
       "    \"mrr\": 0.3381308530898117,\n",
       "    \"ndcg@5\": 0.37466257308814527,\n",
       "    \"ndcg@10\": 0.45555226935042104\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>impression_time</th><th>labels</th><th>scores</th><th>is_known_user</th><th>ranked_scores</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>datetime[Î¼s]</td><td>list[i8]</td><td>list[f64]</td><td>bool</td><td>list[i64]</td></tr></thead><tbody><tr><td>475656</td><td>[9777856, 9778328, â€¦ 9779498]</td><td>[9775978, 9789494, â€¦ 9777339]</td><td>[9790559]</td><td>417162050</td><td>2023-06-01 04:24:06</td><td>[0, 0, â€¦ 0]</td><td>[0.575569, 0.523114, â€¦ 0.459132]</td><td>true</td><td>[3, 7, â€¦ 12]</td></tr><tr><td>302970</td><td>[9776508, 9776049, â€¦ 4108820]</td><td>[9782996, 9783004, â€¦ 9781998]</td><td>[9783164]</td><td>32922802</td><td>2023-05-27 09:45:06</td><td>[0, 0, â€¦ 0]</td><td>[0.558757, 0.591558, â€¦ 0.535333]</td><td>true</td><td>[6, 4, â€¦ 10]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 10)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ user_id â”† article_id â”† article_i â”† article_i â”† â€¦ â”† labels    â”† scores    â”† is_known_ â”† ranked_sc â”‚\n",
       "â”‚ ---     â”† _fixed     â”† ds_inview â”† ds_clicke â”†   â”† ---       â”† ---       â”† user      â”† ores      â”‚\n",
       "â”‚ u32     â”† ---        â”† ---       â”† d         â”†   â”† list[i8]  â”† list[f64] â”† ---       â”† ---       â”‚\n",
       "â”‚         â”† list[i32]  â”† list[i32] â”† ---       â”†   â”†           â”†           â”† bool      â”† list[i64] â”‚\n",
       "â”‚         â”†            â”†           â”† list[i32] â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 475656  â”† [9777856,  â”† [9775978, â”† [9790559] â”† â€¦ â”† [0, 0, â€¦  â”† [0.575569 â”† true      â”† [3, 7, â€¦  â”‚\n",
       "â”‚         â”† 9778328, â€¦ â”† 9789494,  â”†           â”†   â”† 0]        â”† ,         â”†           â”† 12]       â”‚\n",
       "â”‚         â”† 9779498]   â”† â€¦         â”†           â”†   â”†           â”† 0.523114, â”†           â”†           â”‚\n",
       "â”‚         â”†            â”† 9777339]  â”†           â”†   â”†           â”† â€¦         â”†           â”†           â”‚\n",
       "â”‚         â”†            â”†           â”†           â”†   â”†           â”† 0.459132] â”†           â”†           â”‚\n",
       "â”‚ 302970  â”† [9776508,  â”† [9782996, â”† [9783164] â”† â€¦ â”† [0, 0, â€¦  â”† [0.558757 â”† true      â”† [6, 4, â€¦  â”‚\n",
       "â”‚         â”† 9776049, â€¦ â”† 9783004,  â”†           â”†   â”† 0]        â”† ,         â”†           â”† 10]       â”‚\n",
       "â”‚         â”† 4108820]   â”† â€¦         â”†           â”†   â”†           â”† 0.591558, â”†           â”†           â”‚\n",
       "â”‚         â”†            â”† 9781998]  â”†           â”†   â”†           â”† â€¦         â”†           â”†           â”‚\n",
       "â”‚         â”†            â”†           â”†           â”†   â”†           â”† 0.535333] â”†           â”†           â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation = df_validation.with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the validation, simply add the testset to your flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12232it [00:00, 26241.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/predictions(history 20, ratio 0.1 large).txt to downloads/predictions(history 20, ratio 0.1 large).zip\n"
     ]
    }
   ],
   "source": [
    "write_submission_file(\n",
    "    impression_ids=df_validation[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_validation[\"ranked_scores\"],\n",
    "    path=\"downloads/predictions(history 20, ratio 0.1 large).txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
