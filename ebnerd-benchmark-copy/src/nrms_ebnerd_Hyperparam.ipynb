{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we illustrate how to use the Neural News Recommendation with Multi-Head Self-Attention ([NRMS](https://aclanthology.org/D19-1671/)). The implementation is taken from the [recommenders](https://github.com/recommenders-team/recommenders) repository. We have simply stripped the model to keep it cleaner.\n",
    "\n",
    "We use a small dataset, which is downloaded from [recsys.eb.dk](https://recsys.eb.dk/). All the datasets are stored in the folder path ```~/ebnerd_data/*```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 09:15:43.342618: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 09:15:43.346762: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 09:15:43.396377: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-15 09:15:43.396441: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-15 09:15:43.396477: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 09:15:43.409085: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 09:15:44.688565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_ARTICLE_ID_COL,\n",
    "    DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL, #-------\n",
    "    DEFAULT_HISTORY_READ_TIME_COL #-------\n",
    ")\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "from ebrec.models.newsrec.dataloader import NRMSDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms\n",
    "from ebrec.models.newsrec import NRMSModel, NRMSWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Available devices: []\n",
      "____————____————____————____———\n",
      "HISTORY_SIZE: 30\n",
      "FRACTION: 0.2\n",
      "____————____————____————____———\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 09:15:46.741277: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 9)\n",
      "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ article_i ┆ impressio ┆ impressio ┆ labels    │\n",
      "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ds_clicke ┆ n_id      ┆ n_time    ┆ ---       │\n",
      "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ d         ┆ ---       ┆ ---       ┆ list[i8]  │\n",
      "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ u32       ┆ datetime[ ┆           │\n",
      "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i64] ┆           ┆ μs]       ┆           │\n",
      "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 1610498 ┆ [9764086,  ┆ [100.0,   ┆ [27.0,    ┆ … ┆ [9772813] ┆ 225635116 ┆ 2023-05-1 ┆ [0, 1, …  │\n",
      "│         ┆ 9764008, … ┆ 52.0, …   ┆ 5.0, …    ┆   ┆           ┆           ┆ 9         ┆ 0]        │\n",
      "│         ┆ 9766873]   ┆ 32.0]     ┆ 33.0]     ┆   ┆           ┆           ┆ 20:49:08  ┆           │\n",
      "│ 1690637 ┆ [9743702,  ┆ [48.0,    ┆ [1.0,     ┆ … ┆ [9776385] ┆ 563717511 ┆ 2023-05-2 ┆ [0, 0, …  │\n",
      "│         ┆ 9758561, … ┆ 44.0, …   ┆ 9.0, …    ┆   ┆           ┆           ┆ 2         ┆ 1]        │\n",
      "│         ┆ 9767646]   ┆ 62.0]     ┆ 2.0]      ┆   ┆           ┆           ┆ 23:28:45  ┆           │\n",
      "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n",
      "shape: (2, 9)\n",
      "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ article_i ┆ impressio ┆ impressio ┆ labels    │\n",
      "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ds_clicke ┆ n_id      ┆ n_time    ┆ ---       │\n",
      "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ d         ┆ ---       ┆ ---       ┆ list[i8]  │\n",
      "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ u32       ┆ datetime[ ┆           │\n",
      "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i32] ┆           ┆ μs]       ┆           │\n",
      "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 2302800 ┆ [9750718,  ┆ [100.0,   ┆ [302.0,   ┆ … ┆ [9788720] ┆ 280015603 ┆ 2023-05-3 ┆ [1, 0, …  │\n",
      "│         ┆ 9750789, … ┆ 97.0, …   ┆ 19.0, …   ┆   ┆           ┆           ┆ 0         ┆ 0]        │\n",
      "│         ┆ 9778728]   ┆ 100.0]    ┆ 51.0]     ┆   ┆           ┆           ┆ 15:32:53  ┆           │\n",
      "│ 2555765 ┆ [9778369,  ┆ [100.0,   ┆ [28.0,    ┆ … ┆ [9788766] ┆ 213466077 ┆ 2023-05-3 ┆ [1, 0, …  │\n",
      "│         ┆ 9778381, … ┆ 51.0, …   ┆ 3.0, …    ┆   ┆           ┆           ┆ 1         ┆ 0]        │\n",
      "│         ┆ 9772548]   ┆ 100.0]    ┆ 1079.0]   ┆   ┆           ┆           ┆ 20:06:00  ┆           │\n",
      "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var i…</td><td>&quot;Politiet frygt…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den ø…</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars …</td><td>&quot;Biografgængern…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har …</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natascha… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind for…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "# Setup and load everything\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "print(\"Loading data\")\n",
    "\n",
    "# List all physical devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available devices:\", physical_devices)\n",
    "\n",
    "\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "## Load dataset\n",
    "# #-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        # .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL,DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL)\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL,DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL,DEFAULT_HISTORY_READ_TIME_COL) #------------\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df_behaviors\n",
    "  \n",
    "  \n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "  ### Generate labels\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "# We sample a few just to get started. For testset we just make up a dummy column with 0 and 1 - this is not the true labels.\n",
    "\n",
    "PATH = Path(\"/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/ebnerd_data\").expanduser()\n",
    "DATASPLIT = \"ebnerd_small\" # TODO if change to change make_embedding_artifacts.ipynb file (embeddings)\n",
    "\n",
    "# DATASPLIT = \"ebnerd__testset\"\n",
    "DUMP_DIR = PATH.joinpath(\"dump_artifacts\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "#In this example we sample the dataset, just to keep it smaller. Also, one can simply add the testset similary to the validation.\n",
    "\n",
    "### Define the Data Cols -- New ones here\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL, #--------neu \n",
    "    DEFAULT_HISTORY_READ_TIME_COL, #------- neu\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "]\n",
    "\n",
    "#____————____————____————____————\n",
    "# Define the history size and fraction\n",
    "# ____————____————____————____————\n",
    "HISTORY_SIZE = 30 #30\n",
    "FRACTION = 0.2 #Fraction af datasæt\n",
    "\n",
    "#____————____————____————____————\n",
    "\n",
    "print(\"____————____————____————____———\")\n",
    "print(\"HISTORY_SIZE:\", HISTORY_SIZE)\n",
    "print(\"FRACTION:\", FRACTION)\n",
    "print(\"____————____————____————____———\")\n",
    "print(\"\")\n",
    "\n",
    "#____————____————____————____————\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=6,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "print(df_train.head(2))\n",
    "print(df_validation.head(2))\n",
    "\n",
    "\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "## Load articles\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "df_articles = pl.read_parquet(PATH.joinpath(DATASPLIT, \"articles.parquet\"))\n",
    "df_articles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>scroll_percentage_fixed</th><th>read_time_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>inview_hour_differences</th><th>inview_article_categories</th><th>history_article_categories</th><th>inview_article_types</th><th>history_article_types</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[f32]</td><td>list[f32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>list[i64]</td><td>list[i64]</td><td>list[str]</td><td>list[str]</td></tr></thead><tbody><tr><td>2302800</td><td>[9750718, 9750789, … 9778728]</td><td>[100.0, 97.0, … 100.0]</td><td>[302.0, 19.0, … 51.0]</td><td>[9788720, 9788183, … 9788752]</td><td>[9788720]</td><td>280015603</td><td>[1, 0, … 0]</td><td>[0.249167, 0.992222, … 0.128611]</td><td>[142, 414, … 140]</td><td>[118, 414, … 142]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td></tr><tr><td>2555765</td><td>[9778369, 9778381, … 9772548]</td><td>[100.0, 51.0, … 100.0]</td><td>[28.0, 3.0, … 1079.0]</td><td>[9788766, 9790548, … 9788794]</td><td>[9788766]</td><td>213466077</td><td>[1, 0, … 0]</td><td>[2.476111, 3.537222, … 27.857778]</td><td>[2975, 140, … 142]</td><td>[142, 142, … 142]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td><td>[&quot;article_default&quot;, &quot;article_default&quot;, … &quot;article_default&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 13)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ scroll_pe ┆ read_time ┆ … ┆ inview_ar ┆ history_a ┆ inview_ar ┆ history_a │\n",
       "│ ---     ┆ _fixed     ┆ rcentage_ ┆ _fixed    ┆   ┆ ticle_cat ┆ rticle_ca ┆ ticle_typ ┆ rticle_ty │\n",
       "│ u32     ┆ ---        ┆ fixed     ┆ ---       ┆   ┆ egories   ┆ tegories  ┆ es        ┆ pes       │\n",
       "│         ┆ list[i32]  ┆ ---       ┆ list[f32] ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│         ┆            ┆ list[f32] ┆           ┆   ┆ list[i64] ┆ list[i64] ┆ list[str] ┆ list[str] │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2302800 ┆ [9750718,  ┆ [100.0,   ┆ [302.0,   ┆ … ┆ [142,     ┆ [118,     ┆ [\"article ┆ [\"article │\n",
       "│         ┆ 9750789, … ┆ 97.0, …   ┆ 19.0, …   ┆   ┆ 414, …    ┆ 414, …    ┆ _default\" ┆ _default\" │\n",
       "│         ┆ 9778728]   ┆ 100.0]    ┆ 51.0]     ┆   ┆ 140]      ┆ 142]      ┆ , \"articl ┆ , \"articl │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ e_def…    ┆ e_def…    │\n",
       "│ 2555765 ┆ [9778369,  ┆ [100.0,   ┆ [28.0,    ┆ … ┆ [2975,    ┆ [142,     ┆ [\"article ┆ [\"article │\n",
       "│         ┆ 9778381, … ┆ 51.0, …   ┆ 3.0, …    ┆   ┆ 140, …    ┆ 142, …    ┆ _default\" ┆ _default\" │\n",
       "│         ┆ 9772548]   ┆ 100.0]    ┆ 1079.0]   ┆   ┆ 142]      ┆ 142]      ┆ , \"articl ┆ , \"articl │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ e_def…    ┆ e_def…    │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "# Loading the article embeddings and other feature stuff\n",
    "\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "### Added features and hourly difference between published and viewed article\n",
    "\n",
    "## NEW\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert polars DataFrame to pandas\n",
    "df_train = df_train.to_pandas()\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_time\n",
    "article_time_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"published_time\"\n",
    ").to_dict(as_series=False)\n",
    "article_time_dict = dict(zip(\n",
    "    article_time_dict[\"article_id\"], \n",
    "    article_time_dict[\"published_time\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their timestamps\n",
    "def get_article_times(article_ids):\n",
    "    return [article_time_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the published-time\n",
    "df_train[\"inview_article_times\"] = df_train[\"article_ids_inview\"].apply(get_article_times)\n",
    "\n",
    "#add new column with the last publish_time for the clicked article\n",
    "df_train[\"clicked_article_time\"] = df_train[\"article_ids_clicked\"].apply(get_article_times)\n",
    "\n",
    "# Create a function to calculate hour differences\n",
    "def calculate_hour_differences(impression_time, article_times):\n",
    "        # If article_times is a single value (for clicked articles)\n",
    "    if not isinstance(article_times, list):\n",
    "        if article_times is None:\n",
    "            return None\n",
    "        return (impression_time - article_times).total_seconds() / 3600\n",
    "    \n",
    "    # If article_times is a list (for inview articles)\n",
    "    differences = [(impression_time - article_time).total_seconds() / 3600 \n",
    "                  if article_time is not None else None \n",
    "                  for article_time in article_times]\n",
    "    return differences\n",
    "\n",
    "# Use for inview articles\n",
    "df_train['inview_hour_differences'] = df_train.apply(\n",
    "    lambda row: calculate_hour_differences(row['impression_time'], row['inview_article_times']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# # Use for clicked article\n",
    "# df_train['clicked_hour_difference'] = df_train.apply(\n",
    "#    lambda row: calculate_hour_differences(row['impression_time'], row['clicked_article_time']), \n",
    "#    axis=1\n",
    "# )\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_category\n",
    "article_cat_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"category\"\n",
    ").to_dict(as_series=False)\n",
    "article_cat_dict = dict(zip(\n",
    "    article_cat_dict[\"article_id\"], \n",
    "    article_cat_dict[\"category\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their category\n",
    "def get_article_category(article_ids):\n",
    "    return [article_cat_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "#  Add new column with the article category\n",
    "df_train[\"inview_article_categories\"] = df_train[\"article_ids_inview\"].apply(get_article_category)\n",
    "\n",
    "df_train[\"history_article_categories\"] = df_train[\"article_id_fixed\"].apply(get_article_category)\n",
    "\n",
    "# Create a mapping dictionary from article_id to article_type\n",
    "article_type_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"article_type\"\n",
    ").to_dict(as_series=False)\n",
    "article_type_dict = dict(zip(\n",
    "    article_type_dict[\"article_id\"], \n",
    "    article_type_dict[\"article_type\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their type\n",
    "def get_article_type(article_ids):\n",
    "    return [article_type_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the article type\n",
    "df_train[\"inview_article_types\"] = df_train[\"article_ids_inview\"].apply(get_article_type)\n",
    "\n",
    "df_train[\"history_article_types\"] = df_train[\"article_id_fixed\"].apply(get_article_type)\n",
    "\n",
    "#drop columns with the time\n",
    "df_train = df_train.drop(['inview_article_times', 'clicked_article_time','impression_time'], axis=1)\n",
    "\n",
    "df_train = pl.from_pandas(df_train)\n",
    "\n",
    "df_train.head(2)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "# Convert polars DataFrame to pandas\n",
    "df_validation = df_validation.to_pandas()\n",
    "\n",
    "# Create a mapping dictionary from article_id to last_modified_time\n",
    "article_time_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"published_time\"\n",
    ").to_dict(as_series=False)\n",
    "article_time_dict = dict(zip(\n",
    "    article_time_dict[\"article_id\"], \n",
    "    article_time_dict[\"published_time\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their timestamps\n",
    "def get_article_times(article_ids):\n",
    "    return [article_time_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the published-time\n",
    "df_validation[\"inview_article_times\"] = df_validation[\"article_ids_inview\"].apply(get_article_times)\n",
    "\n",
    "#add new column with the last publish_time for the clicked article\n",
    "df_validation[\"clicked_article_time\"] = df_validation[\"article_ids_clicked\"].apply(get_article_times)\n",
    "\n",
    "# Create a function to calculate hour differences\n",
    "def calculate_hour_differences(impression_time, article_times):\n",
    "        # If article_times is a single value (for clicked articles)\n",
    "    if not isinstance(article_times, list):\n",
    "        if article_times is None:\n",
    "            return None\n",
    "        return (impression_time - article_times).total_seconds() / 3600\n",
    "    \n",
    "    # If article_times is a list (for inview articles)\n",
    "    differences = [(impression_time - article_time).total_seconds() / 3600 \n",
    "                  if article_time is not None else None \n",
    "                  for article_time in article_times]\n",
    "    return differences\n",
    "\n",
    "# Use for inview articles\n",
    "df_validation['inview_hour_differences'] = df_validation.apply(\n",
    "    lambda row: calculate_hour_differences(row['impression_time'], row['inview_article_times']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# # Use for clicked article -- might be leaky??\n",
    "# df_validation['clicked_hour_difference'] = df_validation.apply(\n",
    "#    lambda row: calculate_hour_differences(row['impression_time'], row['clicked_article_time']), \n",
    "#    axis=1\n",
    "# )\n",
    "# Create a mapping dictionary from article_id to last_modified_category\n",
    "article_cat_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"category\"\n",
    ").to_dict(as_series=False)\n",
    "article_cat_dict = dict(zip(\n",
    "    article_cat_dict[\"article_id\"], \n",
    "    article_cat_dict[\"category\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their category\n",
    "def get_article_category(article_ids):\n",
    "    return [article_cat_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "#  Add new column with the article category\n",
    "df_validation[\"inview_article_categories\"] = df_validation[\"article_ids_inview\"].apply(get_article_category)\n",
    "\n",
    "df_validation[\"history_article_categories\"] = df_validation[\"article_id_fixed\"].apply(get_article_category)\n",
    "\n",
    "# Create a mapping dictionary from article_id to article_type\n",
    "article_type_dict = df_articles.select(\n",
    "    \"article_id\", \n",
    "    \"article_type\"\n",
    ").to_dict(as_series=False)\n",
    "article_type_dict = dict(zip(\n",
    "    article_type_dict[\"article_id\"], \n",
    "    article_type_dict[\"article_type\"]\n",
    "))\n",
    "\n",
    "# Create a function to map article IDs to their type\n",
    "def get_article_type(article_ids):\n",
    "    return [article_type_dict.get(aid, None) for aid in article_ids]\n",
    "\n",
    "# Add new column with the article type\n",
    "df_validation[\"inview_article_types\"] = df_validation[\"article_ids_inview\"].apply(get_article_type)\n",
    "\n",
    "df_validation[\"history_article_types\"] = df_validation[\"article_id_fixed\"].apply(get_article_type)\n",
    "\n",
    "\n",
    "#drop columns with the time\n",
    "df_validation = df_validation.drop(['inview_article_times', 'clicked_article_time','impression_time'], axis=1)\n",
    "\n",
    "\n",
    "df_validation = pl.from_pandas(df_validation)\n",
    "\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____————____————____————____———\n",
      "Using transformer model: Maltehb/danish-bert-botxo\n",
      "____————____————____————____———\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "## Init model using HuggingFace's tokenizer and wordembedding\n",
    "\n",
    "# In the original implementation, they use the GloVe embeddings and tokenizer. To get going fast, we'll use a multilingual LLM from Hugging Face. \n",
    "# Utilizing the tokenizer to tokenize the articles and the word-embedding to init NRMS.\n",
    "#-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "# TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "# TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-robe rta-large\"\n",
    "# TRANSFORMER_MODEL_NAME = \"google-bert/bert-base-multilingual-uncased\" \n",
    "#Argue for cased vs uncased.  TODO\n",
    "# #Cased might be better but to compare with malteHb we use uncased\n",
    "\n",
    "TRANSFORMER_MODEL_NAME = \"Maltehb/danish-bert-botxo\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30 #hardcoded somewhere ?? error if change\n",
    "\n",
    "print(\"\")\n",
    "print(\"____————____————____————____———\")\n",
    "print(\"Using transformer model:\", TRANSFORMER_MODEL_NAME)\n",
    "print(\"____————____————____————____———\")\n",
    "print(\"\")\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "\n",
    "\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")\n",
    "\n",
    "#_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "# print(\"df_train columns:\", df_train.columns)\n",
    "# print(\"df_validation columns:\", df_validation.columns)\n",
    "#_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=128,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: []\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# List all physical devices\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available devices:\", physical_devices)\n",
    "import torch.nn as nn\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSWrapper init\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 [Train]:   4%|▍         | 30/733 [00:04<01:51,  6.28it/s, loss=0.4383]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 [Train]: 100%|██████████| 733/733 [02:37<00:00,  4.64it/s, loss=0.3889]\n",
      "Epoch 1/4 [Valid]: 100%|██████████| 1530/1530 [06:09<00:00,  4.14it/s, loss=0.6621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.3889, Val Loss: 0.3172\n",
      "\n",
      "Validation loss improved from inf to 0.31719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 [Train]: 100%|██████████| 733/733 [03:11<00:00,  3.83it/s, loss=0.3791]\n",
      "Epoch 2/4 [Valid]: 100%|██████████| 1530/1530 [08:58<00:00,  2.84it/s, loss=0.6765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 0.3791, Val Loss: 0.3241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 [Train]: 100%|██████████| 733/733 [03:21<00:00,  3.64it/s, loss=0.3720]\n",
      "Epoch 3/4 [Valid]: 100%|██████████| 1530/1530 [10:02<00:00,  2.54it/s, loss=0.6823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 0.3720, Val Loss: 0.3269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4 [Train]: 100%|██████████| 733/733 [04:02<00:00,  3.03it/s, loss=0.3676]\n",
      "Epoch 4/4 [Valid]: 100%|██████████| 1530/1530 [10:07<00:00,  2.52it/s, loss=0.7182]\n",
      "/dtu/blackhole/14/155764/DeepL-Project-Corn2/ebnerd-benchmark-copy/src/ebrec/models/newsrec/nrmspy_1.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(filepath))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 0.3676, Val Loss: 0.3441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1530/1530 [11:18<00:00,  2.26it/s]\n"
     ]
    }
   ],
   "source": [
    "#_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "# Original Model\n",
    "#_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
    "\n",
    "# Works fine -- Can change epochs on line 67\n",
    "\n",
    "# import os\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# MODEL_NAME = \"NRMS\"\n",
    "# LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "# WEIGHTS_DIR = f\"downloads/data/state_dict/{MODEL_NAME}\"\n",
    "# MODEL_WEIGHTS = f\"{WEIGHTS_DIR}/weights.pt\"\n",
    "\n",
    "# os.makedirs(LOG_DIR, exist_ok=True)\n",
    "# os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# # Create a custom ModelCheckpoint for PyTorch\n",
    "# class PyTorchModelCheckpoint:\n",
    "#     def __init__(self, filepath, model_wrapper=None, save_best_only=True, save_weights_only=True, verbose=1):\n",
    "#         self.filepath = filepath\n",
    "#         self.model_wrapper = model_wrapper  # Store the model wrapper reference\n",
    "#         self.save_best_only = save_best_only\n",
    "#         self.save_weights_only = save_weights_only\n",
    "#         self.verbose = verbose\n",
    "#         self.best_val_loss = float('inf')\n",
    "    \n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         val_loss = logs.get('val_loss', None)\n",
    "#         if val_loss is None:\n",
    "#             return\n",
    "        \n",
    "#         if self.save_best_only:\n",
    "#             if val_loss < self.best_val_loss:\n",
    "#                 if self.verbose:\n",
    "#                     print(f'\\nValidation loss improved from {self.best_val_loss:.5f} to {val_loss:.5f}')\n",
    "#                 self.best_val_loss = val_loss\n",
    "#                 # Use the model_wrapper reference\n",
    "#                 self.model_wrapper.save_weights(self.filepath)\n",
    "#         else:\n",
    "#             self.model_wrapper.save_weights(self.filepath)\n",
    "\n",
    "# # Initialize model first\n",
    "# hparams_nrms.history_size = HISTORY_SIZE\n",
    "\n",
    "# pytorch_model = NRMSModel(\n",
    "#     hparams=hparams_nrms,\n",
    "#     word2vec_embedding=word2vec_embedding,\n",
    "#     seed=42,\n",
    "# )\n",
    "# model = NRMSWrapper(pytorch_model)\n",
    "\n",
    "# # Then create the callback with the model reference\n",
    "# modelcheckpoint = PyTorchModelCheckpoint(\n",
    "#     filepath=MODEL_WEIGHTS,\n",
    "#     model_wrapper=model,  # Pass the model wrapper\n",
    "#     save_best_only=True,\n",
    "#     save_weights_only=True,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Training\n",
    "# hist = model.fit(\n",
    "#     train_dataloader,\n",
    "#     validation_data=val_dataloader,\n",
    "#     epochs=4, ### EPOCHS INPUT\n",
    "#     callbacks=[modelcheckpoint]\n",
    "# )\n",
    "\n",
    "# # Load weights using the wrapper\n",
    "# model.load_weights(filepath=MODEL_WEIGHTS)\n",
    "\n",
    "# # Get predictions\n",
    "# pred_validation = model.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(outputs, labels):\n",
    "    # AUC\n",
    "    auc = roc_auc_score(labels, outputs)\n",
    "\n",
    "    # MRR\n",
    "    sorted_indices = np.argsort(outputs)[::-1]\n",
    "    ranks = np.where(labels[sorted_indices] == 1)[0] + 1  # 1-based ranks\n",
    "    mrr = 1 / ranks[0] if len(ranks) > 0 else 0\n",
    "\n",
    "    # NDCG@5\n",
    "    top_k = 5\n",
    "    top_k_indices = sorted_indices[:top_k]\n",
    "    ideal_dcg = sum(1 / np.log2(i + 2) for i in range(min(top_k, np.sum(labels))))\n",
    "    dcg = sum(\n",
    "        labels[i] / np.log2(rank + 2)\n",
    "        for rank, i in enumerate(top_k_indices)\n",
    "    )\n",
    "    ndcg = dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "    return auc, mrr, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 09:30:23,440] A new study created in memory with name: no-name-34772e3d-115c-473a-8dc4-cf69a0307f01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bff5aa5c284db1bd4112bcad909ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import torch\n",
    "hparams_nrms.history_size = HISTORY_SIZE # JUST leave this here -- it is annoying\n",
    "\n",
    "# ___———___———___———___———___\n",
    "# INPUTS\n",
    "EPOCHSS = 2\n",
    "NUM_TRIALS = 10 #Change according to how many iterations of hyperparameter tuning you want\n",
    "            # techmically it is how many branches of tree you want to explore so u dont run all\n",
    "# ___———___———___———___———___\n",
    "\n",
    "# Theres a bunch of stuff to do for the objective funciton but i tried keeping it basic -- maybe do multi objective -- but then again how interested are we in MRR or somthin else -- maybe minimize loss but idk if you can maximize one thing and minimize another -- i guess you could just maximize -loss\n",
    "def objective(trial):\n",
    "    # Change theese? \n",
    "    hparams_nrms.head_num = trial.suggest_int('head_num', 10, 30)\n",
    "    hparams_nrms.head_dim = trial.suggest_int('head_dim', 10, 30)\n",
    "    hparams_nrms.attention_hidden_dim = trial.suggest_int('attention_hidden_dim', 100, 300)\n",
    "    hparams_nrms.dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    hparams_nrms.learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    \n",
    "    # DEFAULT HPARAM CLASS (src/ebrec/models/newsrec/model_config.py):\n",
    "                    # class hparams_nrms:\n",
    "                        # # INPUT DIMENTIONS:\n",
    "                        # title_size: int = DEFAULT_TITLE_SIZE  -- Hardcoded somwhere i think\n",
    "                        # history_size: int = 50                -- NO need to reload all data for this\n",
    "                        # # MODEL ARCHITECTURE\n",
    "                        # head_num: int = 20                   -- included\n",
    "                        # head_dim: int = 20                   -- included\n",
    "                        # attention_hidden_dim: int = 200      - included\n",
    "                        # # MODEL OPTIMIZER:\n",
    "                        # optimizer: str = \"adam\"              -- possible?        \n",
    "                        # loss: str = \"cross_entropy_loss\"     -- possible?\n",
    "                        # dropout: float = 0.2                 -- included\n",
    "                        # learning_rate: float = 0.0001        -- included\n",
    "\n",
    "\n",
    "    # Initialize the model -- same as b4\n",
    "    pytorch_model = NRMSModel(\n",
    "        hparams=hparams_nrms,\n",
    "        word2vec_embedding=word2vec_embedding,\n",
    "        seed=42,\n",
    "    )\n",
    "    model = NRMSWrapper(pytorch_model)\n",
    "\n",
    "    # get model params from model.model.parameters() basically\n",
    "    optimizer = torch.optim.Adam(model.model.parameters(), lr=hparams_nrms.learning_rate)\n",
    "    # loss_fn = torch.nn.BCELoss()  # Use the appropriate loss function for your problem\n",
    "    loss_fn = torch.nn.CrossEntropyLoss() \n",
    "        #alternatives are:\n",
    "            # BCEWithLogitsLoss() - combines a sigmoid layer and the BCELoss in one single class\n",
    "            # CrossEntropyLoss() - combines LogSoftmax and NLLLoss in one single class\n",
    "            # KLDivLoss() - the Kullback-Leibler divergence loss\n",
    "            # MSELoss() - the mean squared error loss\n",
    "            # NLLLoss() - the negative log likelihood loss\n",
    "            \n",
    "            #But does it make sense to change theese since our model is based on CEL?\n",
    "            #cuz like i think it should be the same as the model right....?\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHSS): \n",
    "        model.model.train()\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            history, candidates = inputs\n",
    "\n",
    "            history = torch.from_numpy(history).to(model.device)\n",
    "            candidates = torch.from_numpy(candidates).to(model.device)\n",
    "            labels = torch.from_numpy(labels).float().to(model.device)\n",
    "\n",
    "            # Forward pass using the underlying model\n",
    "            try:\n",
    "                outputs = model.model(history, candidates, training=True)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"History shape: {history.shape}\")\n",
    "                print(f\"Candidates shape: {candidates.shape}\")\n",
    "                raise e\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.model.eval()  # Now use evalation mode\n",
    "    all_outputs, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            history, candidates = inputs\n",
    "            history = torch.from_numpy(history).to(model.device)\n",
    "            candidates = torch.from_numpy(candidates).to(model.device)\n",
    "            labels = torch.from_numpy(labels).float().to(model.device)\n",
    "\n",
    "            outputs = model.model(history, candidates, training=False)\n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "    # Calculate AUC\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    auc = roc_auc_score(all_labels, all_outputs)\n",
    "\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    # Here we optimize for AUC, but can also do other stuf\n",
    "    return auc\n",
    "\n",
    "# Create a single-objective Optuna 'study'\n",
    "study = optuna.create_study(direction='maximize')  # Maximize AUC -- dont minimize it ;) \n",
    "study.optimize(objective, n_trials=NUM_TRIALS, show_progress_bar=True)\n",
    "\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(f\"Best AUC (train): {study.best_value:.4f}\")\n",
    "\n",
    "# Print best trial stuff -- basically just claude\n",
    "def evaluate_best_trial(study, MAKE_SUBMISSIONS=False):\n",
    "    # Get best trial hyperparameters\n",
    "    best_trial = study.best_trial\n",
    "    hparams_nrms.head_num = best_trial.params['head_num']\n",
    "    hparams_nrms.head_dim = best_trial.params['head_dim']\n",
    "    hparams_nrms.attention_hidden_dim = best_trial.params['attention_hidden_dim']\n",
    "    hparams_nrms.dropout = best_trial.params['dropout']\n",
    "    hparams_nrms.learning_rate = best_trial.params['learning_rate']\n",
    "\n",
    "    # TODO couldt think of sometihng better than to just retrain it ;)\n",
    "    pytorch_model = NRMSModel(\n",
    "        hparams=hparams_nrms,\n",
    "        word2vec_embedding=word2vec_embedding,\n",
    "        seed=42,\n",
    "    )\n",
    "    model = NRMSWrapper(pytorch_model)\n",
    "    model.model.eval()\n",
    "\n",
    "    # Generate predictions for the validation set\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in val_dataloader:\n",
    "            history, candidates = inputs\n",
    "            history = torch.from_numpy(history).to(model.device)\n",
    "            candidates = torch.from_numpy(candidates).to(model.device)\n",
    "            outputs = model.model(history, candidates, training=False)\n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "\n",
    "    # Add predictions to the validation DataFrame\n",
    "    pred_validation = np.concatenate(all_outputs)\n",
    "    df_validation_with_preds = add_prediction_scores(\n",
    "        df_validation, pred_validation.tolist()\n",
    "    ).pipe(\n",
    "        add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    "    )\n",
    "\n",
    "    # Metrics\n",
    "    metrics = MetricEvaluator(\n",
    "        labels=df_validation_with_preds[\"labels\"].to_list(),\n",
    "        predictions=df_validation_with_preds[\"scores\"].to_list(),\n",
    "        metric_functions=[\n",
    "            AucScore(),\n",
    "            MrrScore(),\n",
    "            NdcgScore(k=5),\n",
    "            NdcgScore(k=10),\n",
    "        ],\n",
    "    )\n",
    "    metric_results = metrics.evaluate()\n",
    "    \n",
    "    # MAKE SUBMISSIONS FILE!!!!\n",
    "\n",
    "    if MAKE_SUBMISSIONS:\n",
    "        # Rank the predictions\n",
    "        df_validation_with_preds = df_validation_with_preds.with_columns(pl.col(\"scores\").map_elements(lambda x: list(rank_predictions_by_score(x))).alias(\"ranked_scores\"))\n",
    "\n",
    "        write_submission_file(\n",
    "        impression_ids=df_validation_with_preds[DEFAULT_IMPRESSION_ID_COL],\n",
    "        prediction_scores=df_validation_with_preds[\"ranked_scores\"],\n",
    "        path=\"downloads/predictions.txt\",\n",
    "        )  \n",
    "        print(\"Submission file created!\")\n",
    "    \n",
    "    return metric_results\n",
    "\n",
    "# Evaluate metrics for the best trial\n",
    "best_trial_metrics = evaluate_best_trial(study)\n",
    "print(\"Metrics for Best AUC Trial:\", best_trial_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48929it [00:01, 31023.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/predictions.txt to downloads/predictions.zip\n",
      "Submission file created!\n"
     ]
    }
   ],
   "source": [
    "best_trial_metrics = evaluate_best_trial(study, MAKE_SUBMISSIONS=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate parameter importances with only a single trial.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     edf_plot \u001b[38;5;241m=\u001b[39m vis\u001b[38;5;241m.\u001b[39mplot_edf(study)\n\u001b[1;32m     32\u001b[0m     edf_plot\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 34\u001b[0m \u001b[43mgenerate_optuna_plots\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mgenerate_optuna_plots\u001b[0;34m(study)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mGenerate and display multiple Optuna visualization plots.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    study (optuna.study.Study): The Optuna study object to visualize\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 1. Param Importances Plot\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m param_importances \u001b[38;5;241m=\u001b[39m \u001b[43mvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_param_importances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m param_importances\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 2. Optimization History Plot\u001b[39;00m\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/optuna/visualization/_param_importances.py:168\u001b[0m, in \u001b[0;36mplot_param_importances\u001b[0;34m(study, evaluator, params, target, target_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Plot hyperparameter importances.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m.. seealso::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m _imports\u001b[38;5;241m.\u001b[39mcheck()\n\u001b[0;32m--> 168\u001b[0m importances_infos \u001b[38;5;241m=\u001b[39m \u001b[43m_get_importances_infos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_importances_plot(importances_infos, study)\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/optuna/visualization/_param_importances.py:82\u001b[0m, in \u001b[0;36m_get_importances_infos\u001b[0;34m(study, evaluator, params, target, target_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m study\u001b[38;5;241m.\u001b[39m_is_multi_objective():\n\u001b[1;32m     80\u001b[0m     target_name \u001b[38;5;241m=\u001b[39m metric_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m metric_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m target \u001b[38;5;28;01melse\u001b[39;00m target_name\n\u001b[1;32m     81\u001b[0m     importances_infos: \u001b[38;5;28mtuple\u001b[39m[_ImportancesInfo, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 82\u001b[0m         \u001b[43m_get_importances_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     n_objectives \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mdirections)\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/optuna/visualization/_param_importances.py:54\u001b[0m, in \u001b[0;36m_get_importances_info\u001b[0;34m(study, evaluator, params, target, target_name)\u001b[0m\n\u001b[1;32m     46\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudy instance does not contain completed trials.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ImportancesInfo(\n\u001b[1;32m     48\u001b[0m         importance_values\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m     49\u001b[0m         param_names\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m     50\u001b[0m         importance_labels\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m     51\u001b[0m         target_name\u001b[38;5;241m=\u001b[39mtarget_name,\n\u001b[1;32m     52\u001b[0m     )\n\u001b[0;32m---> 54\u001b[0m importances \u001b[38;5;241m=\u001b[39m \u001b[43moptuna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimportance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_param_importances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m importances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(importances\u001b[38;5;241m.\u001b[39mitems())))\n\u001b[1;32m     59\u001b[0m importance_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(importances\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/optuna/importance/__init__.py:111\u001b[0m, in \u001b[0;36mget_param_importances\u001b[0;34m(study, evaluator, params, target, normalize)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(evaluator, BaseImportanceEvaluator):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluator must be a subclass of BaseImportanceEvaluator.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 111\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[1;32m    113\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(res\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/optuna/importance/_fanova/_evaluator.py:89\u001b[0m, in \u001b[0;36mFanovaImportanceEvaluator.evaluate\u001b[0;34m(self, study, params, target)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m study\u001b[38;5;241m.\u001b[39m_is_multi_objective():\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the `study` is being used for multi-objective optimization, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease specify the `target`. For example, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`target=lambda t: t.values[0]` for the first objective value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 89\u001b[0m distributions \u001b[38;5;241m=\u001b[39m \u001b[43m_get_distributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(distributions\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/optuna/importance/_base.py:69\u001b[0m, in \u001b[0;36m_get_distributions\u001b[0;34m(study, params)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_distributions\u001b[39m(study: Study, params: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseDistribution]:\n\u001b[1;32m     68\u001b[0m     completed_trials \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mget_trials(deepcopy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, states\u001b[38;5;241m=\u001b[39m(TrialState\u001b[38;5;241m.\u001b[39mCOMPLETE,))\n\u001b[0;32m---> 69\u001b[0m     \u001b[43m_check_evaluate_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompleted_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m intersection_search_space(study\u001b[38;5;241m.\u001b[39mget_trials(deepcopy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m/dtu/blackhole/14/155764/DeepL-Project-Corn2/.venv/lib/python3.11/site-packages/optuna/importance/_base.py:114\u001b[0m, in \u001b[0;36m_check_evaluate_args\u001b[0;34m(completed_trials, params)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot evaluate parameter importances without completed trials.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(completed_trials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot evaluate parameter importances with only a single trial.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot evaluate parameter importances with only a single trial."
     ]
    }
   ],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "def generate_optuna_plots(study): #thanks claude\n",
    "    \"\"\"\n",
    "    Generate and display multiple Optuna visualization plots.\n",
    "    \n",
    "    Args:\n",
    "        study (optuna.study.Study): The Optuna study object to visualize\n",
    "    \"\"\"\n",
    "    # 1. Param Importances Plot\n",
    "    param_importances = vis.plot_param_importances(study)\n",
    "    param_importances.show()\n",
    "    \n",
    "    # 2. Optimization History Plot\n",
    "    optimization_history = vis.plot_optimization_history(study) ## Remember it is test AUC\n",
    "    optimization_history.show()\n",
    "    \n",
    "    # 3. Parallel Coordinate Plot\n",
    "    parallel_coordinate = vis.plot_parallel_coordinate(study)\n",
    "    parallel_coordinate.show()\n",
    "    \n",
    "    # 4. Slice Plot\n",
    "    slice_plot = vis.plot_slice(study)\n",
    "    slice_plot.show()\n",
    "    \n",
    "    # 5. Contour Plot\n",
    "    contour_plot = vis.plot_contour(study)\n",
    "    contour_plot.show()\n",
    "    \n",
    "        # EDF (Empirical Distribution Function) plot\n",
    "    edf_plot = vis.plot_edf(study)\n",
    "    edf_plot.show()\n",
    "    \n",
    "generate_optuna_plots(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Study Statistics:\n",
      "Number of completed trials: 4\n",
      "Number of pruned trials: 0\n",
      "Number of complete trials: 4\n",
      "\n",
      "Top 2 Trials:\n",
      "\n",
      "Trial 0\n",
      "AUC: 0.5578\n",
      "Parameters:\n",
      "  head_num: 29\n",
      "  head_dim: 12\n",
      "  attention_hidden_dim: 117\n",
      "  dropout: 0.06297744623833967\n",
      "  learning_rate: 5.48331531711811e-05\n",
      "\n",
      "Trial 1\n",
      "AUC: 0.5548\n",
      "Parameters:\n",
      "  head_num: 10\n",
      "  head_dim: 24\n",
      "  attention_hidden_dim: 210\n",
      "  dropout: 0.15981037963782108\n",
      "  learning_rate: 6.660790310583853e-05\n"
     ]
    }
   ],
   "source": [
    "# Get trial statistics\n",
    "print(\"\\nStudy Statistics:\")\n",
    "print(f\"Number of completed trials: {len(study.trials)}\")\n",
    "print(f\"Number of pruned trials: {len(study.get_trials(states=[optuna.trial.TrialState.PRUNED]))}\")\n",
    "print(f\"Number of complete trials: {len(study.get_trials(states=[optuna.trial.TrialState.COMPLETE]))}\")\n",
    "\n",
    "            \n",
    "print(\"\\nTop 2 Trials:\")\n",
    "sorted_trials = sorted(study.trials, key=lambda t: t.value, reverse=True)\n",
    "for trial in sorted_trials[:2]:\n",
    "    print(f\"\\nTrial {trial.number}\")\n",
    "    print(f\"AUC: {trial.value:.4f}\")\n",
    "    print(\"Parameters:\")\n",
    "    for param_name, param_value in trial.params.items():\n",
    "        print(f\"  {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE ⛄️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
