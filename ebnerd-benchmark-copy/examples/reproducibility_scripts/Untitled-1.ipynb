{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ebrec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Running DOCVEC\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mebrec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_nlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_transformers_word_embeddings\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mebrec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_articles\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_text2encoding_with_transformers\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ebrec'"
     ]
    }
   ],
   "source": [
    "# Running DOCVEC\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from ebrec.utils2._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils2._articles import convert_text2encoding_with_transformers\n",
    "\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import polars as pl\n",
    "import shutil\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from ebrec.utils2._constants import *\n",
    "\n",
    "from ebrec.utils2._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    "    ebnerd_from_path,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "\n",
    "from ebrec.utils2._python import (\n",
    "    write_submission_file,\n",
    "    rank_predictions_by_score,\n",
    "    write_json_file,\n",
    ")\n",
    "from ebrec.utils2._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils2._polars import split_df_chunks, concat_str_columns\n",
    "\n",
    "from ebrec.models.newsrec.dataloader import NRMSDataLoader, NRMSDataLoaderPretransform\n",
    "from ebrec.models.newsrec.model_config2 import (\n",
    "    hparams_nrms,\n",
    "    hparams_nrms_docvec,\n",
    "    hparams_to_dict,\n",
    "    print_hparams,\n",
    ")\n",
    "from ebrec.models.newsrec.nrms_docvec2 import NRMSDocVec\n",
    "from ebrec.models.newsrec import NRMSModel\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from args_nrms_docvec import get_args\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "for arg, val in vars(args).items():\n",
    "    print(f\"{arg} : {val}\")\n",
    "\n",
    "PATH = Path(args.data_path).expanduser()\n",
    "# Access arguments as variables\n",
    "SEED = args.seed\n",
    "DATASPLIT = args.datasplit\n",
    "DEBUG = args.debug\n",
    "BS_TRAIN = args.bs_train\n",
    "BS_TEST = args.bs_test\n",
    "BATCH_SIZE_TEST_WO_B = args.batch_size_test_wo_b\n",
    "BATCH_SIZE_TEST_W_B = args.batch_size_test_w_b\n",
    "HISTORY_SIZE = args.history_size\n",
    "NPRATIO = args.npratio\n",
    "EPOCHS = args.epochs\n",
    "TRAIN_FRACTION = args.train_fraction if not DEBUG else 0.0001\n",
    "FRACTION_TEST = args.fraction_test if not DEBUG else 0.0001\n",
    "\n",
    "NRMSLoader_training = (\n",
    "    NRMSDataLoaderPretransform\n",
    "    if args.nrms_loader == \"NRMSDataLoaderPretransform\"\n",
    "    else NRMSDataLoader\n",
    ")\n",
    "\n",
    "# =====================================================================================\n",
    "#  ############################# UNIQUE FOR NRMSModel ################################\n",
    "# =====================================================================================\n",
    "\n",
    "# Model in use:\n",
    "model_func = NRMSDocVec\n",
    "hparams = hparams_nrms_docvec\n",
    "#\n",
    "hparams.title_size = args.title_size\n",
    "hparams.history_size = args.history_size\n",
    "hparams.head_num = args.head_num\n",
    "hparams.head_dim = args.head_dim\n",
    "hparams.attention_hidden_dim = args.attention_hidden_dim\n",
    "hparams.newsencoder_units_per_layer = args.newsencoder_units_per_layer\n",
    "hparams.optimizer = args.optimizer\n",
    "hparams.loss = args.loss\n",
    "hparams.dropout = args.dropout\n",
    "hparams.learning_rate = args.learning_rate\n",
    "hparams.newsencoder_l2_regularization = args.newsencoder_l2_regularization\n",
    "\n",
    "\n",
    "# =============\n",
    "# Data-path\n",
    "DOC_VEC_PATH = PATH.joinpath(f\"artifacts/{args.document_embeddings}\")\n",
    "print(\"Initiating articles...\")\n",
    "df_articles = pl.read_parquet(DOC_VEC_PATH)\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=df_articles.columns[-1]\n",
    ")\n",
    "\n",
    "# =====================================================================================\n",
    "#  ############################# UNIQUE FOR NRMSDocVec ###############################\n",
    "# =====================================================================================\n",
    "\n",
    "print_hparams(hparams)\n",
    "\n",
    "# Dump paths:\n",
    "DUMP_DIR = Path(\"ebnerd_predictions\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "#\n",
    "DT_NOW = dt.datetime.now()\n",
    "#\n",
    "MODEL_NAME = model_func.__name__\n",
    "MODEL_OUTPUT_NAME = f\"{MODEL_NAME}-{DT_NOW}\"\n",
    "#\n",
    "ARTIFACT_DIR = DUMP_DIR.joinpath(\"test_predictions\", MODEL_OUTPUT_NAME)\n",
    "# Model monitoring:\n",
    "MODEL_WEIGHTS = DUMP_DIR.joinpath(f\"state_dict/{MODEL_OUTPUT_NAME}/weights\")\n",
    "LOG_DIR = DUMP_DIR.joinpath(f\"runs/{MODEL_OUTPUT_NAME}\")\n",
    "# Evaluating the test test can be memory intensive, we'll chunk it up:\n",
    "TEST_CHUNKS_DIR = ARTIFACT_DIR.joinpath(\"test_chunks\")\n",
    "TEST_CHUNKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "N_CHUNKS_TEST = args.n_chunks_test\n",
    "CHUNKS_DONE = args.chunks_done  # if it crashes, you can start from here.\n",
    "# Just trying keeping the dataframe slime:\n",
    "COLUMNS = [\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "]\n",
    "# Store hparams\n",
    "write_json_file(\n",
    "    hparams_to_dict(hparams),\n",
    "    ARTIFACT_DIR.joinpath(f\"{MODEL_NAME}_hparams.json\"),\n",
    ")\n",
    "write_json_file(vars(args), ARTIFACT_DIR.joinpath(f\"{MODEL_NAME}_argparser.json\"))\n",
    "\n",
    "# =====================================================================================\n",
    "# We'll use the training + validation sets for training.\n",
    "df = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            ebnerd_from_path(\n",
    "                PATH.joinpath(DATASPLIT, \"train\"),\n",
    "                history_size=HISTORY_SIZE,\n",
    "                padding=0,\n",
    "            ),\n",
    "            ebnerd_from_path(\n",
    "                PATH.joinpath(DATASPLIT, \"validation\"),\n",
    "                history_size=HISTORY_SIZE,\n",
    "                padding=0,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    .sample(fraction=TRAIN_FRACTION, shuffle=True, seed=SEED)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=NPRATIO,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    ")\n",
    "\n",
    "# We keep the last day of our training data as the validation set.\n",
    "last_dt = df[DEFAULT_IMPRESSION_TIMESTAMP_COL].dt.date().max() - dt.timedelta(days=1)\n",
    "df_train = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).dt.date() < last_dt)\n",
    "df_validation = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).dt.date() >= last_dt)\n",
    "\n",
    "# =====================================================================================\n",
    "print(f\"Initiating training-dataloader\")\n",
    "train_dataloader = NRMSLoader_training(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BS_TRAIN,\n",
    ")\n",
    "\n",
    "val_dataloader = NRMSLoader_training(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BS_TRAIN,\n",
    ")\n",
    "\n",
    "# =====================================================================================\n",
    "# CALLBACKS\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=LOG_DIR,\n",
    "    histogram_freq=1,\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_auc\",\n",
    "    mode=\"max\",\n",
    "    patience=4,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=MODEL_WEIGHTS,\n",
    "    monitor=\"val_auc\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1,\n",
    ")\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_auc\",\n",
    "    mode=\"max\",\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "callbacks = [tensorboard_callback, early_stopping, modelcheckpoint, lr_scheduler]\n",
    "\n",
    "# =====================================================================================\n",
    "model = model_func(\n",
    "    hparams=hparams,\n",
    "    seed=42,\n",
    ")\n",
    "model.model.compile(\n",
    "    optimizer=model.model.optimizer,\n",
    "    loss=model.model.loss,\n",
    "    metrics=[\"AUC\"],\n",
    ")\n",
    "f\"Initiating {MODEL_NAME}, start training...\"\n",
    "# =>\n",
    "hist = model.model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(f\"loading model: {MODEL_WEIGHTS}\")\n",
    "model.model.load_weights(MODEL_WEIGHTS)\n",
    "\n",
    "# =====================================================================================\n",
    "print(\"Initiating testset...\")\n",
    "df_test = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(\"ebnerd_testset\", \"test\"),\n",
    "        history_size=HISTORY_SIZE,\n",
    "        padding=0,\n",
    "    )\n",
    "    .sample(fraction=FRACTION_TEST)\n",
    "    .with_columns(\n",
    "        pl.col(DEFAULT_INVIEW_ARTICLES_COL)\n",
    "        .list.first()\n",
    "        .alias(DEFAULT_CLICKED_ARTICLES_COL)\n",
    "    )\n",
    "    .select(COLUMNS + [DEFAULT_IS_BEYOND_ACCURACY_COL])\n",
    "    .with_columns(\n",
    "        pl.col(DEFAULT_INVIEW_ARTICLES_COL)\n",
    "        .list.eval(pl.element() * 0)\n",
    "        .alias(DEFAULT_LABELS_COL)\n",
    "    )\n",
    ")\n",
    "# Split test in beyond-accuracy TRUE / FALSE. In the BA 'article_ids_inview' is 250.\n",
    "df_test_wo_beyond = df_test.filter(~pl.col(DEFAULT_IS_BEYOND_ACCURACY_COL))\n",
    "df_test_w_beyond = df_test.filter(pl.col(DEFAULT_IS_BEYOND_ACCURACY_COL))\n",
    "\n",
    "df_test_chunks = split_df_chunks(df_test_wo_beyond, n_chunks=N_CHUNKS_TEST)\n",
    "df_pred_test_wo_beyond = []\n",
    "print(\"Initiating testset without beyond-accuracy...\")\n",
    "for i, df_test_chunk in enumerate(df_test_chunks[CHUNKS_DONE:], start=1 + CHUNKS_DONE):\n",
    "    print(f\"Test chunk: {i}/{len(df_test_chunks)}\")\n",
    "    # Initialize DataLoader\n",
    "    test_dataloader_wo_b = NRMSDataLoader(\n",
    "        behaviors=df_test_chunk,\n",
    "        article_dict=article_mapping,\n",
    "        unknown_representation=\"zeros\",\n",
    "        history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "        eval_mode=True,\n",
    "        batch_size=BATCH_SIZE_TEST_WO_B,\n",
    "    )\n",
    "    # Predict and clear session\n",
    "    scores = model.scorer.predict(test_dataloader_wo_b)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Process the predictions\n",
    "    df_test_chunk = add_prediction_scores(df_test_chunk, scores.tolist()).with_columns(\n",
    "        pl.col(\"scores\")\n",
    "        .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "        .alias(\"ranked_scores\")\n",
    "    )\n",
    "\n",
    "    # Save the processed chunk\n",
    "    df_test_chunk.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "        TEST_CHUNKS_DIR.joinpath(f\"pred_wo_ba_{i}.parquet\")\n",
    "    )\n",
    "\n",
    "    # Append and clean up\n",
    "    df_pred_test_wo_beyond.append(df_test_chunk)\n",
    "\n",
    "    # Cleanup\n",
    "    del df_test_chunk, test_dataloader_wo_b, scores\n",
    "    gc.collect()\n",
    "\n",
    "df_pred_test_wo_beyond = pl.concat(df_pred_test_wo_beyond)\n",
    "df_pred_test_wo_beyond.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "    TEST_CHUNKS_DIR.joinpath(\"pred_wo_ba.parquet\")\n",
    ")\n",
    "# =====================================================================================\n",
    "print(\"Initiating testset with beyond-accuracy...\")\n",
    "test_dataloader_w_b = NRMSDataLoader(\n",
    "    behaviors=df_test_w_beyond,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=BATCH_SIZE_TEST_W_B,\n",
    ")\n",
    "scores = model.scorer.predict(test_dataloader_w_b)\n",
    "df_pred_test_w_beyond = add_prediction_scores(\n",
    "    df_test_w_beyond, scores.tolist()\n",
    ").with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "df_pred_test_w_beyond.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "    TEST_CHUNKS_DIR.joinpath(\"pred_w_ba.parquet\")\n",
    ")\n",
    "\n",
    "# =====================================================================================\n",
    "print(\"Saving prediction results...\")\n",
    "df_test = pl.concat([df_pred_test_wo_beyond, df_pred_test_w_beyond])\n",
    "df_test.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "    ARTIFACT_DIR.joinpath(\"test_predictions.parquet\")\n",
    ")\n",
    "\n",
    "if TEST_CHUNKS_DIR.exists() and TEST_CHUNKS_DIR.is_dir():\n",
    "    shutil.rmtree(TEST_CHUNKS_DIR)\n",
    "\n",
    "write_submission_file(\n",
    "    impression_ids=df_test[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_test[\"ranked_scores\"],\n",
    "    path=ARTIFACT_DIR.joinpath(\"predictions.txt\"),\n",
    "    filename_zip=f\"{MODEL_NAME}-{SEED}-{DATASPLIT}.zip\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
